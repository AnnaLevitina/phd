Chapter I Human Intervention as a Defensive Safeguard

1. Before the Data Protection Directive

1.1 Algorithm Aversion	

This work will study the development of the legislative intentions along with the current regulation regarding the automated decision making to make broader understanding of the applicable concepts, risk assessment and instruments which are to be used for ensuring the respect of human rights and fulfilment of obligations enacted by the legislators. Before moving towards the studying the development of the European legislative regulation we find it vital to explore the background preceding the Directive 95/46/EC of the European Parliament and of the Council of 24 October 1995 on the protection of individuals with regard to the processing of personal data and on the free movement of such data (the Data Protection Directive) being the first European attempt to harmonize the legislative approach to provision of protection of human rights against the automation tools. 
It is an obvious fact that the human beings through the whole history of human existence are constantly trying to invent the supportive tools for their life. History has witnessed such breakthroughs as the invention of wheel, which is of no surprise to anybody at the moment, the invention of steam engine and the development of machine tools which resulted in the industrial revolution and, consequently, substantial change of humans’ life, and nowadays we are living the digital revolution characterized with such attributes as high speed of technological development and appearance of such technological advancements as robotics and artificial intelligence  among others. However, perceptions towards various technological tools are not equal and while people tend to rely on certain tools the others may not be so much welcome.  
Thus, in 2019 BEUC (the European Consumer Organization) conducted a survey regarding knowledge about and trust in AI across nine European countries , notably, 82% of respondents of the survey previously heard of AI. Respondents suppose that AI is somewhat useful when dealing with routine tasks, and more than half of the respondents think that advanced technology is already able to autonomously speak/interact and recognise images and faces. 
At the same time the majority of respondents believe that AI will lead to abuse of personal data and that governments are using AI to control citizens. 51% of survey participants consider AI as leading to unfair discrimination. In Belgium, Italy, Portugal, and Spain 66% of respondents agreed or strongly agreed that AI can be hazardous and should be banned by authorities, moreover, more than half of the respondents have low trust in authorities to exert effective control over AI. In Belgium, Italy, Portugal, and Spain 77% and in Denmark, France, Germany, Poland, and Sweden 66% agree that users should have the right to say “no” to automated decision-making.
These findings from consumers’ answers to the survey held by BEUC may show us that the majority thinks of AI as a potentially hazardous technology and demonstrates lack of trust both to the technology itself and to the governments regulating its usage. The issue of trust arises not only in the society in general but has already received extensive discussion between experts and scholars of the industry as well. 
Generally, trustworthiness of algorithms and overall trust towards automated decision making constitute the fundamental question which shall be explored firstly before moving forward to the theme of this work. Understanding human perceptions about ADMS may complement the background of putting human decision maker above automotive systems as well as bring attention to the issue of trust to human decision makers themselves. This leads us to the notion of algorithm aversion which is, actually, not a new phenomenon. 
Algorithm aversion may be defined as a tendency to prefer human forecasts rather than those of algorithms and give more value to human input even if the human predictions are less accurate . Although the notion of algorithm is barely new  it has been drastically modified during the ages. What we tend to understand as algorithm today was influenced by a German mathematician David Hilbert who presented in 1928 a decision problem  - a challenge for an algorithm to decide whether a statement is universally valid, and further development of the problem by Alonzo Church and Alan Turing. It is notable that automation of problem solving has fascinated the specialists of different fields from mathematicians to neuroscientists and human decision making was being explored for the purpose of building of computational intelligence. Newell and Simon may be named as pioneers in bringing together human reasoning and computer science. 
In the year 1958 Shaw, Newell and Simon introduced a computer program aimed at simulating of human problem-solving process – General Problem Solver . This program was further developed by Newell and Simon through the next decades. Basically, General Problem Solver functioned via heuristic search in the particular problem space, e.g., chessboard, using permissible actions. This program was supposed to solve any general problem via its algorithm, but it ended up as infeasible to be applied to a real-world problem solving.
Notably, the research on the comparison of human to algorithm accuracy in forecasting, predicting and, therefore, decision supporting may be rooted to 1955 when Paul Meehl demonstrated the outperformance of algorithms over humans across different domains . Such outperformance was evidenced in various studies , nonetheless, humans tend to demonstrate distrust to the algorithmic outcome along with appreciation of human forecasts and predictions . 
As for today there is no consensus on the reasons of algorithm aversion  and possible way to mitigate it. Moreover, there are different point of view regarding algorithm aversion itself: some authors suggested that algorithm aversion is of a general nature , e.g., the degree of trust to algorithm is determined before human-algorithm interaction, while others assumed that aversion appears only when users become aware of algorithm performance, and consequently see them erring . However, some grounds driving algorithm aversion were presented across the studies: inability of algorithms to learn  and deemed ability of human to learn from the experience , inability of algorithms to properly address individual targets  or inability to operate with qualitative data , the desire of perfect forecast , i.e., there is no room for a mistake in case of algorithms’ employment in predicting, the issue of ethics and dehumanization . 
The later research also contributed to the idea that human tends to trust human agent more, even though they demonstrated less accurate than an algorithmic agent, due to the errors of algorithms , however overall trust to algorithmic predictions grew when the users saw the ability of algorithms to learn . It may be seen that the presented reasons of algorithm aversion related not only to the characteristics of employed algorithmic tools (accuracy, errors, ability to learn), but also to the ethics of such employment (dehumanization, possibility to rely on machine when making the substantive decisions ).
Obviously, the scope of trust to human agent and to algorithm agent may not be the same . Despite that it may be deemed that machines are more reliable than human beings due to their objectivity, impartiality, and rationality it may be argued that human tends to trust more to another human rather than to a machine. Level of confidence in human-algorithm relationships is more precarious. Having experienced an error in automated system even once human demonstrates tendency to replace machine agent with human agent. At the same time human exhibits much more indulgence towards another human being. But just think how often you have heard about another failure which occurred due to the human factor.  Think of a concept of the second opinion widely spread in medicine and even law which basically refers to seeking second judgement on the matter and to a certain extent represents mistrust or at least doubt in the initial decision. This concept appeared long before algorithms were presented. Yet, paradoxically humans continue to believe, trust, and rely on other humans which may, probably, be supported by the friend-or-foe identification, or that is to say belonging to a certain group, in this particular case, defining him or herself as a member of mankind contrasting to machines. 
Another interesting issue in exploring level of trust in human-computer interaction and algorithm aversion is the position of “trustor”. On the one hand, in case the algorithmic tools are employed by a human decision maker to support decision-making process the assessment of the algorithmic outcome is carried out by the same human, i.e., the algorithmic outcome is compared with the internal decision of the same human agent. On the other hand, a human may be given with two decisions: the algorithmic and the human one, and consequently the question will be which decision is more credible or trustworthy. In other words, the question is whether we tend to trust ourselves and another human being at the same degree. It is argued that there is a strong overall tendency to give more credibility to one’s own decisions compared to the others’ advice . Nonetheless, some research shows that underestimation of automated tools does not depend on the position of human being either the human decision-maker or the human subject to decision . This brings us to another interesting fact regarding the trust in human-human and human-computer interactions: human tends to trust a machine less than a human expert, but more than a human non-expert . However, human-experts, when comparing the algorithmic outcome with their own decisions, rely more on themselves, even if this results in inferior final decision, which may be dictated by overconfidence of a human decision-maker . 
These findings are of particular interest for the purpose of this work as individual automated decision-making foresees a tandem of human operators with distinct levels of expertise and automated tool to produce a decision regarding another human being in different domains and sectors which means that a subject may be affected by a decision which accuracy and trustworthiness are dependent from a human operator’s self-assessment as well as his or her attitude towards algorithms. Commonly, several approaches are identified: misuse, disuse and abuse of algorithms . Misuse refers to unreasonable overreliance; conversely, disuse implies under-reliance on the algorithm without regards to quality of its performance; abuse, in turn, represents poor design and implementation of algorithm along with ignorance of its effect on human performance . These approaches demonstrate deviant, disproportionate response of human operator to algorithmic aid which means that the real functionality and performance are not considered, human-computer interaction is improper and, therefore, the final decision may be impaired .
In the recent research,  Burton explored systematized reasons of algorithm aversion and possible solution to overcome it. Based on previous studies he highlights five problems which serve as a basis for algorithm aversion. Firstly, Burton addresses the issue of user’s false expectations which refers to a decision maker’s expectations about what algorithm should do and how it functions. Additionally, he draws a distinction between the perceptions to algorithmic outcome of people having specific experience with decision supporting systems and specific experience in the certain domain, indicating that the first group tends to consider the advice produced by the automated means while the latter do not. To solve this issue, he suggests the development of algorithmic literacy among human operators. 
Secondly, Burton refers to lack of control as a reason of algorithm aversion, meaning that human decision maker lacks the trust in automated system if he or she cannot understand or modify its functionality. As a cure for this problem proposes human-in-the-loop decision making which means that human operator has an opportunity to provide input, intervene to the operations and override the outcome of the employed automated means. 
The third problem addressed in the Burton’s research is lack of incentivization, i.e., the chance of success of implementation of algorithms is lower if the human operator is not additionally motivated from both economic and social perspectives. It is interesting that social encouraging plays the greater role : the societal perceptions to usage of algorithmic system and conformity with such perceptions by a human decision-maker may force him or her to use or to abandon automated tools. 
Another suggested reason of algorithm aversion is human intuition. Although algorithms are often criticized for what is known as a black box: opacity and non-transparency of their operations and, therefore, lack of reasoning of the outcome, human decision-making process is neither fully traceable, that is to say human intuition being a part of the human decision making also constitutes a black box. We wish to additionally introduce here the notion of tacit human knowledge which is also challenging to reproduce in algorithmic systems . Another concept worth mentioning is “qualia”, or the way a particular human experiences the world . This relatively new philosophical matter has not received a unique assessment; however, it is hard to deny the existence of subjectivity in human perception and judgement which may lead to both: on the one hand this subjectivity constitutes a part of incomputable tacit knowledge, but on the other hand it may result in human bias and unfair human decisions. 
The last reason of algorithm aversion suggested by Burton is conflicting concepts of rationality between humans and algorithms. The perceptions about human and algorithmic decision-making process and about what is rational pave the road to a possible failure of harmonizing of the decisional outcome assessment and, consequently, algorithm aversion. Favoring one concept along with denial of others may lead to introduction of undesirable human biases as well as misuse of algorithmic tools which means in any case loss of quality of the final decision. 
Therefore, calibration of trust in algorithmic systems depends on various factors which may correlate with algorithm-specific features and human-specific qualities. The researchers foresaw that all proposed reasons may be negotiated and, consequently, algorithm aversion may be reduced, e.g., improvement of algorithmic tools, and to a greater extent their ability to learn and traceability, may enhance their usage and reliability, as well as human operators’ literacy and incentivization may contribute to achieving the same objective. However, trying to find the ways to achieve a goal only will be meaningful if the goal itself is determined and desired. 
Here it is noteworthy to revert to one of the suggested reasons of algorithm aversion which is not connected to algorithm-specific or human-specific characteristics but refers to more philosophical or ethical settings: dehumanization and possibility to rely on automated tools when making significant decisions. The question of algorithms employment as a decision making or even decision support tool is always discussed alongside the appeal to such concepts as human dignity, human self-determination, human agency, human oversight . All these concepts refer to primacy of a human being, human autonomy and overall anthropocentrism. Algorithmic tools shall be developed as an instrument to help people and should not replace or undermine them. Human wellbeing and human fundamental rights shall always preside over any automated tool usage. The current discussion is led by antagonistic character of human-machine positioning, and it may be noted that the certain amount of human fear to be replaced or usurped by the machine is presented. Perhaps, such an attitude is dictated mostly by science fiction and has no rational basis behind it. Nonetheless, the issue of dehumanization and inappropriateness of delegating of decisions vital to human beings to the algorithms are evidenced. 
We suppose that it is essential to make a reference to the current state of art of artificial intelligence to deeply understand what we can ask from algorithms and why the issue of trust is addressed. We are not targeting here at profound explanation of the technologies involved rather than giving a short general overview of the level of development to make it more clear why the issue of trust is faced. 
As for today, there are three types of artificial intelligence : weak, strong, and super AI. Weak AI, also known as Narrow AI, is represented by any particular program that performs certain task and, normally, outperform human in such a task. It functions on a specific dataset, it is neither conscious, nor emotional driven. General AI, or Strong AI, is deemed to be more human-like intelligent, emulate any intellectual task that human can, and will incorporate creativity and imagination. The most “intelligent” Super AI is considered to exceed human intelligence in all spheres. All today known applications of artificial intelligence are examples of Weak AI, meaning that they only can outperform human in one particular task which they are designed to do.
The question that may be posed to the data scientists in this regard is whether we are able to educate algorithms to fully emulate human intelligence, including emotional intelligence, compassion and what shall be done to improve the level of trustworthiness of algorithms. We agree here that even with better performing of artificial intelligence in the issue of trust remains an open question  as the basis for the demonstrated distrust is more complex rather than solely technology-based and also includes such concepts as human primacy and superiority of human autonomy which may be found throughout European policies and regulation regarding the use of algorithmic decision tools. 
Notably, there are some researches suggesting that these concepts of juxtaposition of human and machine are driven by the European historical legacy . The issue of human dignity and human autonomy and assessing algorithmic tools as a threat to these fundamental principles of human existence due to the statistical and mathematical character of the algorithms opposed to the human flexible, empathic and intuitive approach to decision making may be rooted to the World War II and Nazi’s privacy abuse  when Third Reich used census data to track their Jewish victims. 
These violations of privacy and fundamental human rights led to Europe associating automated data processing with indignity of human beings and contraposing human and machines  . However, not all jurisdictions have the same attitude. As an example, we want to refer to the United States which have close connections with the European Union regarding the data privacy protection . The U.S. approach to regulating the algorithms differed from that of Europe. First of all, it might be characterized by computational neutrality , i.e., computers were not considered as a threat to human dignity, and, moreover, technological development was an indicator of national progress . The U.S. law makers while protecting the privacy did not prohibit the automated tools considering that they might help avoiding human biases . Thus, the U.S. followed more optimistic perspective towards algorithms comparing to the fear of being oppressed by machines that suffused Europe in the post-war period. We will see further how these perceptions formed the European legislative approach regarding regulation of algorithms.
It is worth to mention that despite algorithm aversion and tendency of disuse there are some studies showing that in some domains human may rely more on automated tools, for example, when dealing with objective and non-personal decision tasks, e.g., when obtaining a financial advice  or numeric estimations . At the same time, personal decisions are deemed to be fairer and more reliable when come from a human agent . As previously said, the confidence in algorithmic advice grows if a human operator sees that algorithm demonstrates the ability to learn, which means that with the development of technologies and quality of their functioning people may rely more on automated tools and therefore delegate some tasks, including decision making, or at least decision support, to algorithms with a higher degree of trustworthiness which may result in increase of efficiency of the overall process of decision making. 
Additionally, people tend to count on algorithmic outcome if they experience the positive operations of algorithmic tools . It is worth mentioning, that if human operator him- or herself cannot identify algorithmic errors, he or she continues to adopt false advice . This may be explained by the human expectation of faultless performance of the algorithmic tools and consequently, blind implementation of their outcome . Also, some previously research shows that lay people rely more on algorithmic advising tools . All these may result in undesirable overreliance on algorithmic outcome and implementation of inaccurate advice to the particular case which depending on its sensitivity could constitute discrimination and human rights violation. 
It is interesting to note here some data concerning the Polish initiative “Unemployed profiling” which took place from 2012 and consisted in automated profiling of unemployed people. According to AI Watch’s “Artificial intelligence in Public services”  study during the operation of this initiative “less than 1 in 100 decisions made by the algorithm have been questioned by the responsible clerk” – a person who was considered to serve as a censor for the final better decision. 
Obviously, algorithms like any other phenomena bear their own risks and benefits. On the one hand, technological development makes it possible to delegate decision-making from human to algorithms, e.g., to ease the workload, and eliminate human subjectivity and biases. On the other hand, algorithms with their opacity, usually addressed as “black box” problem, may introduce their own biases and, moreover, threaten human fundamental rights and freedoms, human dignity, and human autonomy. The algorithmic-specific characteristics and uniqueness of human decision-making process along with the overall anthropocentrism resulted in algorithm aversion which is broadly discussed by the researchers, and which led to disuse of automated decision supporting or decision-making instruments. However, failure to adequately use algorithmic tools is presented not only by disuse, but also by misuse or over-reliance which is also referred to as algorithm appreciation . 
Despite the general tendency to algorithm aversion or under-reliance which evidently provided the fuel for the first European protective regulatory approaches, we consider that over-reliance on algorithms as malicious practice as well. We reckon that adequate and balanced approach shall be employed to guaranty the proper protection of human fundamental rights including human dignity, right to the integrity of the person, right to good administration, non-discrimination, and protection of personal data. We will explore further the development of regulation regarding the algorithms, in particular, algorithmic decision-making and requirements with respect thereto which are aimed at better decision making from the perspective of the affected human being.


1.2 National Regulation of European Member States

Previously, we have already mentioned that the European approach to the regulation of data protection and automated individual decision-making was driven by the Third Reich and privacy abuse during the World War II. The history of personal record-keeping may be tracked to the ancient civilizations , however, the modern systematic approach to the complex record-keeping in Western Europe was evidenced only in last centuries due to codification, levying of taxes and centralized administration . Such systematic approach might have contributed to the changing of citizen-government relations and tension between privacy rights and information necessary for governance . 
With growth of amount of structured personal data correspondingly increased the possibility of privacy infringement. The automation comparing to manual record-keeping may ease such infringement. Thus, with the help of the IBM’s punch cards which were based on the national census data facilitated the government of Adolf Hitler to pursue Jewish and other “undesirable” citizens and to monitor the concentration camps . The post-war European states, obviously, were concerned about fundamental human rights and in the end of 1950 the European Convention on Human Rights , which provide for privacy rights, was opened for signatures. Nonetheless, the European Convention on Human Rights did not foresee any rights (or prohibitions) with regard to automated data processing, including automated decision making. 
It is interesting that even with acknowledgement of and awareness about privacy abuse during the World War II and emerging trend of privacy protection in 1960s automated means were not considered as a threat to data subjects by the legislators and the main goal was to achieve the efficiency with automated data processing . For instance, notwithstanding the first world-recognized data protection law – the Hessian Data Protection Act of 1970 - the German government assumed that the level of regulation, and, therefore, protection of data subjects, was adequate up to the beginning of 1970s . The seventh decade of XX century in Europe was marked by various legislative attempts to grant adequate protection for individuals in the field of data privacy. Thus, Sweden enacted its first national Data Act in 1973, Germany, Belgium, Norway, Luxembourg, Denmark made their proposals for data protection acts in 1977 and France passed its law concerning data processing, files and liberties in 1977 . 
All the above-mentioned legislative attempts tried to protect individuals from abusive usage of their personal data and the scope of suggested protection was quite similar going from one to another. We would like to make below a short overview of the introduced regulation to explore the roots of today’s European regulatory approach. Obviously, the restriction or, at least, regulation of collection, storage, processing and transmission of personal data were subject of adopted or proposed legislative acts in the seventh decade of the last century. The acts addressed what is now considered as automated data processing in different ways. 
The world’s first national data protection law was enacted in Sweden in 1973. The Swedish Data Act applied to data processing in both private and public sectors and regulated “personal register systems” – a register, list of other notations which are maintained with the help of automatic data processing and contained personal data . Existence of such registers was permitted if was authorized by the Data Inspection Board, a supervisory body, always considering whether the register did not create undue intrusion to the privacy of individuals . The Data Inspection Board might regulate obtaining data for registers, performance of automated data processing, technical equipment, adaptation of data through automated data processing, disclosure of information to data subjects, the scope of accessible information, issuance of data to the persons other than data subject, keeping and selection of information, control, and security . It is noteworthy that what we now think as of rights of data subjects (e.g., right to access, information, accuracy, and completion of data) was considered by the Swedish Data Act as obligations of data processors. The Act also envisaged fines and imprisonment for the breach of data protection law . Remarkably, automated decision making was not addressed by the regulation. 
Further, the German Data Protection Law envisaged protection of personal data which were processed (i.e., stored, altered, deleted, or transmitted) in information storage systems which were defined as organized according to certain criteria collection of data which might be rearranged and utilized under other criteria through automated procedures . The regulation recognized data processing in both public and private sectors (where two types of processing was envisaged: for the internal use or for third parties on commercial basis) . For the public bodies data processing was permitted for the purpose of fulfilment of their tasks , at the same time they were obliged to publicly notify of type of stored data, duties for which the data is required, concerned groups, addressee of transmitted data, type of transmitted data . For the private-sector entities the lawful basis for data processing was a contractual or contract-like relations between the parties always when there was no assumption of adverse effect on data subject  The processing was only permissible if authorized by law or consented by the data subject . Furthermore, for those persons and corporations which processed data for third parties on commercial basis the Law foresaw the obligation to notify the supervisory authority about their operations and such processing was permitted only if there was no adverse effect on data subjects . The German Data Protection Law envisaged particular rights of data subjects: right to information about the stored data, to correction of such data in case it was incorrect, to “blocking” of data in case there was no possibility to establish correctness or incorrectness of data, and to deletion of data if the processing was not or terminated to be permissible . The Law also foresaw application of technical and organizational measures to data processing . 
It is noteworthy that the German Data Protection Law provided for appointment of an agent for data protection for those persons or corporations which were to function under the private law having more than five employees and, moreover, there were established the requirements for such agents: the special expertise and reliability necessary for fulfillment of the duties, and independent character of the agent’s positions, i.e., no instruction might have been given to an agent . Additionally, the existence of the supervisory body in public sector was considered – a Federal Commissioner for Data Protection – which shall have exercised control over provisions on data protection by authorities and other public agencies and had to maintain a list of automated data processing bodies and had access to them . As a liability for infringement of data protection regulation the Law stipulated the penalties and, even, deprivation of liberty.
The German Data Protection Law did not address automated decision making; however, the automated processing of personal data was restricted enough to guarantee that personal data might be processed only within the scope of the regulation. Notwithstanding the requirement to the data processing, rights of individuals concerned and requirement to the data processors, obviously, decisions based on automated processing might be taken in that epoque in Germany, it seems that the legislators aimed at regulating collection and the purpose of use of personal data without being preoccupied about effect of decisions based on automated processing that might affect individuals. 
The French Data Protection Law of 1978 appears to envisage more well-developed regulation. First of all, the principles of data processing were established: it should have been at the service of every citizen and might not harm human identity, private life, individual or public liberty . The Law gave a definition to automated data processing, this should have included all operations performed by automatic means related to collection, registering, addition to, modification, storage, and destruction of personal  data as well as operations of the same kind related to the use of files or data bases and especially to interconnection or linkages, referrals, or communications of personal data . The Law specifically highlighted interconnection and linkages of personal data, which, in our opinion, means that the legislators were aware of possible threat to individuals created by executing analysis and assessment of personal data.
The French Data Protection Law considered data processing in public and private sectors, where both should have been subject to prior authorization (opinion) by or declaration to, accordingly, the National Commission on Data Processing and Liberties – a supervisory body established under this regulation . Besides its controlling and supervising functions, the Commission should have maintained a public list of data processing operations stating the authorization or date of operation’s declaration for public and private data processors, correspondently; the purpose of processing; the service where data subject’s right to access might be addressed; the categories and the recipients of personal data . It is our view that provision of such difference in requirements (authorization versus declarative way) to establishing of data processing in public and private sector may evidence that lawmakers assumed that data processing by the entities of public sector might be potentially more harmful to the data subjects.
Furthermore, the French Data Protection Law foresaw special categories of data: race, political, philosophical, or religious opinions, trade unions membership, which might not be automatically processed (entered and stored) without the data subject’s express consent with a derogation when such a processing performed for reasons of public interest . Gathering of any personal data by fraudulent, unfair, or illicit means was forbidden . The security of the processing and the data should have been guaranteed by assurance measures taken by data processors .
The Law envisaged certain rights of data subjects regarding the processing of their data. It is interesting that the consent to the data being process was deemed as a right of data subject  and not the mean by which the processor might receive the permission for processing. Other permissible processing under the regulation was that carried out in public sector always when approved by the supervisory Commission. The data subjects had right to information about the obligatory character of furnishing data, the consequences of data not being furnished, the persons or legal entities to receive data and about their rights of access to, correction of collected data .
The right to access – the data subjects’ right to make inquiries for the purpose of obtaining information whether there were personal data processed and, if yes, what kind of data were processed  – is a key right of data subjects. While exercising their right of access data subject might request require correction, completion, clarification, update, or obliteration of data in case the data were inaccurate, incomplete, ambiguous, out-of-date or data processing is forbidden . What is interesting for the purpose of this work is that the French Data Protection Law was a pioneer in addressing automated decision making, moreover, the legislators placed the prohibition of any court, administrative or private decision to be based on automatically processed data into Chapter I (Principle and Definitions) of the Law, therefore, in our view, gave special meaning to this prohibition being the cornerstone of data processing. Thus, Article 2 of the regulation provided as follows:
“No court decision resulting in a judgment on human behavior may use as its basis automatically processed data which outline the profile or the personality of the person concerned.
No administrative or private decision resulting in a judgment on human behavior may use as its basis automatically processed data which outline the profile or the personality of the person concerned.”
Notably, the regulation envisaged the prohibition of decisions in both public and private sectors which might be based on automatically processed data which allows to define data subject’s profile of personality if such decisions were to result in a judgement on human behavior. Interpretating the wording of this article 2 it may be concluded that assessment or evaluating of the data subject based on automated data processing were prohibited. Obviously, this legal provision formed the basis of further European data protection regulation with respect to automated decision making and profiling.
It is interesting that the Belgium lawmakers addressed not only the personal data, but also illicit listening and picture taking in the Belgium Privacy Act of 1977. However, for the purpose of this work we will explore the regulation of data processing only. The Belgium Data Privacy Act applied to the data banks which might operate in public and private sectors. Notably, all then-current legislative acts recognized and addressed automated data processing in both sectors (with exception of the Denmark Data Protection Bill which excluded government for the scope of the regulation ). Data bank was defined as any filing cabinet or register established with a view to automated processing of data or to an automated system of processing data . The Belgium Privacy Act foresaw general principles for data banks: they should not have violated the right to privacy and might not have as their purpose any discrimination. The Act also excluded certain categories of data which might not be stored in data banks if not authorized by law or consented by data subject: e.g., data regarding race, political, union, insurance, cultural, philosophical, religious opinions, social assistance, medical examinations, criminal records . The formation of data banks should have been authorized by the supervisory authority – the Office for the Protection of Privacy with respect to Data Banks. The Denmark Data Protection Bill, the Luxembourg Data Bank Bill and the Norwegian Bill Relating to Personal Data Registers also envisaged the procedure of authorization by the relevant supervisory authorities . All the regulations mentioned above also restricted the use of special categories of data, e.g., race, political, religious beliefs, criminal circumstances, health information, sexual life and family affairs .
The scope of data subjects’ rights in Belgium, Denmark, Luxembourg and Norway was maintained at the same level: data subjects was granted with the right to information, to correction, completion or deletion of the data. As for the data processor the regulations envisaged the appointment of a responsible person and implementation of measures to guarantee security of the data processing and compliance with data protection laws. 
Notwithstanding the absence of direct specific regulation of automated decision making in these four legislative acts, it is noteworthy that some predecessors thereto were presented. Thus, Chapter 3 of the Denmark Data Protection Bill specifically regulated the establishment and operations of credit bureaus – recording of information for evaluation of economic standing and credit worthiness for the purpose of dissemination. Although there was no prohibition of such activity, automated data processing with the aim of evaluating economic circumstances and credit rating of data subjects had some restrictions. First of all, anybody who was to establish a credit bureau should have notified the supervisory body about its activity. Credit bureaus might only record and publish such information that was important for fulfilment of the purpose, and special categories of information might not be recorded nor published. There was a direct prohibition to record or publish information that “spoke against” positive credit rating or was older than five years with the derogation of that it was totally important for the execution of evaluation. Data subjects had the right of information, correction or deletion of the information recorded or published by credit bureaus. Obviously, the criteria of “importance” for evaluation of economic status and credit ratings is very vague and decrease the data subjects’ protection regarding the possibility of being evaluated. However, this regulation demonstrated that carrying out assessment activities based on automated data processing reserved special attention of legislators. Also, the approach not to process data which might result in negative rating appeared, which, in our opinion, may be a prototype of further national regulation under the General Data Protection Regulation which we will further explore. 
Specific regulation for credit information services existed also in the Norwegian Data Bill . The term “credit information services” was defined as activities consisting of providing information regarding creditworthiness or financial reliability of data subjects. Such activity required prior permission of the King who should have considered whether such activity was to be operated in an honest and sound manner . The credit information enterprise was obliged to ensure the completeness of information used and that such information would have not gave grounds for unjustified or unreasonably negative attitude to the data subject; information older that five years might been use only if it was abundantly clear that such information continues to be of a substantial significance for the proper assessment of data subjects  and special categories of data might be used only if so authorized by the King . Credit information might only be transmitted to those parties that could justify their request and, normally, should have been made in written form . Although the regulation also seems of weak protection for data subjects it is obvious that the legislators aimed to introduce restrictions regarding their evaluation through automated data processing.
To sum up, then-current legislative acts and proposals of the European countries recognized automated data processing as a threat to privacy of individuals and attempted to restrict their usage. Automated data processing systems made personal data easily accessible and capable of easy application. The legislators aimed to regulate data processing in both public and private sectors (save for Denmark), restricted types of data which might have been used, imposed obligations on data processors, mainly pointed at maintaining the accuracy and security of data and data processing and compliance with data protection laws and gave certain rights to data subjects – right to information and correction of data may be named as the most important ones. Mainly, automated data processing was permitted if authorized by law or consented by data subjects, in most cases prior authorization of supervisory bodies was necessary to conduct data processing based on automated means. However, all the above reviewed regulations seem to be ambiguous and vague and provided no or weak protection to data subjects regarding the output of automated data processing, evaluation of data subjects and decisions that might have been made based on such processing. Obviously, the French Data Protection Law may be deemed as more developed and that which constituted the base for further European regulation in the field of privacy and data protection. 

2. Directive 95/46/CE (Data Protection Directive)

2.1. Human Intervention as a Formal Safeguard

Further convergence of the European countries required the harmonization of data protection regulation of the European approach to personal data processing. The European Commission was preoccupied with the possible negative impact of incoherent national legislation on internal market and, consequently, in 1990 made a proposal for a directive with a view of consistency of regulation of data protection . On 24 October 1995 the Data Protection Directive (officially: Directive 95/46/EC on the protection of individuals with regard to the processing of personal data and on the free movement of such data) was created as a crucial element of European privacy and human rights law. The Data Protection Directive came into force on 13 December 1995 and required European Member States to implement the corresponding provisions in national law by 24 October 1998.
The Data Protection Directive had two main objectives: firstly, the Member States should have provided the protection of fundamental rights and freedoms of natural persons, and in particular their right to privacy with respect to the processing of personal data and, secondly, the Member States should have not restricted nor prohibited the free flow of personal data between them . The DPD set out the standards for the most important concepts of data protection regulation: provided the definition of personal data, addressed quality and categories of data, determined the scope of data protection rules, prescribed data processors’ obligations and data subjects’ rights and established the European level supervisory authority – Article 29 Working Party. 
Among other provisions article 15 was introduced which stated as follows:
“Automated individual decisions
1. Member States shall grant the right to every person not to be subject to a decision which produces legal effects concerning him or significantly affects him and which is based solely on automated processing of data intended to evaluate certain personal aspects relating to him, such as his performance at work, creditworthiness, reliability, conduct, etc.
2. Subject to the other Articles of this Directive, Member States shall provide that a person may be subjected to a decision of the kind referred to in paragraph 1 if that decision:
(a) is taken in the course of the entering into or performance of a contract, provided the request for the entering into or the performance of the contract, lodged by the data subject, has been satisfied or that there are suitable measures to safeguard his legitimate interests, such as arrangements allowing him to put his point of view; or
(b) is authorized by a law which also lays down measures to safeguard the data subject's legitimate interests.”
The Data Protection Directive envisaged that the Member States should have implemented in their national laws the provisions regarding the automated individual decisions at the level of protection proposed by this article 15 of the DPD. Further we are going to explore what did this protection granted regarding the data subjects being subject of automated decisions constituted of. Before delving into how the DPD addressed the automated individual decisions (the “how”) we need to understand what the subject matter of the regulation (the “what”) was. 
The Data Protection Directive defined automated individual decisions as decisions which produces legal effects concerning him or significantly affects him and which is based solely on automated processing of data intended to evaluate certain personal aspects relating to data subject. This definition consists of several components. First of all, we want to pay attention to the element which provides for decisions to be based solely on automated processing of [personal] data. The DPD envisaged that the term “personal data” should have meant any information relating to an identified or identifiable natural person while an identifiable person is one who can be identified, directly or indirectly, in particular by reference to an identification number or to one or more factors specific to his physical, physiological, mental, economic, cultural or social identity . 
The legislator did not specifically define the notion of automated processing of data, however, processing of personal data was defined as “any operation or set of operations which is performed upon personal data, whether or not by automatic means, such as collection, recording, organization, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, blocking, erasure or destruction” . From this definition we can withdraw the understanding of automated processing of personal data as any operation or set of operations which is performed upon personal data by automatic means. The notion of automatic means did not receive any definition in the Data Protection Directive, and it may be argued that those days the processing involved only filing systems and computer mainframes . We consider that employment of such a language might provide a broader protection for a data subject in the environment of constantly emerging technologies as the automatic means may refer to any mean that functioning without needing a person to operate them. 
Another point to be noted is the word “solely” which posed additional barrier to the applicability of article 15 (1) of the Data Protection Directive. Obviously, the lawmakers were driven by fear of sophisticated machines to deprive human dignity. Additionally, in the Proposal for data protection regulation of 1992  they stressed out that the automated outcome in the decision making “has an apparently objective and incontrovertible character to which a human decision-maker may attach too much weight…abdicating his own responsibilities”, consequently, human intervention was required to be implemented in the final decision. Automated processing was deemed to serve as an aid and the strict application of its outcome by human decision maker was considered as an automated decision notwithstanding the formal presence of the latter in the decision-making process. Thus, the legislators for the first time addressed the verge of human intervention in automated decision making. 
The second element of the provision of article 15 (1) we want to address is that automated processing of [personal] data should have as its intention evaluation of certain personal aspects relating to data subjects. The legislators envisaged some examples of such personal aspects: data subject’s performance at work, creditworthiness, reliability, conduct, etc. We reckon the roots of this condition may be traced to the provisions of the Norwegian Data Bill, the Danish Data Protection Bill and the French Data Protection Law of 1970s referring to evaluation of data subjects’ creditworthiness and outline of profile or personality of data subjects. Further the same approach was implemented by the first drafts of the DPD: the first Proposal repeated the earlier French Data Protection law stating that the data processing was to define data subjects’ profile or personality, the second Proposal change the wording to the defining a personality profile. The enacted regulation addressed the processing aimed at evaluation of certain personal aspects of data subjects. Notwithstanding the absence of the specific term “profiling” in the Data Protection Directive, it may be understood that, in terms of the General Data Protection Regulation , the DPD restricted the applicability of the article 15 (1) to the decisions based only on profiling. In our opinion such wording of the article 15 (1) of the DPD imposed limitations to the applicability of the regulation as any decision based on any other form of automated data processing might fall out from its scope.
The last component of the provision of article 15 (1) of the Data Protection Directive is that the decision should have produced legal effects concerning data subjects or significantly affects them. Here we meet two factors: a decision and legal or significant effects produced by such a decision. The DPD did not required that the decision be made in a particular form, obviously, neither the term “decision” was limited to a certain authoritative decision (e.g., judicial, or administrative). Some authors suggested that the decision was deemed to be made once certain attitude, opinion or stance towards the data subject had been made and it had a binding character for the decision maker . We agree that such broader understanding of the term “decision” seems more suitable for the purpose of the application of the regulation. Additionally, it may be noted that the preceding wording of this provision contained in the Proposals of 1990 and 1992 contained the description of the decisions subject to the regulation by article 15 of the DPD – they should have been of an administrative or private character, however, the legislators omitted this specification in the final text of the Directive. 
Another point is the criteria of legal or significant effects which shall be produced by a decision. The first Proposal for the regulation of 1990 did not refer to any effect to be produced by the decision, at the same time it explained that the suggested provision was to protect the interests of the data subject in participating in the making of decisions which are of importance to the data subject. However, such approach seems not to be feasible due to difference of the individuals’ perceptions of importance of given decision. The later Proposal of 1992 introduced the criteria of adverse effect and explained that the decision might have been invoked against the data subject and had consequences for him. However, in our opinion, the scope of the term “adversely” was not clearly spelled out. Finally, the criteria of producing of legal or significant effects was given by the Data Protection Directive, however, the scope of the decisions which article 15 (1) was applicable may be contested. While legal effects may be delineated by influence on one’s legal rights or duties, the scope of significant effects is ambiguous . 
The DPD does not provide any explanation to the notion of significant effects. Some authors suggested that the decision was required to be significantly adverse in its consequences to data subject , which basically excluded favorable automated decisions from the scope of article 15 of the DPD. We consider that such exclusion is not reasonable, firstly, because the legislator themselves ruled out the criteria of adversity from the regulation, secondly, it is in our opinion that legal effects as well may be either adverse or favorable and the question which might be posed why only significant effects should have met the criteria of adversity, and, thirdly, we reckon that such limitation might have provided undesirable narrowing of the scope of applicability of the regulation. From our point of view the lawmakers by introducing the alternative to legal effect by way of referring to the decisions which significantly affect data subject considered that the extent of the latter should have been comparable to that of the legal effect. This suggestion may be also confirmed by overviewing of further European data protection regulation which developed this wording to the “similarly significant effects” , which we address further in Chapter II. 
Hence the Data Protection Directive became the first regional regulation which addressed the ongoing application of automated means in decision-making process. The historical background of this regulation along with overall algorithmic aversion demonstrated by humans dictated the fear of misuse of personal data including by way of employment of the machines which, in opinion of the legislators posed the threat to human dignity and reduced human beings to data shadows. Consequently, the European Parliament enacted the Data Protection Directive which opposed the decisions based on solely automated personal data processing and produced legal or significant effect on data subjects by introducing obligatory human intervention in the decision making. Thus, blind application of the automated outcome by the human operator was not yet possible. However, the legislators did not define the scope of such intervention neither the criteria for it to be qualified as adequate input from the human decision-maker’s side. The practical sense of this legislative clause as may be evidenced today merely existed, so that we cannot assess the impact of this protective provision. Moreover, the final ambit of the regulation of automated individual decision was to be determined by the national laws of the Member States which imposed additional risk of misunderstanding and, consequently, differentiation in the scope of data subjects’ protection vis-à-vis automated decisions safeguarded by the human involvement in the decision making. This notwithstanding it is clear that the Data Protection Directive formally provided for some degree of human presence as a safeguard to automated decision making. 

2.2. Human Intervention as a Sufficient Safeguard

It is noteworthy that along with the introduction of at least some level of human intervention as an obligatory attributed of the automated individual decision to be qualified out of the scope of article 15 (1) of the Data Protection Regulation the lawmakers envisaged additional safeguards for better protection of data subjects’ rights and interests. We would like to firstly address paragraph 2 of article 15 of the DPD which contained derogations from the general rule stipulated by article 15 (1) to explore the extent of the granted protection. 
Article 15 (2) of the DPD stated as follows:
“Subject to the other Articles of this Directive, Member States shall provide that a person may be subjected to a decision of the kind referred to in paragraph 1 if that decision:
(a) is taken in the course of the entering into or performance of a contract, provided the request for the entering into or the performance of the contract, lodged by the data subject, has been satisfied or that there are suitable measures to safeguard his legitimate interests, such as arrangements allowing him to put his point of view; or
(b) is authorized by a law which also lays down measures to safeguard the data subject's legitimate interests.”
The article provided for entering into or performance of a contract and authorization by law as possible exceptions from the general rule which the Member States might have implemented through the national laws. Considering the subject of this work we do not see it as reasonable to deepen into the ambiguities accrued due to the wording of this article 15 (2), but rather dwell on the safeguards the legislators referred to.
First, section (a) envisaged that if automated individual decisions were permitted due to the entering into or performance of a contract “suitable measures” were to be presented for the purpose of safeguarding of data subjects’ legitimate interests. As an example of such measures the legislators provided for arrangements allowing the data subject to put his[her] point of view. It was deemed that such a right to express data subject’s point of view corresponded to the obligation of “those who are formally responsible for the decision concerned”, i.e., the human decision maker to take such point of view into consideration . We agree with such elaboration of the given right and, moreover, from our perspective, although this provision of the Data Protection Directive did not foresee the right to obtain human intervention as a safeguard to the permitted automated individual decisions it might have been withdrawn from its wording. Namely, once provided his or her opinion data subject reserved the right to for that opinion to be weighed by the human agent, therefore, the presence of human decision maker was obvious in such cases. However, relying upon such interpretation of the legislative provision has yet to allow for separate right to obtain human intervention as an independent safeguard for data subjects’ legitimate interests in case of contractual derogation from the general rule of article 15 (1) of the DPD.
Secondly, in case of derogation from a general rule regarding the automated individual decisions due to authorization by law “measures to safeguard the data subject's legitimate interests” should have been employed. It may be questioned whether there was any difference between “suitable measures” and “measures” referred accordingly by section a and b of article 15 (2) of the DPD. However, we are of the opinion that the presented difference in the wording is not meaningful as the intention of the legislators was that the Member States might clarify such measures within the national laws , which might, undoubtedly, end up in the segregation of regulation within the EU single market, but refine the scope of data subjects’ protection against the automated decisions. 
However, there are some points we wish to address in light of safeguards provided by the Data Protection Directive itself. Obviously, the right of data subject to express his or her point of view vis-à-vis automated decision was one of the safeguards which might have been employed in case of the legislatively permitted automated decision making . Taking into consideration the above it is our opinion that in case of the derogation presented in article 15 (2) b human involvement in the automated decision-making might be presented as a derivative safeguard. 
Further, we want to appeal to the first paragraph of article 15 (2) which provided the following: “Subject to the other Articles of this Directive, Member States shall provide that a person may be subjected to a [automated] decision…”. Obviously, the highlighted wording presumed that other rights of data subjects should have been accounted as safeguards when automated decisions were possible to be made. From this point of view one of the cornerstone rights of data subjects is the right of access  which stipulated as follows:
Member States shall guarantee every data subject the right to obtain from the controller:
(a) without constraint at reasonable intervals and without excessive delay or expense:
- confirmation as to whether or not data relating to him are being processed and information at least as to the purposes of the processing, the categories of data concerned, and the recipients or categories of recipients to whom the data are disclosed,
- communication to him in an intelligible form of the data undergoing processing and of any available information as to their source,
- knowledge of the logic involved in any automatic processing of data concerning him at least in the case of the automated decisions referred to in Article 15 (1);
(b) as appropriate the rectification, erasure or blocking of data the processing of which does not comply with the provisions of this Directive, in particular because of the incomplete or inaccurate nature of the data;
(c) notification to third parties to whom the data have been disclosed of any rectification, erasure or blocking carried out in compliance with (b), unless this proves impossible or involves a disproportionate effort.
From the perspective of this work particular attention should be given to the right of data subject to obtain from data controller “knowledge of the logic involved in any automatic processing of data concerning him[/her] at least in the case of the automated decisions…”. At the same time Recital 41 of the Data Protection Directive, notwithstanding its non-binding character, gave some authoritative guidelines to the extent of this right: it should have not adversely affected trade secrets or intellectual property and in particular the copyright protecting the software which in turn might not result in the data subject being refused all information. This means that even respecting the intellectual property rights data controller was obliged to provide data subject with the information regarding the logic involved in automated data processing resulting in automated decision. 
It appears that the depth of disclosure of the required information to be given to data subject should have allowed him or her to understand the reasoning of the made decision. It may be also confirmed by the right to object to data processing granted to the data subject by article 14 of the DPD and objection requires the understanding the rationale which lies with the automated processing (including that which might result in automated decision). Additionally, although in then-current data protection regulation transparency of data processing was not enshrined as a legislative principle the DPD made a reference to transparency as a characteristic of data processing which should have been ensured in the Member States . Obviously, the issue of transparency required a certain level of disclosure regarding the operation of methods and tools employed for the purpose of automated data processing and automated decisions, as the case might be. 
To provide the required “knowledge of logic involved in automated [data] processing” data controller should have had such information in its own possession, or, as referred by Bygrave, should have had a possibility to “comprehend the logic of the automated steps involved ” . Evidently, comprehension of logic involved in automated data processing did not necessarily require participation of human controller in overall decision-making process and, particularly, in its final stage of application of the automated outcome to the certain case, and thus did not represent human intervention in the decision as required by the regulation. However, in our opinion, the data subjects’ right to access which was corresponding to the data controllers’ obligation to provide the requested information and hence the data controllers’ right to obtain such knowledge from, e.g., the developer of the employed automated means, provided the human decision maker with the possibility to understand the way which decision-making systems operated in, and, consequently, to assess the level of reliability of the automated outcome. Such a comprehensive approach to analyze and interpret the Data Protection Directive allows to withdraw additional safeguarding tools not only for better data protection, but also for obtaining of better, more weighed, decision regarding him or her.   
Here we consider useful to appeal to the types of human involvement in automated decision making which are generally acknowledged in ethics and legal field although the suggested typology was elaborated later on. Thus, there are three approaches: human-in-the-loop (HITL), human-on-the-loop (HOTL), and human-in-command (HIC) . High-Level Expert Group on AI (HLEGAI) addresses all three concepts as a possibility to implement human oversight in automated decision making. HITL implies human intervention in any decision cycle of the automated decision-making systems (ADMS), however, HLEGAI considered this as neither desirable nor possible in many cases. HOTL refers to human intervention at the stage of design of the ADMS and controlling its operations which may be more reasonable . HIC seems to be more complex concept and relates not only to interaction with ADMS itself, but to overall assessment of consequences of employment of automated systems for decision making, including societal and ethical impact, and therefore, general possibility to use or not to use ADMS and level of human discretion in particular cases. We agree to the opinion  that HIC seems to be the right approach for regulation of artificial intelligence and automated decision making as one of its applications. Evidently, technological advancements do not exist in a vacuum and their implementation to the habitual human life is linked to and affects a variety of areas. The legislation has as its goals guaranteeing the balance of rights and interests of different stakeholders along with better protection of those who are in a weak position, and with regard to the data protection field, especially to automated individual decisions, obviously, this position is taken by the data subjects. From this perspective, encouraging of the HIC approach seems the most adequate response to the challenges brought by the technologies. 
We also consider that human-in command concept allows to take into consideration the interests of any human being affected by the employment of automated tools rather than only concentrate on the interaction between a human agent and a machine, which means that data subject affected by automated decision making and his/her interests shall be considered for the purpose of evaluation of impact of application (or non-application, as the case may be) of automated means as well. From the human-in-command point of view it might be deemed reasonable to put human operator’s intervention in automated decision making, even in view of its applicability only in the final stage of decision-making process and absence of any requirements to its scope and limits as an antidote to suppressing of human rights and possible threats introduced by machines. Moreover, in recognition of necessity to better protect rights and legitimate interests of data subjects (being a part of human component of the HIC concept) it may be said that the level of such protection is defined not only (if at all) by human intervention in automated decisions, but also by collateral safeguards granted by the data protection regulation.
Today, obviously, we can evidence the immaturity of the Data Protection Directive regarding the extent to which it was applied to automated individual decisions. This may also be confirmed by absence of any practical use of article 15 of the DPD. Nonetheless, then-current level of data subjects’ protection was considered adequate by the lawmakers, and it was not before the year 2009 when the European Commission recognized the necessity to develop the data protection law  to address new challenges for data privacy due to the development of modern technologies which resulted in the comprehensive data protection reform started in the year 2012 .
Notwithstanding its weaknesses the Data Protection Directive represented the first regional attempt to harmonize data protection rules across the European Union and to strengthen data subjects’ position by providing certain instruments such as the right no to be subject to a decision which produces legal effects concerning him or her or significantly affects him or her and which is based solely on automated processing of data intended to evaluate certain personal aspects relating to him or her. Namely, the right to have human operator involved in the decision making at least at its final stage was guaranteed as a foremost safeguard to preserve human dignity and human control over human lives. Maintaining the anthropocentric approach, the DPD provided data subjects with the additional rights against being treated as data shadows such as right to obtain the knowledge of the logic involved in the automated data processing and the possibility for data controllers to obtain the corresponding information and, therefore, to improve the quality of the decisional outcome on their side. 
However, it is our opinion that human-in-command concept, especially from the perspective of considering the protection of a data subject, being the one who is affected mostly by the recruitment of automated decision-making tools, was not actually implemented by the Data Protection Directive. The introduced human agent involvement which was deemed to defend the human against the machine did not receive an adequate legislative determination. Thus, such questions as to what extent a human operator should have intervene in the automated decision making, by what means a human operator might examine and assess the automated outcome, what criteria should have been considered at the stage of (non)application of such outcome and what criteria might demonstrate the sufficiency of the human intervention were not addressed in the regulation. Another set of issues which were not handled in the DPD relates to the requirements to the automated decision-making system and automated processing itself. This might include such matters as transparency, accuracy, technical robustness and security of the employed means and their operation. Another question to be addressed is the requirements to human operators themselves: obviously, their technology-specific education and overall qualification, measures of quality of human decision-making, ways of monitoring of human input, its quality and scope were not covered by then-current European data protection law. 
It is our view that observing and addressing the above issues in unified legislative norms and standards may ensure the human-in-command concept implementation, guarantee the principles of human dignity, integrity, as well as with fundamental human rights, and, moreover, allow data subject to receive better, fairer, non-discriminatory decisions which produces legal or significant effect on their lives. 

3. Human Intervention as a Worldwide Safeguard

3.1. Council of Europe. Convention 108 and Protocols

One of the most prominent initiatives regarding automated data processing at the international arena is that of the Council of Europe. The Council of Europe is an international organization founded after the World War II which has as its goal protection of human rights, democracy, and rule of law. Obviously, data protection constitutes an important part of protection of human rights. Thus, on 28th of January 1981 the Council of Europe opened for signatures the Convention 108  – the first international initiative for protection of data subjects vis-à-vis automatic data processing. The Convention 108 was to shape the protection of privacy and data subjects not only in Europe, but beyond . However, in practical terms the 1981-year version of the document was not ratified nor implemented by any other state rather than members of the Council of Europe. Nonetheless, it may be said that the Convention 108 served as the first unified regulation which aimed at transnational legal stability in the field of data protection . Obviously, the regulation was inspired by the national legal attempts in the data protection field of 1970s and was enacted with the goal of safeguarding fundamental rights and, particularly the right to privacy with respect to automatic personal data processing by employing of unified principles to be applicable within the states ratified the Convention 108 without regard to nationality of residence of the individual. 
The Convention 108 elaborated the definition of “automatic processing” of personal data and basic principles for data protection such as fairness, lawfulness and accuracy of data processing, limitations on purpose and time . It recognized special categories of data which were prohibited to processed automatically  and provided for additional rights of data subjects such as the right to establish the existence of automatic data file (i.e., the right to obtain information) and, therefore, the right of rectification or erasure of the data concerning data subject . These instruments were deemed to allow data subject to obtain better control over the personal data related to him or her both in public and private sectors. It is interesting that although 1970s’ national laws of some European countries in the field of data protection had already recognized the limitation of automated data processing in particular domains such as assessment of creditworthiness of data subjects or fully prohibited decisions based on automated data processing the Convention 108 did not include the corresponding provisions in its text. Thus, at the international, or at least at regional European, level the necessity of human presence in the decision-making process based on the automated means was not admitted. 
Nevertheless, the Convention 108 played a significant role as a model-maker for data protection approach across the world. Even considering that it was ratified only by the member states of the Council of Europe it might be used as a template for adoption of European approach towards safeguarding data subjects’ rights by other countries. Obviously, it set the general principles for data processing and elaborated the structural vision regarding the privacy and protection of fundamental human rights when employing automated data processing and even more once the modernized Convention 108 (the Convention 108+ ) was open for signatures in 2018 . 
For the purpose of this work the Convention 108+ is more interesting as it addresses automated decision making among other matters pertaining to automated data processing. It is undeniable that the necessity of modernization of the 1981-year version of the Convention 108 was dictated by the evolving technologies including those involved in the decision-making process. Thus, for the first time at international level by way of enacted Convention 108+ it was enshrined as a legally binding provision that “every individual shall have a right not to be subject to a decision significantly affecting him or her based solely on an automated processing of data without having his or her view taken into consideration” . 
It is interesting that, as compared to the European data protection regulation we will see in Chapter Two, article 9 littera a of the Convention 108+ does not prohibit the automated decisions themselves, rather than provide for the right of data subject to express his or her point of view regarding the decision making. The Explanatory report  in its paragraph 75 envisages that “individual who may be subject to a purely automated decision has the right to challenge such a decision by putting forward, in a meaningful manner, his or her point of view and arguments”. Thus, the Convention 108+ does not disclaim the possibility of automated decision to be made regarding the data subject but empowers the latter to influence the final decision by providing his or her view as a sufficient safeguard challenging the automated decision. Moreover, as we have already mentioned above the contest of automated decision or putting forward data subject’s view and arguments vis-à-vis such a decision also requires a human operator on the side of decision making to handle the challenge and, therefore, the human intervention in the decision making shall be presented either way. However, contrasting to the European data protection regulation which requires human intervention in a form of assessment of automated output along with other sources of relevant information for the purpose of making the final decision (see Chapter Two), we reckon that the Convention 108+ stipulates human intervention in form of weighing of the data subject’s view and arguments vis-à-vis the automated outcome.  
It is noteworthy that the Explanatory report  providing the clarification to the provision of article 9 of the Convention 108+ in a part of considering data subject’s view explicates that “[i]n particular, the data subject should have the opportunity to substantiate the possible inaccuracy of the personal data before it is used, the irrelevance of the profile to be applied to his or her particular situation, or other factors that will have an impact on the result of the automated decision”. Thus, the Convention 108+ provides for possibility of data subject to participate in the shaping of input which will serve as a basis of the automated decision as a part of the right to have his or her view considered within the decision making which forms ex ante control of data processing by data subjects.
Additionally, the Convention 108+ introduces the right of data subject to obtain knowledge of the reasoning underlying data processing where the results of such processing are applied to him or her  and the Explanatory report extend the scope of this provision by specifying that not only reasoning, but also the consequences of such reasoning, which led to any resulting conclusions shall be available to data subjects. This knowledge shall contribute to the effective exercise of other essential safeguards such as the right to object and the right to complain to a competent authority . These two safeguards constitute the ex-post supervision accessible by the data subjects which together with ex ante data subjects’ involvement in the decision-making process, in our opinion, create the comprehensive mechanism of control of personal data and data processing by the concerned data subjects. 
It is argued that the regulation enacted in the Convention 108+ was influenced a lot by the General Data Protection Regulation of the European Union which led to the “Europeanisation” of the data protection throughout the world . The reasons for non-European regions being persuaded to the European data protection policy preferences are still a matter of scholars’ research . Yet, it is obvious that the European data protection standards were promoted by the European Commission to be adopted by third parties and international organizations  (including the Council of Europe). The European Commission itself recognized that the Convention 108+ helped spreading the “European data protection model” globally . At the same time, it is believed by some authors that the Convention 108+ is “more understandable data protection template” which provide clear and concise regulatory approach, including for automated decision-making regulation . 
We want to contribute to the differentiation of the above-mentioned regulation by highlighting that, despite the apparent similarity of the employed language in the legislative provision regarding the automated decision, it may be noted, as we explained above, that the wording of article 9 of the Convention 108+ provides for slightly distinct scope of protection of data subjects. We consider that incorporation of data subject’s view in the process of decision making at the stage preceding the human decision maker involvement allows better control over resulted decision and allows data subject to obtain separate knowledge about automated outcome, which may be satisfactory to the data subject, and prevent bias (being provoked by the own mindset of human operator or by the algorithmic aversion/appreciation, as the case may be) which may be introduced by human decision-maker beforehand.
Furthermore, the regulation of automated decision making is strongly connected with the values promoted by the Council of Europe in connection with data protection being human dignity and personal autonomy . The reference to human dignity is echoed throughout all Europe-originated legal instruments and, obviously, is borrowed by the Convention 108+ as an international regulation from that of Europe. As for the value of personal autonomy it is notable that the Preamble of the Convention 108+ states that personal autonomy is “based on a person’s right to control his or her personal data and the processing of such data”. Thereby, the Convention 108+ posed data subject and his or her ability to exercise control over their data and data processing at the forefront of data protection apparatus. The Council of Europe explains that individuals shall be put in the position to know about, to understand and to control the processing of their personal data by others . Additionally, the Directorate General of Humans Rights and Rule of Law provides in its Guidelines that artificial intelligence applications should allow meaningful control by data subjects over the data processing and related effects on individuals and on society . This guideline also highlights the central place of data subjects in overall employment of automated means which undoubtedly includes decision making being fully automated decision making or only decision supportive systems. 
It is our opinion that the wording of the Explanatory report, the Guidelines and the Preamble of the Convention 108+ itself may be interpreted so as the regulatory approach does not only contrast human operator to the automated means in the decision-making process, which is enshrined by the regulation of automated decision making, but also foresees that the ultimate objective of the data protection regulation is to empower data subject to effectively and profitably manage data processing related to them. If the Guidelines refers more to the relations between data subjects and AI applications, the Explanatory report directly states that individuals shall be enabled to control the processing of their personal data by others  which may include human operators in case of automated decision making. From our point of view, this means that not only automated decision-making tools, but also human decision-maker shall satisfy certain requirements to implement better protection of individuals subject to the decisions based on the automated data processing. From this perspective, it seems reasonable and beneficial to put data subject’s view to be considered at the earlier stages of overall decision-making process and the wording of automated decisions regulation of the Convention 108+ corresponds to the values promoted by it. 
However, we observe the lack of requirements to be asked from the human decision-maker in the Convention 108+ (as well as in the European data protection law) which, in our opinion, would allow data subject to enjoy better decision-making as well as better control over processing of his or her personal data. For this purpose, potentially, may serve the newly introduced obligation imposed on data controllers (or processors, where applicable) of implementation of appropriate measures to comply with the data protection regulation  and examination of data processing impact . The Explanatory report in its paragraph 85 provides for some examples of appropriate measures considered by the regulation which include training employees and setting up internal procedures to enable the verification and demonstration of compliance. It is our opinion that these measures may help establishing certain requirements which human decision-maker shall meet as they may include training and assessment of human operators involved in the processes where automated decision-making tools are employed. Impact or risk assessment which shall be carried out before the processing as well may foresee human decision-maker as a part of the data processing resulting in the decision affecting data subject and, therefore, impose certain guidelines or standards which human decision-maker shall comply with. Nevertheless, the particular scope of these tools is left to the data controllers (and processors) and no legislative standards are set for them, which, obviously, weakens data subjects’ protection.
To summarize, the Convention 108+ being inspired by the European approach to data protection regulation envisages human intervention as a safeguard to potential automated mistakes. At the same time, we consider that the wording of automated decisions regulation employed by the Council of Europe provides for different, broader, scope of data subjects control over their data processing within the decision-making process which we would like to make more illustrative by the following scheme: 
Despite the human intervention as a safeguarding tool is presented in the Convention 108+, obviously, it may not serve as a panacea to automated fallacy inasmuch as human operator may introduce its own bias and discrimination of data subject as well as the human decision-maker’s input may be preconditioned by automated outcome, either by blind acceptance or blind rejection due to the human perception of automated tools. Therefore, it is essential to impose requirements which shall be met by the human intervenor engaged in overall decision-making process. Under the current version of the data protection regulation standards and assessment of compliance with them may be implemented through the measures and data protection impact assessment procedure stipulated by the data controllers or processors, as the case may be. Nonetheless, we are of the opinion that this loophole shall be further addressed in form of setting of at least minimum legislative standard the human decision-maker would have to comply with, which would sustain better data subjects’ protection.

3.2. Another perspective: the U.S.

3.2.1. Introduction

Notwithstanding the focus of this work being the European data protection regulation and its development, we also want to address the U.S. approach towards automated decision-making. The U.S. regulation is of the particular interest for us due to the data flow and transatlantic commerce and as an example of another approach to the regulation in the field of data protection and privacy. Earlier the EU-U.S. Privacy Shield Framework were elaborated as a mechanism under which data controller (and processors, as the case may be) might have demonstrated the compliance with the European data protection requirements. However, in the year 2020 this framework has been recognized as invalid by the Court of Justice of the European Union judgement . This notwithstanding the U.S. government continues to administrate the U-U.S. Privacy Shield program. As for today no new mechanism of compliance with the European Data Protection regulation were offered, nonetheless, the U.S. demonstrate their awareness regarding the scope of protection of data subjects, including in case of automated decision making, notwithstanding the U.S. concept of “technologies’ neutrality”. Thus, several automated decision-making systems bills were proposed by the states . 
We deem necessary before addressing the subject mentioning the specifics of the U.S. legal system. The U.S. Constitution provides for dual legal system: the federal one and that of the states . If the competences are not withheld by the federal government, the states reserve the right to regulate the matter. Nonetheless, the federal laws are the “supreme laws of the land” and, consequently, in case of any contradiction prevail on the state laws . From the perspective of this work, the U.S. has not introduced so far, any general (federal) prohibition of automated decisions similar to that of the European regulation, however, the issue is addressed in several sector-specific laws, e.g., in the consumer protection law and regulation of credit system at the federal level, also there is some prominent state laws in the field of privacy such as the California Privacy Rights Act . Taking the above into consideration we contemplate for the further research the following structure: firstly, we will address the Due process of law, as a complex concept introduced by the U.S. constitutional law, and the consumer protection regulation presented within the sector-specific laws and, finally, we will consider the latest state privacy regulation initiatives. 

3.2.2. The Due Process Clause.

The Due process is a concept introduced by the U.S. Constitution in its Fifth and Fourteenth amendments. Under these rulings neither federal, nor state government may deprive any person of life, liberty, or property, without due process of law . These provisions provide for certain requirements mostly to the governmental actions, i.e., the public sector, nonetheless, they may be applicable to the private sector as well if the private party was employed for the governmental purpose . The Due process clause impose procedural safeguards for an individual stipulating that the process affecting such an individual be fair. This notwithstanding the Due process is somewhat flexible and does not provide one-size-fits-all remedy for the individuals rather than depends on the gravity of interests concerned. In the case of Goldberg v. Kelly  regarding the termination of welfare benefit it was ruled out that in case of governmental actions the affected individual shall have a right to be heard, a right to adequate notice providing the reasoning and the opportunity to present his or her own arguments and evidence.
 Later, in the case Mathews v. Eldridge  the U.S. Supreme Court derived a three-factor analysis which should be applied when weighing the constitutionality of the administrative actions for the termination of disability’s benefits in the context of ensuring of the due process: “(1) the private interest that will be affected by the official action; (2) the risk of an erroneous deprivation of such interest through the procedures used, and probable value, if any, of additional procedural safeguards; and (3) the Government's interest, including the fiscal and administrative burdens that the additional or substitute procedures would entail”. What is interesting about this case is that is procure two factors: the private interest which is, in out opinion, may correspond to the legal or similarly significant effects of the data subjects in terms of the GDPR and the possibility of the resulted decision to be erroneous which means that such a decision may be unfair, discriminatory or, including biased among other things. Further case law elaborated relatively unified principles to be presented which allows to assess the process as a due: (1) participatory procedures (participation of the concerned individual); (2) a neutral arbiter; (3) prior process (the hearing precedes the adverse action); and (4) continuity (the hearing rights attach at all stages) . 
Evidently, the Due process clause does not address directly automated decision making, however, it provides an instrument to ensure the proper procedure for any governmental decision of whatever nature, including made on the basis of automated data processing. Considering the major subject of this work being human intervention in automated decision making it should be highlighted that the above-mentioned requirements envisages that neutral arbiter shall be presented, therefore, human operator of automated decision-making system shall be involved in the process. Here the consequent question would be what the scope of the human intervention is deemed adequate in terms of employment of automated means for governmental decision making. The answer may be found also in the U.S. case law. 
One of the cases we want to appeal is the case of Malenchik v. State  where Anthony Malenchik being a plaintiff argued against legitimacy of consideration of the numerical score (LSI-R), elaborated, including, on the basis of the automated personal data processing, by the trial court. Nonetheless, the Supreme Court of Indiana ruled that the algorithmic risk assessment score may be deemed as adequate evidence if considered as a supplemental  to other presented evidence. Another relevant case is State v. Gordon  in which the Court of Appeals of Iowa vacated the defendant’s imprisonment due to the fact that the lower court assigned importance to the risk level score and considered it as an aggravating factor for the sentence. The iconic case for the subject which also defines the admissibility of the algorithmic tools in the decision making is State v. Loomis . The Supreme Court of Wisconsin considered algorithmic risk score consideration by the lower court legitimate as it was supported by other independent factors and was not determinative. These cases show that the algorithmic output may be considered in the decision-making as a complementary factor, nonetheless other evidence shall be presented and prevail in the final decision. In our opinion, this ruling corresponds to the authoritative guidelines of the Article 29 Working Party stating that “[i]f a human being reviews and takes account of other factors in making the final decision, that decision would not be ‘based solely’ on automated processing”. 
The above observations allow us to conclude that the Due process clause envisages the requirement of neutral human decision-maker in any governmental decision and in case of algorithmic decision-making algorithmic output shall play only supplemental role to other factors to be considered in the decision-making process. 
Further, we want to pay attention to the part of an affected individual and his or her role and attached rights in the Due process. From the point of view of the individual, the Due process clause stipulates his or her participation in the decision-making process, exercising of hearing rights and the right to have a hearing prior to adverse action. These requirements mean that the concerned individual shall be engaged in the decision-making process, express his or her point of view, present his or her arguments and evidence before any decision is made. Such scope of involvement, in out opinion, allows the individual to understand the reasoning and the factors, including algorithmic output, taken into consideration and object to any of them, as the case may be, which provides opportunity to ensure better decision-making. From our point of view, the Due process clause considering the attached rights of the concerned individual is more similar to the provisions of the Convention 108+ rather by the European General Data Protection Regulation and provides for a safeguard against human-introduced bias or discriminatory practices. The above notwithstanding, there is no sufficient caselaw regarding the fully automated decision-making so far and it will be seen further how the Due process clause will be applied by the courts in this field in the future . Moreover, the clause is only applicable in the public sector (and for certain extent to the private parties hired for the governmental purposes) and does not cover automated decision making in the private sector.

3.2.2. Consumer protection laws

The private sector regulation in the context of automated decision-making is addressed mostly by the sector-specific statutes. There are two meaningful frameworks operating with automated decisions we want to address in this work: the Fair Credit Reporting Act  and the Equal Credit Opportunity Act . The FCRA protects information collected by the consumer reporting agencies (CRAs) which may be gathered in consumer reports. The FCRA defines consumer reports as any written, oral, or other communication of any information by a consumer reporting agency bearing on a consumer’s credit worthiness, credit standing, credit capacity, character, general reputation, personal characteristics, or mode of living which is used or expected to be used or collected in whole or in part for the purpose of serving as a factor in establishing the consumer’s eligibility for credit or insurance to be used primarily for personal, family, or household purposes; employment purposes; or any other purpose authorized under the FCRA . Such permissible purposes are listed in §604 of the FCRA and constitute quite a broad range of industries. Consumer score also includes numerical or other evaluation of data by a CRA, such as a credit score . Notably, the FCRA contains the list of adverse actions which includes such fields as crediting, insurance, employment, and includes any adverse to the interest of the consumer which provides for scope of the expanded application of the prescribed rules. The Equal Credit Opportunity Act provides more narrow regulation and concerns discriminatory practices in crediting on the basis of special categories of information such as race, gender, age or marital status. 
Both Acts provide for certain information rights of the individuals. The FCRA envisages that upon request CRAs shall provide consumer with all information in the consumer’s file, source of such information, addressees of such information. Credit scores are to be disclosed upon the specific request. CRAs are also required to provide trained personnel to explain to the consumer any disclosed information. The Act also envisages correction and update of the information used in consumer files. This right to obtain information allow individuals to control the accuracy of data regarding them possessed by the CRAs, however, does not prevent any unfair or discriminatory action (including decision) against them. §615 of the FCRA stands for the establishing of the requirements on “users” of the consumer reports in case such users take adverse actions on the basis of information contained in consumer reports. In case of adverse action, the individual shall be informed about such an action and provided with the use credit score. The individual reserves the right to dispute the accuracy of information provided by the CRA to the user and the right to obtain the reasoning for the adverse action, however, only nature of considered information and not the criteria which led to the decision shall be disclosed. In contrast, the ECOA envisages the provision of actual reasons for the decision. The decision itself may be contested under the U.S. law.
Considering the above it may be said that in case of automated decision making in private sector the concerned individual will have the right to obtain human involvement on the stage of data input, i.e., through the access to and correction of information on the basis of which decision is taken, or, similarly, at the stage of appealing of the decision, e.g., through the judicial procedure. Although human intervention in automated decision making in terms of the European data protection regulation is not required by the reviewed statutes, the U.S. lawmakers consider the possibility to ensure accuracy of input information and the right to contest the final decision as sufficient safeguards for protection of individuals. 

3.2.3. State Privacy Regulation.

One of the most prominent approach is introduced by the State of California, which regulation is also interesting due to the size of population and its importance from the point of view of concentration of companies involved into tech industry. The California Consumer Privacy Act  (CCPA) of 2018 which came into force from 2020 envisages the privacy rights and protections for California consumers, yet in the same 2020 year the lawmakers made a step even further and came up with the proposition for California Privacy Rights Act  (CPRA) which updates and expands the preceding regulation.
First of all, the CPRA provides the definition of personal information which is rather broad and includes among others commercial information, including records of personal property, purchased products and services, internet activity information, professional and education information. Further, it defines terms processing and profiling  which are extremely similar to those of the European data protection regulation. Although the CPRA operate with the term consumer, it defines it as a natural person who is a California resident. In turn, the term business includes legal entities of any kind, including a person who does business, that collects or on behalf of which personal information is collected, yet there are specific threshold requirements for businesses to fall into the scope of the definition. The CPRA required businesses to inform consumers about what information they collect, sources and ways of use of such information, they are limited by the purpose of collection of the information. Consumers are entitled to access the personal information and to request the correction and deletion of their personal information. These rights enable individual to control their data and, therefore, to the certain extent influence the data processing and its outcome, however, ex post validation and the quality of the outcome, i.e., fair, non-discriminatory outcome, is not guaranteed and the individual it not provided with such rights to efficiently manage the data processing rights under the CPRA. Although the Act does not address automated decision making itself, it incorporates the provision which delegates to the California Attorney General the power and obligation to adopt regulation “governing access and opt-out rights with respect to businesses’ use of automated decision-making technology, including profiling and requiring businesses’ response to access requests to include meaningful information about the logic involved in those decision-making processes, as well as a description of the likely outcome of the process with respect to the consumer”. 
Further, the California lawmakers introduced the Assembly Bill  regarding the public contracts relating to automatic decision systems which is aimed to minimize the risk of adverse and discriminatory impacts resulting from the design and application of automated decision systems. The proposed Bill provides for an automated decision system impact assessment requiring among other explanation of how ADMS functions, logic relationship between input and output, description of potential disparate impacts and best practices to mitigate such impacts. So far, neither of regulations envisages human decision maker input to the final decision, however, it is evident that the lawmakers are preoccupied with the possible adverse effects of ADMS employment in both public and private sector. We will have the opportunity to see in the nearest future how the proposed regulation prove itself. 
Taking in consideration the above observations we may conclude that despite that the U.S. legislative does not contain general prohibition of automated decision making and does not necessarily require human participation in the decision making as the European regulation does, there are legislative instruments which address automated decision making ensuring fair, unbiased, and non-discriminatory decisions regarding individuals in the majority significant domains. It may be said that the U.S. regulation is not homogenous due to the specifics of the U.S. legal system separating federal and state laws and, moreover, the regulation of automated decision making in public and private sector varies. In our opinion, individuals in their relations with public entities are granted with the broad protection under the Due process clause which allows them to participate and, therefore, to control more efficiently the decision-making process at the different stages. Private sector automated decision-making regulation is even more fragmentated and is represented by several sector-specific statues and state legislation. Nonetheless, the most critical domains such as crediting and banking, insurance, employment which are usually deemed as emblematic spheres where adverse effects of automated decision-making may significantly affect individuals’ lives are addressed by the U.S. legislation. Moreover, there are a growing number of the state legislative initiatives specifically addressing automated decision making which means that the lawmakers notwithstanding the general perception of algorithmic neutrality are becoming concerned with the possible adverse effect of the automated decision-making systems. Nevertheless, it seems that the U.S. legal pathway stands for ensuring quality of the final algorithmic decision and absence of discriminatory practices throughout the ADMS for the benefit of the concerned individual rather than contrasting human decision-makers to decision-making machines. 

Chapter II Human Intervention as a Sufficient Safeguard

1. Automated decision-making: definition and concepts

1.1.	GDPR, a new framework for automatic decisions

Acknowledging the drastic development of technologies and expansion in use of digital services and Internet the European Commission set out strategy on strengthening of data protection rules . The main issues which were named as to be further addressed by legislation were improving of individuals protection, transparency of obligation of data controllers and processors and establishing of coherent, unified legislation across the Europe. In 2012 the European Commission proposed a comprehensive legislative reform in the field of data protection by recognizing of a necessity of a new regulation – the General Data Protection Regulation – a game changer which introduced a single set of rules on data protection, valid across the EU, set up a cross-border application of the EU regulation and determined higher penalties for non-compliance. 
Among other things, the General Data Protection Regulation , which was adopted on 24 May 2016 and is applicable as of 25 May 2018, elaborated on regulation of automated individual decision making. Not only the language of the notion of automated decision making was developed, but also the scope of regulation was amended by the GDPR comparing to the Data Protection Directive of 1995. The newly introduced Article 22 of the GDPR “Automated individual decision-making, including profiling” states:
“The data subject shall have the right not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her.”
The GDPR articulates several wordings with respect to the notion of “automated”. Article 2(1) defines the applicability of the regulation to “the processing of personal data wholly or partly by automated means…”. Article 4(4) refers to profiling which “means any form of automated processing of personal data…”. Article 13(2) introduced “automated decision-making”. All these wordings may be faced through the whole text of the GDPR, including its recitals, but the GDPR does not introduce the definition of neither of these formulations.
However, the GDPR addresses the notion of profiling and defines it as follows:
‘profiling’ means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person's performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements.
The Guidelines on Automated individual decision-making and Profiling for the purposes of Regulation 2016/679  of Article 29 Data Protection Working Party shed some light on the definition of automated decision-making and the scope of article 22 of the GDPR. According to the WP29 Guidelines “solely automated decision-making is the ability to make decisions by technological means without human involvement” and “automated decisions can be made with or without profiling; profiling can take place without making automated decisions”. 
Such a wording of article 22 of the GDPR expands the scope of the applicability of the regulation comparing to that of article 15 of the DPD. The lawmakers acknowledges that automated decision-making may or may not involve profiling as well profiling may not necessarily lead to a decision, meaning that the profiling is not the purpose of the automated processing, rather than profiling constitutes a type of automated processing which may or may not result in the decision regarding a data subject. 
From our point of view, such extensive construction of article 22 of the GDPR allows to consider those situations which fell out of the scope of article 15 of the DPD, but undoubtedly could affect data subject in the sense recognized by the purpose of this general prohibition of automated decision-making regarding a data subject. It may be understood that article 22 of the GDPR is applicable only to the decisions based solely on automated processing and such processing may include profiling, yet those decisions that are not based solely on automated processing are still required to comply with the GDPR. 

1.2.	“Based solely” on automatic decisions 

The WP29 Guidelines also pave understanding of the term “based solely” on automated processing by specifying that the scope of the general prohibition introduced in article 22 of the GDPR should cover all decision which does not provide the meaningful human intervention , or in other words, in which human involvement is not more than token gesture. 
Decisions based solely on the automated processing means that there is no human involvement in the decision, or there is no human influence on the outcome of the automated processing. The WP29 came up with the example that if a human reviews and assesses other factors than the result of the automated processing in the decision making, then the resulted decision would not be considered as based solely on automated processing. The UK Information Commissioner’s Office additionally explained that the intervention of a human being at the input stage, on the contrary, will not be sufficient for the decision to comply with the criteria of presence of human involvement in the decision making if the decision itself is made only by an automated system . Consequently, to fall into the scope of the article 22 of the GDPR decision shall be made with no ex post meaningful human participation. 
Consideration might be given to the fact that a lot of applications used in the decision making are not purely automated but constitute decision support systems  - systems that aims to aid or assist in the human decision-making rather that make the decision itself - and therefore the provisions of article 22 of the GDPR might not be applicable to such cases. The reality, however, is not as clear-cut as it may meet the eye. 
According to the WP29 Guidelines the presence of a human operator per se does not constitute human involvement necessary to qualify a decision as non-automated. The WP29 clarified that to be recognized as not based solely on the automated processing decision shall be assessed by a human being who has a real power and competence to overcome the algorithmic output. This means that human oversight is presented when a human decision-maker does not blindly agree to automatic decision-maker. 
In practical terms, it may be stated that wherever the automated processing appears, being it decision support system or purely automated decision-making system, if the human involvement does not include the assessment of other sources of data relating to a certain case and any actual influence on the outcome of ADM, the adopted decision may be considered as automated and consequently fall under the provision of article 22 of the GDPR. This means that data subject may rely on its right not to be subject to a decision based solely on automated processing even if the human being is presented but only as a silent implementor of algorithmic output. 
On the other side this directly leads to the question of how the meaningfulness of human oversight may be verified and guaranteed. This work will try to contribute to the examination of this matter when addresses the data protection impact assessment (DPIA) as it seems obviously that not only ADMS, but also human in charge of application of the corresponding ADMS shall be assessed. The concept of meaningfulness as well as the scope of the DPIA will be addressed further in the Chapter VII.

1.3. Level of automatization 

Another debatable matter is whether the level of automatization of the particular stage of the processing and decision-making influences the applicability of article 22 of the GDPR. Lee A. Bygrave commented that the automation shall relate to the final stage, whereas the initial elaboration, such as collecting data, can be carried out including in manual or semiautomated manner . The issue of locating of a decision is also considered by other experts stating that attribution of “automation” criteria to the final stage of the decision-making process only may generate adverse loophole for excluding of de facto automated decisions even in case of presence of a human decision-maker at a final stage, e.g., in such cases where the automated outcome even being transferred to a human already implies a sort of predetermined alternatives to be chosen among by a human decision-maker  . The recent case was brought to the Court of Justice of the European Union by the Administrative Court of Wiesbaden (Germany) to clarify whether credit scoring calculation itself by credit agencies would fall within the scope of Article 22(1) GDPR .[to update with the decision in due time]
 The above notwithstanding, as it is mentioned in the WP29 Guidelines and is being discussed in the literature, the controller, in terms of the GDPR, shall ensure that the human operator shall consider all source of data, which, in our opinion means that to be considered as non-automated decision, human operator shall revisit and evaluate any information and circumstances of the case before rejecting or confirming the automated output. Furthermore, it may be stated that if the human operator relies on several sources of information to be considered for the purpose of making a decision in respect to a data subject, but all of them constitute automated processing, and no meaningful assessment is made by such a human operator, meaning that a human operator solely choose between output of various ADMS, the resulted decision will be considered as subject matter of Article 22(1) of the GDPR . 
The ambiguities produced by the nature of automated decision-making tools and possible stratification of the automated decision-making process generate the room for loopholes for (non)applicability of Article 22(1) and, in practical terms, require profound understanding of decision locus, i.e., determination of the specific site for human intervenor to contribute to a final decision which may result even in several iterations of human input at different stages of overall decision-making process. We reckon that these ambiguities and step-by-step guidance to determine the “roadmap” for human presence and its scope shall be addressed rather by data controller by way of establishing of organizational measures and data protection impact assessment than by withdrawing of one-size-fits-all interpretation of the legislative provisions of Article 22(1) of the GDPR. Nonetheless, a general criterion could be advanced since now and it is “sufficiency” or meaningfulness: human intervention as duly informed, relevant, and capable of changing the final decision. This can be settled case-by-case in DPIA. 
We have already mentioned the limits in automatization of the decision making and came to the scope of the notion of meaningfulness of human intervention in ADM as the core concept of legislator in terms of safeguarding of the rights of data subject, but before addressing this point it is sensible to fully describe the conditions of applicability of article 22 of the GDPR. Therefore, we want to consider the scope of automated decisions covered by this regulative provision.





1.4.	Legal or similarly significant effect on a data subject

The GDPR states that automated decisions which generally are prohibited  by the General Data Protection Directive shall produce legal or similarly significant effect on a data subject. Consequently, we need to describe what constitutes such effects to address the scope of the regulatory provision. The GDPR itself does not give any definition to the term legal or similarly significant effect and legal uncertainty may arise with respect hereto. Meanwhile, Recital 71, even not being a binding clause, provides some examples of such possible effects: automatic refusal of an online credit application or e-recruiting practices.
At the same time, the WP29 has introduce far more wide explanations in this respect. According with its Guidelines on Automated individual decision-making and Profiling legal effect is presented when legal rights, legal status or rights under the contract are somehow affected. The following examples of the above are given:
•	the freedom to associate with others, 
•	the right to vote in an election, 
•	the right to take legal action,
•	cancellation of a contract,
•	entitlement to or denial of a particular social benefit granted by law, such as child or housing benefit,
•	refused admission to a country or denial of citizenship.
The GDPR introduced the concept of similarly significant effects comparing to the wording of the Data Protection Directive 95/46/EC which foresaw only significant effects. The word similarly referred to the legal effect, which means that the degree of the significant effect must be equal or comparable to legal effects. 
In this sense, two examples contained in the Recital 71 of the GDPR may be useful for understanding of the scope of similarly significant effects: automatic refusal of an online credit application or e-recruiting practices. Th WP29 stated that the level of the significance of the produced effects shall be equitable to legal ones and provided additional criteria to rely on when assessing the applicability of article 22 of the GDPR:
•	significantly affect the circumstances, behaviour or choices of the individuals concerned,
•	have a prolonged or permanent impact on the data subject, or
•	at its most extreme, lead to the exclusion or discrimination of individuals.
It is noteworthy that the WP29, even though considers online advertising which relies on automated individual decision-making or profiling as not producing similarly significant effect on a data subject, provides certain characteristics  that, as the case may be, trigger the applicability of the provision of article 22 of the GDPR. The European governors acknowledge the threats which may be delivered by the algorithmic techniques to the consumers  and on 27 September 2021, European Parliament Committee on the Internal Market and Consumer Protection held a Public Hearing on "Consumer protection and automated decision-making tools in a modern economy"  where such issues as prices differentiation, unfair discrimination, manipulation and impact on consumers’ autonomy were raised. As for the time of writing of this work the industry experts participated in the Public Hearing consider consumer protection laws of the European Union as insufficiently developed to protect consumers from the threats brought by the ADM, neither the GDPR is deemed to be of the appropriate level of consumer protection which we will further address in Chapter 6 of this work.  
Moreover, having acknowledged the complexity of the determination of the threshold of significance of other effects eligible for recognition as similar to that of the legal effects, the WP29 provides more examples of the decisions which may be subject to the prohibition of article 22 of the GDPR:
•	decisions that affect someone’s financial circumstances, such as their eligibility to credit;
•	decisions that affect someone’s access to health services;
•	decisions that deny someone an employment opportunity or put them at a serious disadvantage;
•	decisions that affect someone’s access to education, for example university admissions.
The UK Information Commissioner’s Office makes an interesting remark by providing the illustration of a possible decision which may result in both legal and similarly significant effects at the same time , namely, it is stated that in particular situations decisions which have little impact generally could have a significant effect for more vulnerable individuals. 
Obviously, such a language of article 22 of the GDPR regarding decisions that produce legal effects or similarly significantly affect a data subject illustrated by the certain examples in the explanatory form such as Recital 71 of the GDPR or the Guidelines on Automated Individual Decision-Making and Profiling of the WP29 demonstrates that there is no one-size-fits-all approach to the definition of the scope of the possible decisions which may fall into the application of this regulatory provision.
Another discussion presented by the experts’ concerns locating of the “significance” of the decisions. Hence, Binn and Veale introduced two approaches to determine when significance of the decision may be affirmed: 1) if an outcome has a potential to be significant or 2) once a significant outcome is realised  . They reasoned the possibility of interpretation of the GDPR to be read according with both suggested concepts by relying on the right to obtain information and the right to access as for the “potential” approach and by referring to safeguards provided in article 22(3) of the GDPR (to obtain human intervention, to express a point of view and to contest a decision) in case of “realisation” approach. Furthermore, they appealed to the basis of data processing, being a legitimate interest, if no significant decision was made, or data subject’s consent, in case of data processing led to such a decision, which neither is feasible (as they noted merely any data subject would consent to a be subject to a negative decision), nor legitimate (as the WP29 explained that data controllers may not change the basis during the data processing has been already presented ). We agree that the potential approach is more desirable due to vulnerability of the data subjects and provides for better protection, which, in turn, may impose more obligations on the data controllers. Considering that the data subjects’ rights under Articles 13(2)(f), 14(2)(g) of the GDPR shall be granted at the time when personal data are obtained it only make sense that data controllers foresee the possibility of an automated decision to have a significant nature. However, it is our opinion that this only affects the practical mechanisms employed by a certain data controller when choosing for its operations model, e.g., opting for a certain basis of data processing or practical implementation of legislative safeguards, i.e., this affects compliance with Articles 6, 13(2)(f) and14(2)(g), but does not impact applicability of Article 22. We consider that the differentiation in placing of the “significance” criteria for the automated decision to fall into the scope of Article 22 of the GDPR is not relevant for the matter of human intervention, inasmuch as once a significant automated decision is made it shall be subject to a human review, or in case of derogations from Article 22(1) a human intervention may be requested by a concerned data subject.

1.5. “Decision”

One more question to consider is the term decision itself. The GDPR is silent on defining the term decision for the purpose of regulation. However, Recital 71 pointed out that decision may include measure which is also has not received any definition. In the literature  the term decision is considered in a broad sense and shall not require certain formalization to fall into the scope of the regulation. One of the possible approaches to define a decision is that suggested by Isak Mendoza and Lee A. Bygrave . According to them “the condition that a decision is made means essentially that a particular attitude or stance is taken towards a person and this attitude/stance has some degree of binding effect in the sense that it is likely to be acted upon.” 
It may be concluded that under article 22 of the GDPR decision is not a specific formalized act, neither it should be taken by the authorities only, instead a decision-maker may be represented by the natural or legal person, public authority, agency or other body, following the provision of articles 4(7) and 4(8) of the GDPR, and, consequently article 22 of the GDPR may be applicable to decision made in any sector whether public or private and within any sphere, from banking and insurance to education and justice. 
It may be seen in the Second European AI Alliance Assembly Event Report from 9th of October 2020  that European policy towards artificial intelligence application is still an open issue and sector-specific approach to the regulation of AI is being discussed. In context of personal data regulation, considering the above conclusion of possibility of appearance of decisions based solely on automated processing which produce legal or similarly significant effects, which are obviously is of a high risk, on data subject in any sector we share the opinion that sector-based regulation may not be adequate for appropriate protection of data subjects . Hence, the horizontal regulation applicable in any sector, but in turn considering the level of risk generated by usage of automated decision-making system, which is enacted in the General Data Protection Regulation, seems to provide more pertinent protection to individuals. 
Further we want to address the type of data which may be used in automated processing resulting in a decision with legal or similarly significant effects which may influence the applicability of article 22 of the GDPR to a particular case.
At the first glance, it might be thought that as the article 22 of the GDPR referred only to the cases where a data subject was assessed and consequently the decision was provoked on the basis of automated processing, including profiling, related to the data of the same data subject. However, neither the definition of processing, nor the definition of profiling (article 4(2) and 4(4) of the GDPR, accordingly) contains the statement that such a process shall operate with the particular personal data belonging to the particular data subject. Therefore, article 22 of the GDPR shall be also applied to such situations when the decision was made on the basis of automated processing, including profiling, of any third-party personal data (meeting the other requirements of the present article) .

1.6. Nature of data: processing and profiling

Another controversial issue here is nature of data, or such characteristic as “affiliation” with a particular person. The GDPR provides for “the data subject shall have the right not to be subject to a decision based solely on automated processing, including profiling…”. The point we want to reveal here is processing, including profiling. In its article 4 the GDPR expressly define the terms “processing” and “profiling”, accordingly article 4(2) states:
‘processing’ means any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction;
and article 4(4) states:
‘profiling’ means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person’s performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements.
Thus, the GDPR specifically identifies that processing is operation or set of operations which is performed on personal data or sets of personal data and profiling constitutes any form of automated processing of personal data with the specific purposes. 
From our point of view, it is necessary to strengthen that the wording of article 22 of the GDPR does not provide for the exhaustive list of forms of the processing, rather than refers to “automated processing, including profiling” which means that other form of processing shall fall into the scope of this provision, e.g. pseudonymisation. 
Pseudonymisation is defined in article 4(5) of the GDPR as follows:
‘pseudonymisation’ means the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person;
Processing in form of pseudonymisation enables the technical “clearance” of data from the personal attribution to a particular data subject, but nonetheless is based on personal data and is still deemed as personal data for the purpose of regulation.  Contrary to pseudonymization anonymization is a technique which allows to remove personal data; thus, it is no longer possible to establish a connection between the data and identified or identifiable natural person . Therefore, such anonymous information shall not fall into the scope of regulation under the GDPR. 
It has been argued that automated decision making under article 22 of the GDPR may result from processing of any kind of data including non-personal data . This statement may be based on the explanations of the WP29 which states that automated decision making can be based on any type of data and provides the following examples:
•	data provided directly by the individuals concerned;
•	data observed about the individuals;
•	derived or inferred data such as a profile of the individual that has already been created.
The same is referred by the UK Information Commissioner’s Office in its explanation of what automated decision-making is . 
However, we consider that the provided above explanations and given examples relate to the type of data on the criterion of data source or means of obtaining, i.e., data may be received in the result of data subject actions such as, for example, filling out a questionnaire, or without data subject actions: as a result of observing an individual or as a result of development or inference of data, for instance, credit score. 
Obviously, automated decision making may be carried out on any data, including personal and non-personal data. But does any decision resulted from such processing fall into the scope of article 22 of the GDPR? From our point o view, the answer is no. In its article 4 the GDPR envisaged the definitions of processing and profiling, which expressly provides for personal data (or sets of personal data) to be the basis of operations or set of operations which constitute processing and profiling, as a form of processing. 
Siarhei Varankevich suggested  the following diagram to explain the scope of applicability of article 22 of the GDPR, which demonstrates that only decisions producing legal and similarly significant effects, and which are based solely on automated processing of personal data shall be considered as a subject matter of this proposition of law:
 
We want to suggest our graphical interpretation of the scope of article 22 of the GDPR reflecting our vision that automated decision-making may be carried out on whatever data, including non-personal, yet decisions based solely on automated processing shall involve only those which resulted from the processing of personal data, including those of other data subject as we mentioned above, and, moreover, article 22 shall apply on to that part of such decisions that produce legal effects or similarly significantly affects data subject:
 
The analysis of the scope of the applicability of the general prohibition of the decisions based solely on automated processing, including profiling, which produce legal effects or that similarly significantly affect a data subject implies that the provision of the article 22(1) of the GDPR shall not be deemed as rigid and each situation must be considered on a case-by-case basis in terms of relevancy of the above regulatory provision. Here, one of the most important, from our point of view, focus for consideration is the extent of human involvement in each case as this is the vaguest criterion for applicability of the regulation. Summing up the above discussions for ease of reference we present the following table:
Decision	Any act or approach towards data subject without any formal requirements to the form, content, or designated body, which may be made at any stage of data processing
Based solely on automated processing	“solely” refers to the decision made only by automated means without regard to the type of the employed techniques (decision support system, automated decision-making system), the qualifying criteria for the decision being not based solely on automated processing is extent of human intervention.
Human intervention	Meaningful human oversight by a human operator who has authority and competences to override automated decision along with considering other sources of information about data subject concerned.  
Processing	any operation or set of operations which is performed on personal data or on sets of personal data in any form, including profiling. For the purpose of article 22 of the GDPR the processed personal data resulted in a decision may belong to third-party data subject, but non-personal data should be excluded. 
Legal and similarly significant effects	-	effects to legal rights, legal status, rights under a contract;
-	significant effects, similar to threshold of legal effects, i.e. which produce huge impact on data subject’s life, e.g. access to health services, employment opportunities, access to education, access to financial instruments 


2.	Automated decision-making: regulation

2.1. Right or prohibition

There is another discussion born by the wording of article 22 (1) of the GDPR which states that (bold added) “the data subject shall have the right not to be subject to a decision…”. The employed language has created the ambiguity whether the provision is a right or a prohibition. There are some authors that consider the provision of article 22 (1) as a right . This opinion may be justified by the literal approach as the legislators directly chose the word “right” when drafting the GDPR . Obviously, such interpretation of the article 22 (1) would have require the enhanced actions by the data subjects, therefore, there would have been no “by default” protection of data subjects against automated decision-making. Contrary to this the WP29 n the WP29 Guidelines on Automated individual decision-making and Profiling  states that:
“The term “right” in the provision does not mean that Article 22(1) applies only when actively invoked by the data subject. Article 22(1) establishes a general prohibition for decision-making based solely on automated processing. This prohibition applies whether or not the data subject takes action regarding the processing of their personal data”.
This argument may be supported by reading article 22 (1) in conjunction with Recital 71 of the GDPR which is contrasting the exceptions under article 22 (2) which permits certain derogations when automated decision-making may be employed to the general rule of article 22 (1) which should not allow individual automated decisions. Such a framing of article 22 (1) as a general prohibition seems to be more aligned with the overall concept of data subjects’ protection in the European Union concerned about human-centric treatment of the automated decision-making means inasmuch as it excludes the possibility of the data subject been unknowingly adversely affected by the automated tools.  
It may be argued that prohibitive character of article 22 (1) allows for a higher level of ex ante control over the data processing by data subjects, and consequently better data protection . Such opinion may be reasoned by the fact that data subjects by default shall not be affected by automated decisions without a necessity to invoke any action on their part, however, it is our opinion that this notwithstanding the employed wording of article 22 (1) does not guarantee the better quality of a decision regarding a data subject as it is only contraposing human decision-making to automated decision-making without data subjects’ interest in overall decision-making quality being placed in the loop. We will revert to this issue further in this chapter while appealing to article 22 (1) as a general prohibition throughout this work. 

2.2. Additional conditions

We reckon as necessary to observe and analyse the additional conditions enacted in the regulation to explore the scope of (non) applicability of and provided safeguards for the automated individual decision making. First of all, we want to pay attention to the exceptions from article 22 (1) of the GDPR which states the following:
“22.2. Paragraph 1 shall not apply if the decision:
a)	is necessary for entering into, or performance of, a contract between the data subject and a data controller;
b)	is authorised by Union or Member State law to which the controller is subject and which also lays down suitable measures to safeguard the data subject’s rights and freedoms and legitimate interests; or
c)	is based on the data subject’s explicit consent.”
It is noteworthy that for the exceptions contained in sub articles 22.2 a) and c) the legislators foresaw  the minimal safeguards in form of human intervention on the part of the controller, and the right to express data subject’s point of view and to contest the decision on the part of the data subject. As for sub article 22.2 b) the suitable measure to safeguard the data subject’s rights and freedoms and legitimate interests shall be enacted by applicable Union or Member States law, and therefore, theoretically may differ from those prescribed by the General Data Protection Directive.
Comparing to the derogations from the general rule of article 15.2 of the DPD, the GDPR introduced possibility of usage of automated processing in case the explicit consent of data subject was received. From a practical standpoint the GDPR’s novelty of consent may weaken the protection of data subject’s rights and legitimate interest as notwithstanding the concept of explicitness, obviously, imbalance of power in relations between data subject and controller may be presented and the former may prove to be more vulnerable. Moreover, the real ability to understand the processing activity in case of automated processing due to its complexity and opacity may approach zero. 
Though, what is of interest for the purpose of this work is the way the legislators address the safeguarding measures granted to data subject if decision based on solely automated processing legitimately appears. The former regulation permitted automated decisions when permitted by law or in course of entering or performing a contract and in both cases stipulated that “suitable measures to safeguard data subject´s legitimate interests” should be taken, as an example of such measures the right to present data subject´s point of view was given. Certainly, it may be assumed that controller was able to employ other measure of its choice coequal to that foreseen in the DPD, but it impossible to know that due to the absence of ample case law. 
In contrast, the General Data Protection Regulation envisaged at least three safeguards in its article 22.3 for the automated individual decision making: the right to obtain human intervention on the part of the controller, the right to express data subject’s point of view and the right to contest the decision. As a key point of these legislative safeguards the WP29 highlighted human intervention stressing out that human participant shall be in a capacity of and have authority to review the automated decision. We want to underline that in case of safeguard the right to obtain human intervention stipulates action  on part of the data subject affected by already taken decision based solely on automated processing. Thus, this may be contrasted to the human intervention by default which shall be presented in any individual decision making which is not subject to the derogations from article 22.1 of the GDPR and in which case data subject is not required to take any action independently.
Another expansion in regulation presented in the GDPR comparing to the DPD is that special categories of personal data such as racial or ethnic origin, political opinions, genetic and biometric data, data concerning health and sexual life may not serve as a basis for automated individual decision making. However, this rule may be bypassed in case of receipt of data subject’s explicit consent or if the processing is triggered by reasons of public interest on the basis of Union or Member State law along with suitable measures to safeguard data subject’s rights and freedoms and legitimate interests .Presumably, the minimal scope of such safeguards shall not be less than that envisaged in article 22.3 of the GDPR, which means that the right to obtain human intervention on the part of controller continues to remain an important pillar of maintaining the protection of data subject. 

	2.4. Concluding remarks on regulation

Gathering all the conclusions from the observing of the structure of article 22 and Recital 71 of the GDPR and the Guidelines on Automated individual decision-making and Profiling of the Article 29 Data Protection Working Party it may be stated that human intervention serves as qualifying factor in the employment of automated decision-making systems. In case of prohibition of decisions based on solely automated processing, including profiling, which produce legal effects or similarly significantly affect data subject it is vital to define if and in what extent human involvement was presented to determine whether a particular use case falls into the scope of the applicability of article 22.1. In the event that exceptions provided by the GDPR are triggered and therefore, automated individual decision making is legally permitted, the right to obtain human intervention on the part of the controller serves as a key legislative safeguard for data subject’s rights.
In our opinion, introducing of the notion of human intervention in the current data protection regulation as well as provision of criterion of “meaningfulness” with respect to such intervention both in its capacity of safeguard as well as qualification to bypass the condition of solely automated processing presented in the authoritative guidelines of the WP29 is one of the major innovations in terms of regulation of automated decision making. Although the Data Protection Directive prohibited decisions based solely on automated processing which produce legal or similarly significant effects on data subject it did not provide for such a safeguard neither use such a criterion for separation of solely automated processing from other form of processing. The questions are why the law makers shifted to the newly introduced concept of meaningful human intervention at the final stage of decision-making process as a cure for automated decision and whether such intervention may really respond this purpose and benefit data subject and society.

3.	Additional safeguards 

3.1. Data subjects’ rights

As we mentioned earlier the concept of (dis)trust towards algorithms shaped the current legislation and the wording of article 22 of the GDPR in terms of human intervention serves as a particular example for this. We suppose that the algorithm-specific group represents more obvious reasoning of distrust and, basically, concerns technological aspects of algorithms related to any stage of development and deployment of automated decision-making systems. 
Let us have a look at the very beginning of automated decision making, or not even to say to the antecedents of ADM – development of the system itself. The first issue we can meet there is the quality of data which is used to train the algorithms. The European Union considers data quality as one of the key elements of trustworthy AI  and is constantly working on improving of legislative framework considering data . The question of data quality and its criteria remains open especially nowadays when unprecedent amount of data is collected and became available including through such instruments as social networks and phone applications . From legal point of view, this matter shall be considered in terms of bias  presented in data and discriminatory effects which may follow. It is noteworthy that outcome discrimination may arise notwithstanding the procedural compliance of input data with equality law, or without usage of special categories of data . Although bias may arise at any stage of automated decision-making system’s lifecycle, it is obvious that if the input data is biased the whole system will be biased . 
Another threat of ADMS arisen at the stage of their deployment is the broadly discussed black box problem. The opacity borne by algorithms and, in particular, ADMS may bring uncertain risks to human rights and to the society not only by possible introduction of unintended bias, but also by untransparent reasoning and consequently, inability of efficient contest of the taken decision . The reason of such opacity, mainly, lay down in the technological domain as the employed automated means become more and more sophisticated, especially with the rise of deep learning techniques which result in incapability to understand or explain the model in charge of the decision making by stakeholders and developers of that very model alike. From legal perspective transparency of automated decision making may be seen from various angles. 
First of all, it is crucial to consider the less protected group – data subjects affected by automated individual decision making. Besides article 22 of the GDPR the legislators foresaw additional instruments for protection of data subjects in terms of transparency. Thus, the GDPR envisaged in its articles 13.2 (f), 14.2 (g), 15.1 (h) the right of data subject to receive information about the existence of automated decision-making… and at least in those cases, meaningful information about the logic involved, as well as the significance and the envisaged consequences of such processing for the data subject. Additionally, Recital 71 of the GDPR, although not being a binding rule, provides, at least explanatory, for right to obtain an explanation of the decision reached after assessment, however, the binding article 22 of the GDPR does not contain such a right. 
The design of the GDPR in terms of right to be informed or right to explanation gave birth to a heated discussion between legal scholars. Goodman and Flaxman, for instance, referred to the existence of right of explanation under the GDPR . However, according to Sandra Wachter, Brent Mittelstadt and Luciano Floridi the right of explanation with respect to automated decision-making barely exists, the above authors tend to consider the scope of information rights under the GDPR as a right to be informed provided for ex ante explanation which may be addressed only to so called system functionality, while ex post explanation [of a final decision] is not mandated by the regulation . 
Further, Selbst and Powles suggested that although the regulation does not literally provide for the term right of explanation what is of most importance is the actual scope of rights and not how they are named in the GDPR . They referred to a technical possibility of explanation of a final decision once the explainability of system functionality is reached and, therefore, the directly mandated right to obtain meaningful information about logic involved in automated decision making derived from articles 13-15 of the GDPR shall serve as an overall right to explanation with respect to ADM where time division (being ex ante or ex post) is impractical. Selbst and Powles made a point about flexible, or functional, interpretation of the legislative provisions regarding data subjects’ right to be informed and right to access which means that information which may be received under article 13-15 of the GDPR, at least, shall allow data subjects to exercise their rights under article 22 such as a right to contest automated decision, as the case may be. 
Then, Malgieri and Comandé came up with an idea of different layers of explanation: readability – data comprehension by individuals, legibility – comprehension and transparency to individuals, and explanation itself . They argue that the complex approach shall be employed for understanding of the scope of a right to be informed/a right to explanation and, while addressing the vulnerabilities of the wording of the GDPR with respect hereto, acknowledge a sort of right to explanation.
We share the opinion that the more important is the scope of rights data subject actually possesses rather than a naming of such rights and the necessity of compound and functional interpretation of the GDPR’s provisions. Thus, it shall be derived not only from articles 13-15 and 22, but also from Recital 71 and articles 5.1 and 12 of the GDPR. Article 5.1 (a) states that personal data shall be processed lawfully, fairly and in a transparent manner in relation to the data subject and article 12.1 provides that “the controller shall take appropriate measures to provide any information referred to in Articles 13 and 14 and any communication under Articles 15 to 22 and 34 relating to processing to the data subject in a concise, transparent, intelligible and easily accessible form, using clear and plain language…” . It is understandable from the WP29 Guidelines that individuals may not have enough knowledge or level of comprehension to fully understand the complexity of the used techniques, therefore, as a good practice recommendation the WP29 suggested not explaining the mathematical construct of the functionality of algorithms but providing the data subject with understandable to him/her information about, for example:
•	the categories of data that have been or will be used in the profiling or decision-making process;
•	why these categories are considered pertinent
•	how any profile used in the automated decision-making process is built, including any statistics used in the analysis;
•	why this profile is relevant to the automated decision-making process; and
•	how it is used for a decision concerning the data subject .
This explanation of the WP29 although does not constitute a legally binding provision directly pointed out what the authorities will treat as an appropriate conduct, of the controller and this includes provision of information to the data subject not only about existence or general description of the logic involved, but also how the employed techniques may influence a decision concerning the particular data subject, with respect to its duty to comply with the general principle of transparency of data processing and its obligations under article 12 and 14 of the GDPR owed to data subjects.
Further, the WP29 shreds light on the provisions of article 15.1 (h) which comparing to those of article 13.2 (f) and 14.2 (g) envisaged a right of the data subjects and not the obligations of the controllers. According to the WP29 Guidelines the data subject reserves the right to receive “information about the envisaged consequences of the processing, rather than an explanation of a particular decision”, provided that information obtained under article 15 of the GDPR shall be useful for the data subject, including when contests such a decision which brings us back to the functional approach of interpretation of data subjects’ rights to be informed and to access. However, articles 13-15 only refer to articles 22.1 and 22.4 when envisaging these data subjects’ rights and the question of their existence in case of derogations from article 22.1, i.e., contractual and consented as well as legally permitted by the Union of Member State law automated decision-making, may arise.
Nonetheless, as it has been already mentioned above article 22.3. of the GDPR stated the minimum list of safeguards in case of applicability of automated decision-making and despite the absence of direct reference to the right of explanation (or to obtain information, at least) reading together with Recital 71 of the GDPR and the WP29 Guidelines, which constitute authoritative instructions on how to understand and apply the binding provisions  of the regulation, it may be concluded that the same functional approach  is utilized. The WP29 constantly refers to Recital 71 to explore the scope of appropriate safeguards when automated decision-making is presented. Even considering that article 22.3 envisaged safeguarding measures in case of contractual (article 22.2 (a)) or consented (article 22.2 (c)) exceptions from the general prohibition of automated individual decision-making and as for the exception under article 22.2. (b) delegating the regulation of ADM to other Union or Member State law, the WP29 stipulates that for all cases of derogations the transparency principle is crucial and, moreover, such transparency (in form of obtaining information by the data subject about how and on what basis the decision was made) serves as guarantor of the data subject’s right to challenge the corresponding decision.
In summing up the deliberations, legislative provisions, and authoritative guidelines it may be said that the data subject is granted with the right to obtain meaningful information about automated decision-making, its techniques, its significance and consequences which permits the affected data subject to express his/her point of view and to contest the decision. It is noteworthy that the GDPR foresaw that the right of access under article 15 of the GDPR shall not adversely affect the rights or freedoms of others, including trade secrets or intellectual property and in particular the copyright protecting the software . This provision is represented by a non-binding Recital 63 of the GDPR and the WP29 also refers to the Recital while observing the data subject’s right of access. Being an explanatory note to the Regulation  the Recital itself does not constitute a binding rule, but it does not change the fact that intellectual property and trade secrets have received their own protection under the respective regulation .
The matter of potential conflict or prevalence of the certain regulation in this case is of particular attention as it may influence the transparency of automated decision making and result in weakening of data subject’s right to access as well as questioning practical feasibility of meaningful human intervention. Moreover, while a human reserves a right to be informed and to access, a controller has a reciprocal obligation to provide the legally required information, consequently the controller shall have a possibility, including from the legal point of view, to obtain necessary information to comply with its legislative duty. 
Notably, Directive (EU) 2016/943 (on trade secrets) as well as Directive (EU) 2019/790 (on copyright) contain mirroring non-binding provisions which consider that rights and obligations under the data protection regulation shall not be affected by these directives . Besides, the GDPR’s Recital 63 declares that reliance on trade secrecy or intellectual property protection shall not result in refusal to provide all information to the data subject and, in comparison with the respective recitals of the Trade Secrets and Copyright Directives, states that the data subject’s rights shall not adversely affect the rights or freedoms of others. Additionally, the Proposal for an Artificial Intelligence Act states that transparency obligations shall not disproportionately affect the intellectual property rights while only necessary minimum of information shall be provided for the purpose of transparency maintenance . 
While Recital 63 of the GDPR stands for the data subject’s rights exercise and the GDPR does not provide the similar statement for the cases when the access or information are required by the controller, Recitals 34 and 35 of the Trade Secrets Directive points out directly that rights and obligations under the data protection regulation shall not be affected and Recital 85 of the Copyright Directive envisages that “any processing of personal data under the Directive…must be in compliance with…[the GDPR]”, i.e., not only by guaranteeing the rights of data subjects, but also allowing the controllers to fulfil their obligations against those rights. Such a wording of the Trade Secrets and Copyright Directives’ Recitals may be useful for those situations when the controller is not the rightsholder in the employed automated decision-making tools or when the data processing is delegated to the third-party processor and consequently the controller may be withheld from providing of information necessary either for the controller’s obligation of informing the data subject or for the meaningful human intervention under article 22 of the GDPR. 
Notably, Recital 78 also addresses the developers of the automotive tools to be used in the data processing by encouraging to take into account the right to data protection when developing and designing such tools and to make sure that controllers and processors are able to fulfil their data protection obligations. As perennially pointed out although recitals do not constitute a binding regulation their content shall be considered, therefore, it may be said that the developers, including being a third party and neither controller nor processor shall not prevent the latter from the performance of their duties. 
Accordingly, the “non-prevalence” of the GDPR over the Trade Secrets Directive and the Copyright Directive, and vice versa, may be affirmed when considering the data subject’s rights, along with the preferential treatment of the GDPR for both enjoyment of the data subject’s rights and fulfilment of the controller’s obligations  aiming at control over decision-making process, its transparency and fairness

3.2. Obligations for data processors

Another suggested reason for distrust to algorithms is technical vulnerability of the employed automated decision-making systems, ranging from a notorious computer error to manipulation or misuse and cyberattacks. Obviously, the developers aim at error-free excellent performance of their products, but it would be unreasonable to assert that there is zero possibility of occurrence of an error of whatever nature or scale. Comparing to human errors it is deemed that, if present in algorithm, errors may cause disproportionate harm . Providing the character of the decisions regulated by article 22 of the GDPR (their legal or similarly significant effect), obviously, automated decision-making systems may be a valuable target for hackers and cyberattacks not only for the purpose of manipulating the outcome of ADMS usage, but also to gain unlawful access to the data contained. 
Although this constitutes more technological matter rather than legal, it is undoubtedly drawing the legislators’ attention. Security is one of the principles of data processing under article 5 of the GDPR . Article 35 of the GDPR expressly provides for establishing of appropriate technical and organizational measures to ensure a level of security appropriate to the risk by the controllers and specifies in its subsection 3 that adherence to an approved code of conduct…or an approved certification mechanism…may be used as an element by which to demonstrate compliance with these requirements.
Moreover, when observing the legal response to the overall employment of automated decision-making systems it should be noted that the General Data Protection Regulation requires by article 24 that “…the controller shall implement appropriate technical and organisational measures to ensure and to be able to demonstrate that processing is performed in accordance with the [GDPR]” and by article 25 that “…the controller shall, both at the time of the determination of the means for processing and at the time of the processing itself, implement appropriate technical and organisational measures…which are designed to implement data-protection principles…in an effective manner and to integrate the necessary safeguards into the processing in order to meet the requirements of this Regulation and protect the rights of data subjects” .These binding provisions are supported by the wording of Recital 71 of the GDPR which envisages that the controller shall ensure technical and organisational measures appropriate to personal data processing.
The scope of the mentioned appropriate technical and organisational measures, however, is not defined by the GDPR, which besides would not be useful while such a wording leaves some room for the flexibility of improvement of measures according to the development of the employed technologies and maturing of organizational practices. Nonetheless, articles 24, 25 and 32 of the GDPR make references to adherence to codes of conduct and certification  as a possible way to demonstrate the compliance with the legislative data protection requirements and article 24 explicitly provides for implementation of appropriate data protection policies by the controller. Similarly, Recital 78 of the GDPR provides some examples of such measures: minimising the processing of personal data, pseudonymising personal data, transparency with regard to the functions and processing of personal data, enabling the data subject to monitor the data processing, enabling the controller to create and improve security features.
Article 25 of the GDPR contains two sets of aspects which shall be noted for understanding of the controllers’ duties of data protection, including when automated decision making is presented. Firstly, it addresses timeline of measures: they should be implemented at the time of the determination of the means for processing and at the time of the processing. Secondly, the measures themselves shall be technical and organizational. From the point of view of human intervention being either a prerequisite under article 22.1 or a safeguard under article 22.3, and considering requirements of article 22 of the GDPR it may be said that the controller shall consider the possibility of human decision-maker presence beforehand as well as support this possibility throughout the whole lifecycle of the employed automotive tools bearing in mind that the exercise of the right to obtain human intervention as a safeguarding measure may appear with a time lag. From this perspective, obviously, it is crucial to involve the lifetime of result of the processing (existence of the made decision) into the scope of time of the processing. 
As for the types of measures the legislator refers to its technical and organizational nature which means that along with the requirements to the technical side of the processing appropriate conditions of implementation  of the employed processing tools shall be evidenced. We consider that technical measures play supportive role in case of human intervention, i.e., serve to provide a possibility for a human decision maker to assess the reliability of the automotive output. Level of such reliability may vary depending on technical characteristics of employed tools, such as source and quality of data (datasets), transparency, and consequently better reasoning, of the overall mechanics, their security . From our point of view human decision maker shall consider the imposed technical measures while weighting the results of automated decision making.
However, in case of human oversight presence it seems more crucial to establish organizational measures. First of all, as human intervention itself may constitute a part of these measures . Secondly, organizational measures may establish criteria for adequacy and meaningfulness of human oversight and evidence the compliance of this oversight with legislative requirements. What we consider as important component of organizational measures, although not well addressed in the literature, is eligibility of human in charge of (non)implementation of the automated decision. For the purpose of human decision maker assessment, we assume the next list of possible questions to consider:
•	what previous experience of decision-making does the human decision maker (HDM) have, if any?
•	what factors does the HDM rely on when deciding?
•	were there any previous unfair/discriminatory decisions made by the HDM?
•	to what extent the HDM is capable to independent decision making, including to challenge decisions made by other decision makers, inter alia, automated?
•	is the HDM qualified enough, including from technical point of view, to work with automated decision-making systems?
•	does the HDM possess adequate competence and authority to override or to halt automated decision?
•	considering past decisions, has the HDM a tendency to only agree or, oppositely, to only reject automated decisions? Was it reasonably?
•	does the HDM consider sources of information other than the automotive output when deciding? What are these sources?
•	does the HDM understand the consequences of the made decisions?
•	does the HDM provide adequate information about the decision making and the resulted decision to the affected data subject, including to allow the data subject to contest the decision?
Furthermore, the lawmakers envisaged the data protection impact assessment obligations for the controllers. Article 35.1 of the GDPR states as follows:
“Where a type of processing in particular using new technologies, and taking into account the nature, scope, context and purposes of the processing, is likely to result in a high risk to the rights and freedoms of natural persons, the controller shall, prior to the processing, carry out an assessment of the impact of the envisaged processing operations on the protection of personal data. A single assessment may address a set of similar processing operations that present similar high risks.”
The WP29 considers the data protection impact assessment as a key accountability tool in automated decision-making, including profiling . We will further address in detail the DPIA as an instrument of the controller for evaluation of adequacy of both: the employed ADMS and human decision makers in Chapter 7 of this work. However, it is worth to say that we consider that the provisions of this Article 35 are of most importance to ensure data subjects´ protection vis-à-vis automated decision making. We share the opinion of Kaminski and Malgieri  that the DPIA plays crucial role in the assessment and guaranteeing of meaningful exercise of individual rights under Chapter 3 of the GDPR and accountable fulfilment of data controllers´ and processors´ obligations under Chapter 4 of the GDPR. 
Thus, Article 35.7 of the GDPR states the following:
“The assessment shall contain at least:
a systematic description of the envisaged processing operations and the purposes of the processing, including, where applicable, the legitimate interest pursued by the controller;
an assessment of the necessity and proportionality of the processing operations in relation to the purposes;
an assessment of the risks to the rights and freedoms of data subjects referred to in paragraph 1; and
the measures envisaged to address the risks, including safeguards, security measures and mechanisms to ensure the protection of personal data and to demonstrate compliance with this Regulation taking into account the rights and legitimate interests of data subjects and other persons concerned.”
Here we would like to pay our attention to certain wording, including the last paragraph, of the above cited article. First, the legislators directly refer to the suggested scope of assessment as a required minimum, i.e., they leave the amplification of the matters to be assessed to the data controllers, providing in any case that the controllers shall seek prior advice of the data protection officer with regard to the DPIA . Further, in the obligatory minimum of Article 35.7 the legislators envisaged, among others, the necessity to assess [bold added] “the measures envisaged to address the risks, including safeguards, security measures and mechanisms to ensure the protection of personal data and to demonstrate compliance with this Regulation taking into account the rights and legitimate interests of data subjects and other persons concerned”. Notwithstanding the elaboration on the DPIA further in this work we consider relevant to address this wording in this Chapter inasmuch as it concerns both article 22.1 (the right not to be subject to a decision based solely on automated processing…) and article 22.3 (the right to obtain human intervention on the part of the controller, to express data subject’s point of view and to contest the decision as safeguards in case of permitted automated decision-making).
Thereby, the controllers while demonstrating their compliance with the GDPR may include the assessment of the human oversight of automated decision making, first, regarding the provisions of article 22.1, by establishing of certain criteria to the scope of human decision-making and to the human decision-maker herself, second, regarding the provisions of article 22.3, by envisaging of the tools for data subjects to express their point of view, collaborating with the human intervenor, certain scope of human intervention, including providing of meaningful information about the logic involved in the automated decision-making and criteria to the human intervenor herself. Providing that that the European legislators introduced human intervention and oversight as a vital safeguard to protect human rights and overall human dignity in case of automated decision-making and the amplification of the scope such human intervention made by the wording of the GDPR and the WP29 Guidelines to be meaningful we consider the assessment of the human intervenor and establishing of certain criteria to the human decision-making as crucial for the better protection of the concerned data subject. This may include the introduction of the above suggested organizational measures to evaluate the human decision-maker, the procedures to be undertaken by a human-in-the-loop to demonstrate his/her compliance with the established criteria, or a continuous professional development plan for the human decision-makers to guarantee the development of their competences and knowledge, including relative to the technological advancement and diversity trainings to reduce human bias. 
Notably, the General Data Protection Regulation envisages modes of voluntary tools for proof of adherence to legislative requirements and standards . Thus, article 40 of the GDPR provides for codes of conduct which are to be monitored by the supervisory authorities. The controller as well as the processors may adhere to such codes of conduct which may foresee expanded provisions of data protection requirements including technical safety, transparency, accountability, human oversight, and its criteria as well as requirements to the human decision makers. By adherence to such codes of conduct the controllers and the processors may demonstrate compliance with legislation as well as improve the transparency of the principles of operating automated decision-making systems and the role of the human decision-makers. The compliance with approved code of conducts shall also be considered as a part of data protection impact assessment techniques . 
The certification under article 42 of the GDPR, yet being also non-obligatory, constitutes an auxiliary instrument of the authorities to control the compliance of the data controllers and processors with the regulation and at the same time it may be even more useful for the controllers themselves as the preparation for certification may promptly identify vulnerabilities of the implemented measures and potential non-compliance with the regulation. In our opinion the certification may also involve the assessment and conformity even if not the particular human-decision-makers, at least the requirements to them established by the controllers.
Notwithstanding non-obligatory nature and yet undefined scope of the quality control in the form of code of conduct and certification, these provisions may serve as additional instruments to ensure better data subjects protection  not only in the face of automated decision making, but also against undesirable bias, discrimination or injustice of human intervenor combating both under- and over-reliance of human-in-the loop on the algorithmic output.   

4.Two perspectives and important aspects to consider 

4.1. Human data subject vs. automated decision-maker

As we concluded above in the first chapter of this work legal instruments worldwide address automated decision making in different ways. We want to restate our opinion that, for instance, the Convention 108+, albeit obviously inspired by the European data protection laws, seeks to empower data subject to effectively and profitably manage data processing related to them and the Explanatory report to the Convention 108+ affirms that individuals shall be enabled to control the processing of their personal data by others which, from our point of view, may include human operators involved in automated decision making. Thus, while observing the Convention 108+ scope of data subjects protection we offered the following schematic understanding of the language employed by the Council of Europe, which locates data subject’s first inquiry to overall decision-making process in between of automated and human decisional output:
 
Likewise, we concluded that US legislature, although being rather fragmented, concentrates on providing data subject with a fairer, non-discriminatory decision rather than contrasting automated and human decision-makers. Hence, the Due process provision foresees data subject’s her participation in the decision-making process, exercising of hearing rights and the right to have a hearing prior to adverse action, which, in our opinion, mean that the concerned individual shall be engaged in the decision-making process, express his or her point of view, present his or her arguments and evidence before any decision, including final human output, is made. 
However, as we discussed earlier in this chapter the European data protection regulation prevents data subject from facing the automated decisional outcome as a general rule by introducing Article 22 (1) of the GDPR that requires its human review at a final stage of decision-making. Therefore, on one hand data subject is in lack of capability to agree to the automated decisional outcome in case he/she considers such decision fair, or at least acceptable to him- or herself, and on the other hand this leaves room for possible human bias and discrimination to be introduced at sided with the human decision making. This notwithstanding the GDPR provides for certain derogations from a general prohibition of individual automated decision making as per Article 22 (2)  solely on the basis of which data subject may face automated decision of which the consented automated decision making is worthy of note in terms of possibility of yet human, but data subject, intervention in ADM by his or her choice.

4.2. Consent

We would like to address the controversial position of consent in case of automated decision-making, i.e., is it feasible and meaningful for data subject to provide consent to data controller and what are the criteria of such consent, if yes. Consent is one of six legal bases of data processing foreseen in the GDPR  which prescribes certain conditions with regard thereto: consent must be freely given, specific, informed and unambiguous . Article 22 (2) (c) states that automated decision making is possible if explicitly consented by data subject. The GDPR does not contain any specific definition of “explicit consent”, however, the WP29 provided some clarifications  that “the term explicit refers to the way consent is expressed by the data subject”, i.e., explicitness constitutes a formal criterion and does not affect the substantive content of the “consent” notion. Therefore, we will further appeal to the requirements to consent being validly given as provided in article 4 (11) of the GDPR although explored in terms of automated decision making. 
Article 6 (1) (a) states that the consent shall be given for one or more specific purpose which means that data subject shall have a right for purpose specification and shall be in capacity of opting for consent for one purpose and not to consent for another (so called “granularity” of consent) . The condition of unambiguity refers to a statement or to a clear affirmative act on the part of data subject while consenting to data processing . It is our opinion that these two requirements consider mechanics of obtaining of data subject consent, however, similarly to “explicitness” do not affect its substantive side. In turn, the criteria of consent to be freely given and informed do construct certain demands with respect thereto. 
As explained by the WP29 the criteria of “free” consent depend on several factors being imbalance of power (positioning of data subject vis-à-vis data controller), conditionality (“bundling” or “tying up” of consent to a performance of a contract), granularity (possibility to divide and to (not) consent to the certain purposes of data processing, if more than one), and detriment (possibility not to consent or to withdraw a consent without any negative consequences for data subject) . In the realm of automated decision making under Article 22 of the GDPR the most important, and the most controversial, from our point of view, questions to be addressed are imbalance of power and detriment which are also closely interrelated. 
Article 22 considers automated decisions which produce legal or similarly significant effects on data subject, as we described above areas where such decisions may be made vary and cannot be determined by one-stop-shop approach, although the emblematic examples provided by law as well as by authoritative guidance refers to employment, education, banking and insurance, social benefits appearing both in private and public sectors. Considering the above we want to appeal to the WP29 Guidelines on consent  which albeit foresee certain appropriate situations of validly given consent consider relations encountered toward public authorities or employers, which perfectly fall into the scope of applicability of article 22, are likely to result in imbalance of power due to the vulnerability of data subjects in face of more potent counterparty, and therefore, in inability of data subject to deny the provision of her consent. The imbalance of power may also arise in other domains  and such imbalance also may be evidenced due to the nature of consequences of automated decisions under Article 22 (legal or similarly significant effect of the ADM) which may pose data subject in the situation of fear and vulnerability confronted with data controllers responsible for the decisional outcome. 
This notwithstanding the WP29 explained that data subject’s consent is not necessarily invalid if given in the context of employment or in relation with public authorities, for instance, if it is guaranteed that if no consent is provided data subject will not be penalized, or if there is no risk of deception, intimidation, coercion, or significant negative consequences  which leads us directly to further WP29 explanation about detriment . The WP29 clarifies that under the detriment condition it is necessary to demonstrate that withdrawal of consent will not adversely affect data subject, e.g., no costs arise, no downgrade of services is envisaged, no punishment follows. From the data subject point of view, we could assume that in case of automated decision making, bearing in mind that it produces legal or similarly significant effects on data subject, data subject once again may be found vulnerable to, at least alleged, detriment as his/her position vis-à-vis such decisions is deemed to be precarious. Although it is provided by the data protection regulation that the withdrawal of consent shall be possible at any time  vulnerable data subject may have not real capacity of control due to the fear of negative consequences. 
Other criteria of valid consent established by Article 4 (11) of the GDPR is a characteristic of being informed. This characteristic in the most disputable to be feasible considering the nature of automated decision making, meaning the sophistication of the employed techniques (such as deep learning). As stated by the WP29 “for consent to be informed, it is necessary to inform the data subject of certain elements that are crucial to make a choice” . As a minimum scope of information to be provided to data subject it refers to the data controller’s identity, the purpose of each data processing operation, what data will be collected and used, the existence of the right to withdraw consent, information about the use of the data for automated decision-making and on the possible risks of data transfers due to absence of an adequacy decision and of appropriate safeguards as described in Article 46. The WP29 also clearly states that such information shall be provided to data subject in a form (by employing the language) easily understandable for the average person, i.e., data subject shall be provided with the intelligible particularly to him/her information. 
Regarding consented automated decision making the WP29 refers to the information about the use of data for ADM. It is worth noting that obligation to provide information for the purpose of consent is somewhat separate from the data controller’s transparency obligations under the data subject’s right to be informed , therefore, it is hard to say whether it is enough to provide only information about logic involved in ADM. It may be argued that by consenting to ADM under article 22 (2) (c) data subject is placed in even more vulnerable position and consequently shall be granted with the broader scope of protection. Furthermore, information, including when provided for the purpose of consent, constitutes a part of transparency which is one of the fundamental principles of data processing envisaged by the GDPR . Therefore, it may be stated that the understanding of what data subject is consenting to is crucial to define the lawfulness of data processing, including automated decision making. Thus, the recent case brought to the Italian Supreme Court reaffirms that consent without transparency is legally worthless . The decision clearly states that the opacity of algorithms employed for the purpose of establishing of reputational ratings does not allow sufficient understanding of the elements of the underlying system by data subject and, consequently, consent may not be deemed as validly given. This does not necessarily mean that any and all automated decision-making systems fall out of the possibility to be used under data subject consent, otherwise, Article 22 (2) (c) would have no practical sense, however, data controllers when opting for data subject consent as a lawful base for data processing, including automated decision making, shall consider transparent and explainable techniques . 
We have observed above the requirements to the consent for the purpose of derogation under article 22 (2) (c) which allows data subject to face automated decision-making outcome. However, overall decision-making process does not finish at that stage as the data protection regulation provided data subject with additional instrument in case of consented ADM: at least the right to obtain human intervention on the part of the controller, to express data subject’s point of view and to contest the decision. Thus, in both cases (under Article 22 (1) and Article 22 (2) (c)) data subject has the right for human oversight of the algorithmic output with the only difference being data subject access to and, consequently, ability to agree with the algorithmic decision. Graphically the decision-making processes may be reflected as follows: 
-	under Article 22 (1)
 
-	under Article 22 (2) (c)
 
Probably, such data subject positioning between automated and human decision-makers could result in better control of data processing and in better (or satisfactory) decision by avoiding bias and discrimination introduced on the side of human decision-maker and by being an additional tool for control and understanding of scope of human input to preceding automated decision (whether and how human decision-maker challenges and weighs algorithmic input). However, even if the consent meets the legislative requirements, which remain elusive in the context of rapidly developing technologies, we see at least two impediments here. 
First, data subject´s consent serves as a lawful basis for data processing under Article 6 of the GDPR and as a ground for exempting automated decision making from the scope of Article 22 (1). This means that in case off opting for consent in its operating model data controller is obliged to receive valid consent from data subject. However, in turn, data subject has no right to request the acceptance of his/her consent, i.e., right to require automated decision making under Article 22 (2) (c) from data controller at his/her own discretion. We could assume that in case of consented automated decision making was a data subject’s right this would result in alternative for data subject to choose whether he/she wants to obtain human intervention first (Article 22 (1)) or prefers to receive automated decision and then, in case of dissatisfaction with it to exercise the rights under Article 22.3, including the right for human intervenor, which correspond more to the US Due process provisions and the Convention 108+. In such a situation of this hypothetical alternative data subject, arguably, might have more control over data processing and decisional outcome affected him/her as he/she would have a possibility of monitoring and intervening in the overall decision-making process at different phases and, at least weighing whether the automated decision is better for him/her. 
Here we want to move towards the second obstacle. Considering that the transparency requirements are met, valid data subject consent is given and even assuming that particular automated decision may be acceptable or satisfactory for data subject it will not necessarily guarantee that the decision itself is non-discriminatory or fair. For instance, a vulnerable data subject may accept a decision providing him with access to a loan or a particular social security benefit, but he/she would be in a position of inability to assess whether the amount of such loan, applicable interest rate, or type or amount of granted benefit is not discriminatory comparing to other decisions in that particular domain as a data subject would not have access to such decisions, or at least the permissible range of outcomes.
Moreover, such approach may introduce fractionization of overall decisions in particular domain affecting homogeneity of outcomes regarding certain category or type of circumstances which trigger the decision. In other words, data subjects by acceptance of a decision, satisfactory for him/her, but not necessarily fair in general, would form uneven decision-making practice which, in turn, may serve as a malicious precedent for further decisions for both socio-juridical, which is self-explanatory, as well as technological perspective when such a final decisional output is further used as input for automated decision-making system. It is out opinion that in case the above is presented not only individual rights will be affected, but it will also have potentially adverse effects at a larger scale, i.e., on a whole group of concerned individuals. 
Thus, we consider that providing data subject with a possibility to get acquainted with automated outcome and compare it to that of a human decision-maker may be a useful tool, including, for weighing the latter and preventing human bias therein. However, from the point of view of improvement, or at least of guaranteeing of lawful, non-discriminatory, and reliable character of decision-making in general such data subject acknowledgement per se does not allow for eliminating of a competent human supervising.     

5.	Meaningfulness

5.1.	Competency

As we have already observed human decision-maker appears in two possible situations foreseen by the GDPR: by default, under article 22.1, and as a safeguard when permitted automated decision-making (article 22.3) takes place. To serve as a wire and a guarantor of legitimate individual decision-making human intervenor is required to comply with certain criteria of meaningfulness of his/her input in decision making by introducing substantial oversight and having authority and competences to override automated decision . Dictated by algorithmic aversion demonstrated by the European regulator and by insufficient elaboration on preceding article 15 of the Data Protection Directive article 22 of the GDPR with its criteria of meaningfulness of human oversight was intended to ensure trust and stronger data protection in the domain of automated decision-making . 
According to the WP29 Guidelines human reviewer shall be capable and competent to change automated outcome, including by taking in account other available factors to make final decision. As we discussed earlier our opinion is that “final decision” does not necessarily refer to the final stage of decision-making process rather address completeness, exhaustiveness of a particular decision which may arise at a different stages of automated data processing for the purpose of decision-making. Therefore, it is crucial for human decision-maker understand when (the moment) and how (the reasoning) automated decision was made which leads us once again to the issue of explainability, interpretability and traceability of ADMS. We have already addressed the black-box problem and the issue of collision of intellectual property and data protection regulations from the perspective of information which is to be provided by the data controller to data subjects, and here we wish to highlight the matter of feasibility of understanding of ADM by human agent responsible for human oversight. At this point we want to highlight the issue of inadequate competences and training of human controllers of automated decision-making systems. 
There can be no denying of artificial intelligence tools are well established in our lives. Policymakers appeal to human primacy over the technologies and they refer to AI application as a supportive tool which only purpose is to serve human wellbeing. For instance, the High-Level Expert Group on AI points out that both technical and non-technical methods to improve AI trustworthiness shall be presented  and it introduces education and awareness to foster an ethical mind-set as one of non-technical methods. The European Commission’s White Paper on Artificial Intelligence also acknowledges the necessity of development of skills and competences to fit the transformation brought by AI and to “effectively and efficiently implement relevant rules” which may include the data protection regulation . The European Parliament has communicated the European Commission underlining the importance of fostering skills and education to work with AI and of ethicists receive proper training . 
Obviously, the policymakers are preoccupied with changes of labour market due to AI applications which may result in exclusion of human from certain tasks and decrease in workplaces , but, in our opinion, the understanding of inevitability of human operators to properly fit the AI operation may be evidenced as well. This means that humans shall possess appropriate qualification and competences to operate AI applications, including automated decision-making systems as well as improving digital skills may enhance netter understanding of the processes and outcomes of automated decision-making systems they may operate. In our opinion, lifelong learning approach in the sphere of AI technologies is crucial for the balance between technological development and protection of human rights as it provides for better human control and more thoughtful assessment of algorithmic outcome.
One of the issues which is connected to the operating of ADMS by human controller is proper implementation and usage of such systems according to how they were designed, i.e., when employing ADMS organizations cannot enjoy their full potential  . We share the opinion that this may happen due to the fact that the users are barely involved in design and deployment stages, however, we reckon that appropriate training of personnel may shorten this “deployment gap”. Besides covering the lack of skills in technological domain, in our opinion, it is of high importance to address the problem of both algorithm aversion and appreciation among human operators involved in automated decision making while training. In previous chapter we discussed some reasons of these two phenomena and we consider that profound explanations of what may (not) be delegated to ADMS, awareness of under- and overreliance on algorithms, preventing of ignoring of and, the other way round, blind agreeing to automated outcome, developing of certain level of independency regarding ADMS, incentivizing conscious collaboration in domain of ADMS and human decision-maker interaction  may constitute a part of personnel instruction which, in turn, may be implemented as an element of DPIA practices. 
Although human-in-the-loop was introduced and developed as an instrument to safeguard data subjects’ rights and freedoms there is a lot of concerns regarding its feasibility and usefulness: starting from limits in explanatory abilities of ADMS and automated decisions which are vital for understanding reasoning and causation and ending with realism and practical feasibility of human decision-maker to supervise automated decision-making and override its outcome. There are some issues concerning human operators’ capacity to intervene due to the organizational policies and fear to be punished for such intervention , although these problems may and shall be addressed and mitigated, in our opinion, by organizational measures prescribed by article 24 of the GDPR. The last but not the least concern with respect to human decision-makers is their own bias, unfairness and imperfection which may affect scrutinized data subjects. All this leads us to a question whether human intervention may still be considered as a key safeguard for data protection vis-à-vis automated means.

5.2.	Wrongness

At this point we want to appeal to the UK Taskforce on Innovation, Growth and Regulatory Reform (TIGRR) independent report as of 16th of June 2021  and the UK proposal of data protection reform introduced on 10th of September 2021 . The TIGRR, unprecedently for countries influenced by the European data protection regulation, in its report suggested removing the prohibition of automated decisions in terms of article 22 of the GDPR. The TIGRR claimed that this legislative provision imposes costly and unpractical burden on organisations using AI for decision-making . Further, it stressed out that even with involving of a human reviewer decisional output continues to be “wrong, unexplainable and biased” and appeals to the fact that automated decision-making practices are not allowed even if their use outperform human decision-making. Additionally, the TIGRR stated that consented and contractual derogations from the prohibition are not useful, and that consideration must be given to the expansion of permissible use of data, including for automated decision-making purpose. Also, the report suggested downsizing of the requirements regarding explanations to be provided by the data controllers to basic instead of complex ones and eliminating obligation to provide information about the logic involved in ADM. In turn, the TIGRR offered to concentrate on compliance with legitimacy and public interest test along with fairness, accountability, and transparency principles applicable to automated decision-making. It may be withdrawn from the report that the suggested changes were dictated by the aim of enhancement of improvement and proliferation of data usage and growth of competitive businesses including by reducing their costs associated with data protection regulation. 
The UK Department for Digital, Culture, Media and Sport (DCMS) in its Consultation document acknowledges the TIGGR suggestions about removing of article 22 provisions from the data protection regulation, seeking the views from stakeholders  . This notwithstanding the Consultation document stands on refraining from any proposals and looks for further “evidence on the potential need for legislative reform” . The Consultation document refers to uncertainty of operation and efficacy of Article 22 due to its limited application and lack of understanding of how and when it is intended to apply . As we noticed earlier the “when”-issue is broadly discussed between experts and scholars (e.g., the trigger for provision of information, the moment of human intervention), but this is our opinion that the “how”, especially with regards to human intervention, is not really defined save for the criteria of its meaningfulness. 
The UK ICO in their Response to the Consultation document, expressed their concerns about the TIGRR suggestion and provides for elaboration on transparency to provide human intervenor with “the information and skills they need to scrutinise the ways in which decisions are made” . Further, the Consultation document appeals to limited application of Article 22, e.g., falling out of its scope of partly automated decision or decisions based on non-personal data. The UK ICO contested suggesting extension of Article 22 to cover partly automated decisions recognizing that some decisions ay involve human being still significantly shaped by AI. They also offered a risk-based approach to the depth and nature of human reviews . 
It is our opinion that the UK authorities acknowledge, at least, inadequacy of legislation regarding automated decisions, specifically, in terms of human involvement and its scope. Both the DCMS and ICO recognize the necessity of data protection regulation keeping pace with development and growth of AI application in decision-making domain . It may be withdrawn that two main concerns are raised: transparency of ADMS, including to make human intervenor capable to scrutinize automated outcome, and human intervenor input, i.e., its quality, depth and nature, and conscious and unconscious biases presented in the human judgments . This returns us to the issue not only of meaningfulness of human review, but also to the requirements to be complied with by human operators while scrutinizing automated decisions. Additionally, we cannot agree to the UK ICO’s suggestion to expand the regulation on partly automated decisions as, in our, opinion it will impose unreasonable burden on organizations and contradicts to the nature of the Article 22 prohibition. The main object of remastering here is to elaborate on clarity of each component of the provision and define what human intervention suffice to recognize the decision to be based not solely on the automated data processing. Thus, we can assume that the better decision-making for the benefit of data subjects demand a union of trustworthy ADMS and trustworthy HDM. 

5.3.	Human-in-the-loop

It may be evidenced that the European data protection regulation considers human-in-the-loop as a vital antidote to threats brought by automated decision-making tools. The European authorities contrast capabilities of human to automotive systems assuming that the latter can bring much more harm to data subjects violating their fundamental human rights. The GDPR and the authoritative guidelines of the WP29 elaborates on previous wording of the regulation of automated decision making by adding the criteria of its meaningfulness. However, the stakeholders are concerned about the applicability and the scope of Article 22 generally prohibiting decisions based solely on automated processing which bears legal or similarly significant effects. 
Legal specialists and scholars are trying to withdraw a complex safeguarding matrix regarding automated decision making by considering data subjects’ right to obtain information, right to access, data controllers’ obligations to establish technical and organizational measures and executing of data protection impact assessment to secure better protection of data subject when facing automated decisions in addition to those safeguards provided in Article 22. The European legislators also introduced certain possibilities for data controllers, although voluntary, but evidentially recommended for demonstration of adherence to best practices in data protection domain, such as codes of conduct and certification.
The aim of the provided by the GDPR legal instruments is to build trust in disbalanced relations between vulnerable data subjects and data controllers in particular when employing automated decision-making systems. According to the data protection authorities the cornerstone of such trust lays in putting human in the loop and making that human meaningfully review and scrutinize automated decisions. However, a lot of obstacles with regard to human intervention may be evidenced, including, the lack of capacity of human agent to understand and contest automated income due to the black-boxed automated solutions, lack of digital skills across the society and own human biases. This affects the practical sense or feasibility of human intervention. 
Some authors suggested that the quality of data protection may be improved by implementing of dynamic approach to the implementing of legislative requirements within an ecosystem of data protection impact assessment . We agree to their statement on the DPIA practices being a continual process which, in this case, may ensure safeguarding of fundamental rights and interests of the individuals. Such approach may guarantee, or at least improve, algorithmic accountability and may include as well consistent and constant assessment of human intervenor. In our opinion, such conjunction of requirements to both automated and human decision makers may provide better protection to data subjects who are, as a matter of fact, vulnerable party of overall automated data processing and indeed should benefit from the automated decisions’ regulation. 
Although the GDPR foresaw different tools for data subjects’ protection in form of either individual rights or data controller’s obligation, or even voluntary instruments for the latter, i.e., certain delegations to market players allowing them to participate in the regulation and, therefore, data protection improvement, it is our opinion that such “collaborative governance” may not be considered as sufficient as it requires a high degree of conscious and well-developed and implemented ethics throughout data controllers and, in turn, the matter of ethics itself is still much-debated across the industries. Recognizing the pace of technological advancement, decision-making practices based on non-personal information or with regard to group of data subjects rather than to individuals and the European position on the necessity of keeping human in the loop of automated decision making we consider that the current state of legislation does not provide for adequate, trustworthy protection of individuals. We argue that human in the loop as it is represented in the legislation so far may not be longer considered as a sufficient safeguard for human fundamental rights and interests and should be elaborated from the point of view of human-in-command concept, i.e., individuals shall receive better protection and better decision-making from both algorithmic and human decision-makers and the requirements to data used for such decisions shall be addressed as well. 

Chapter III Artificial Intelligence Act: building multi-layered trust

1. Multi-layered risk approach

April 2021 in the European Union was marked by an important initiative of the European lawmakers – the Proposal for An Artificial Intelligence Act  (we will refer to this act throughout the work as to the Artificial Intelligence Act or AI Act for the purpose of convenience). Apart from the well-known and broadly discussed GDPR and its Article 22 this newly introduced regulation obviously will concern automated decision making to the certain extent. It may be said, from the point of view of this work, that the GDPR and the AI Act intersect regarding the legislative requirements to automated decision making, however, the perspective, subject matter, and the scope of regulation, obviously, differ. 
As we discussed above, Article 22 of the GDPR aimed at protecting data subjects vis-à-vis decision based solely on automated processing, yet the nature of automation of the data processing may vary. However, it is becoming increasingly evident that contemporary automated decision-making systems, as the overall trend in the technological advancements, is confining to artificial intelligence applications .
Thus, acknowledging the necessity of artificial intelligence development for the benefit of society and individuals the European legislators proposed the AI regulation to enhance safeguarding of the European values and to align technological progress with the fundamental human rights, including privacy and personal data protection . As the GDPR, the AI Act is a horizontal regulation which is to apply to whichever sector or industry with regard to AI systems . As we will see further in this Chapter, certain ADMS, in terms of Article 22 of the GDPR, may fall under the proposed regulation as well. So, what is the difference in these two regulatory approaches? How will the regulations interact? What is proposed by the European legislators for building trust to and stimulating developments in the AI domain?  
As expressly stated in the AI Act, its objective is the development of an ecosystem of trust with the help of a legal framework for trustworthy AI . The Act shall enhance confidence of AI-based solutions’ users. It may be said that with this regulation Europe is trying to reach balance between benefits which may provide accurate and robust AI systems and mitigation of its risks such as bias, opacity and unpredictability . Already deep-rooted concepts of human oversight and transparency are also reflected in the new regulation, which is designed to be proportionate, flexible, well-defined risk-based regulatory approach . 
The European Union opts for differentiating of certain risk levels for AI systems applicable for the defined purposes and, consequently, for varied legislative requirements thereto. Such risk-based distinction of the regulation shall strengthen development and usage of AI technologies together with alignment with fundamental rights and foster people trust therein. The AI Act states that the extent of the adverse impact produced by AI systems on the fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high-risk ; among other rights this includes protection of personal data which is of our interest. Notably, the overall concept of trust and trustworthiness resonates throughout the European opinions, resolutions and polices regarding AI domain . For this reason, the AI Act foresees the methodology of defining the risks and by mitigating them is deemed to create the ecosystem of trust and excellence built on requirements to both: technology and humans who stand behind its development, deployment, and use . 
Moreover, in its draft opinion on AI Act JURI committee suggested including a concept of trustworthy AI systems in the regulation. The opinion states that “without AI Systems – and the human beings behind them – being demonstrably worthy of trust, serious and unwanted consequences may ensue, and their uptake might be hindered, preventing the realisation of the potentially vast social and economic benefits that trustworthy AI systems can bring” . This assertion is noteworthy from several perspectives. First of all, despite the broadly discussed risks of AI systems, and automated decision-making systems particularly, their benefits are still recognized and proliferated. Second, to enjoy these benefits and to avoid potential harms, AI systems shall be trustworthy, and, furthermore, it is reaffirmed that a human being shall be presented behind them. It is our opinion, and we will elaborate on it further in this Chapter, that from the AI Act perspective, human being shall also satisfy certain legislative requirements and by this contribute to comprehensive ecosystem of trust which shall be maintained for development, deployment, and use of AI systems. 
Considering the above statement, it is remarkable that the JURI’s Opinion suggested amending the AI Act with a new article enshrining a concrete concept to be placed behind the criteria of trustworthiness which up to the date has not been developed into any practical requirements . Thus, it is envisaged the following:
“4 (a)Trustworthy AI
1. Providers of an AI system shall, throughout all stages of development of the AI system, acknowledge the EU Charter of Fundamental Rights and ensure that the AI system is lawful, ethical and robust.
(a) ‘lawful’ means that the AI system is developed to operate in accordance with European, national and international legally binding rules;
(b) ‘ethical’ means that the AI system is developed taking into account the specific benefits of the AI system while respecting the freedom and autonomy of human beings, human dignity as well as mental and physical integrity, and to be fair and explicable;
(c) ‘robust’ means that the AI system performs in a safe, secure and reliable manner, with embedded safeguards to as much as possible prevent any unintended adverse impacts.”
It is our opinion that such a limitation of the above article to address only  providers of AI systems unreasonably excludes other stakeholders, such as users of those systems, and imposes compliance with the criterium of trustworthiness only on providers, i.e., the developers or deployers of the respective systems . Although it seems obvious that once placed on the market AI systems shall, at least, meet regulatory requirements, i.e., be lawful and, at most, comply with the very same benchmarks of lawfulness, ethics and robustness, it may be said that that by adding references to ethics and robustness together with lawfulness to the AI Act the requirements to AI systems are amplifying beyond those imposed by legislation. In particular, the JURI’s Opinion names the Framework of ethical aspects of artificial intelligence, robotics and related technologies  and refers to the European Standardization Organization as to the possible solution to fill the gap in designing the notion of trustworthiness beyond the legally binding rules . 
One may argue that such amplification may pose undesirable burden on the providers of AI systems, especially considering numerous politics regarding ethics in the AI domain, however, the same newly introduced article provides for trustworthy technology assessment and stated that the assessment list, as well as AI toolkit for AI and data protection risk mitigation and monitoring shall be developed by the European Commission , which means that further authoritative guidance will be facilitated to the stakeholders. Notably, JURI separately emphasized data protection in its suggestion for the risk assessment of AI systems  which demonstrates the intended interplay and applicability of both legislative frameworks (the AI Act and GDPR) where automated decision-making systems and personal data meet. 
Further in this Chapter we will explore what is AI system according to the EU regulation, how the risk pyramid is built, what obligations are imposed to the stakeholders and what one can expect within the scope of protection vis-à-vis artificial intelligence with the goal of make AI systems trustworthy and functioning for the benefit of humans.
1.1.	Definition of AI
 Acknowledging that technologies are being developed with a faster pace than legislative processes and procedures move, the legislators tend to rely on existing industry-accepted practices, providing for stakeholders’ consultations and aiming at design of the regulation applicable to AI domain to be flexible, adjustable, and relevant at all times. It is noteworthy though that in the scientific world there is no consensus on what artificial intelligence in particular is rather than this term applies to a number of techniques which may replicate or imitate, at least, some competences of human intelligence. Considering the above, it may be a hard task for the legislators to introduce a legal definition of artificial intelligence systems for the purpose of regulation securing both: its relevance and a certain level of adaptability to meet technological progress on the one hand and legal certainty, on the other. 
The European approach to define AI systems is based on that of the OECD. Thus, article 3 (1) of the AI Act states that:
“‘artificial intelligence system’ (AI system) means a system that is designed to operate with elements of autonomy and that, based on machine and/or human-provided data and inputs, infers how to achieve a given set of objectives using machine learning and/or logic- and knowledge based approaches, and produces system-generated outputs such as content (generative AI systems), predictions, recommendations or decisions, influencing the environments with which the AI system interacts” .
Yet, the definition of an AI system is still being discussed by the legislators . The legislators addressed the concepts of machine learning and logic/knowledge-based approaches in Recitals 6(a) and (b)  that are not binding, however, and aim at providing the general understanding of what may be understood as such techniques. Further, Article 4 envisages that the European Commission “may adopt implementing acts to specify the technical elements of those approaches, taking into account market and technological developments” .
Although the construction of this Article allows to modify the legal definition of AI systems by specifying the technical scope, it is clear that those systems that may produce outputs such as predictions, recommendations, and decisions by using either machine or human-based input (or both) with their diverse levels of autonomous behaviour are subject to this regulation. These systems may be employed in different domains and for various purposes, including for decision making which may produce legal or similarly significant effect on individuals (data subjects). In this case AI Act will be applicable to those systems alongside the General Data Protection Regulation, i.e., Ai Act provision will complement those principles and safeguards granted under the European data protection regulation (See diagram below).
 
Furthermore, dealing with artificial intelligence systems the legislators opted for the risk-based approach to differentiate extent and strictness of regulation applicable to the systems operating in different domains and bearing distinct risks, notwithstanding the attribution to public or private sector. It is also notable that the JURI Committee proposed to amend the AI Act with the definition of risk itself to be “the combination of the probability of occurrence of a harm and the severity of that harm” . Drafting the regulation out of intention to target concrete levels of risk with the adequate legal governance, the legislators developed a classification of risks, already known as “pyramid of risks” recognizing low and minimal risk, limited risk, high risk and unacceptable risk  and corresponding obligations regarding them. 
The AI Act expressly names certain categories of AI practices that shall be prohibited in the European market (unacceptable-risk AI systems) . This includes AI systems that (i) deploy harmful manipulative subliminal techniques, (ii) exploit certain groups vulnerable due to physical or mental disability or age; (iii) are used by public authorities or on their behalf for social scoring, and real-time remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement (with specific exceptions). AI systems which may produce adverse impact on health, safety and fundamental rights being either stand-alone system (in the domains listed in Annex III to the AI Act which may be amended by way of delegated acts) or safe component of a product or a product itself that fall under Union health and safety harmonisation legislation are to be considered as high-risk AI systems . The significant part of the AI Act is dedicated to the regulation of these practices and is of utmost interest for the purpose of this work. Certain AI systems, for instance those that interact with humans, emotion recognition systems, biometric categorisation systems or systems that generate or manipulate image, audio, or video content, shall be considered as posing limited risk and are subject to only transparency requirements . All other AI systems shall be deemed as presenting low and minimal risk and are not subject to additional legal obligations under the AI Act. 
It is worthwhile to dwell on high-risk AI systems as their current scope proposed by the AI Act intersects with the scope of Article 22 of the GDPR to a certain extent. Title III of the AI Act establishes particular criteria to classify AI systems as high risk, requirements to such systems, obligations of their providers, basis for notified bodies to be involved in conformity assessment and conformity assessment procedures themselves. 
As stated multiple times throughout the AI Act, consideration must be given to intended purposes of AI systems, severity, and probability of occurrence of risk of harm to health, safety, and the fundamental rights of persons to be classified as high-risk . This risk assessment is already made by the legislators for those AI systems which are intended to be used in areas indicated in Annex III to the AI Act, but also applies to the systems of an equivalent or greater risk of harm that are not listed therein yet but may be added further by the European Commission to that list. Article 7 of the AI Act provides for the criteria which shall be considered for the purpose of such amendment, among which the extent of the potential adverse impact on persons which are dependent on the AI systems’ outcome “because for practical or legal reasons it is not reasonably possible to opt-out from that outcome”  or are vulnerable “due to an imbalance of power, knowledge, economic or social circumstances, or age”  and reversibility of such outcome . It is our opinion that these criteria precisely address the systems which may produce legal or similarly significant affect on individuals in terms of the GDPR and the obligation of human intervenor to contribute to decisional outcome. 
In addition to these criteria, the JURI Committee suggests introducing such qualifications as the extent of availability and feasibility for monitoring, reliability and correctability of high-risk AI systems  and potential for their misuse and malicious use . As we discussed earlier, reliability and explainability of ADMS (including AI systems) is a paramount matter raised by the stakeholders in automated decision-making domain, first, in terms of quality of and causation in automated decisions and second – for due performance of human intervenors therein, i.e., their comprehension of ADMS and automated outcome and real possibility to actually interfere. Finally, we believe that although misuse and malicious use of AI systems may be considered as a technical matter, e.g., in cybersecurity domain, it may also serve to approach AI systems’ users, or human decision-makers, in the context of their interaction with ADMS, due diligence and application of ADMS and their outcomes.
Furthermore, we want to address already defined areas of use and purposes of AI systems which shall be considered high-risk in accordance with Annex III to the AI Act. Currently, Annex III suggests eight areas of high-risk AI systems: biometric identification and categorisation of natural persons; management and operation of critical infrastructure; educational and vocational training; employment, workers management and access to self-employment; access to and enjoyment of essential private services and public services and benefits; law enforcement; migration, asylum, and border control management; administration of justice and democratic processes. Besides, Annex III to the AI Act lists certain purposes of AI systems in these domains which include assessment, evaluation, and decision-making about access to education, employment opportunities, creditworthiness, public benefits to name a few. One could note that purposes and domains referred in Annex III constitute iconic examples used for explanation of legal and similarly significant effects of automated decision-making prohibited under Article 22 of the GDPR . Therefore, it may be assumed that the legislators intend to provide additional regulation of new technologies, namely, AI systems, which, although represent automated means, impose more risk on affected individuals than other existing technologies; and, assumably, should be subject to enhanced requirements compared to those already provided by the data protection regulation.  
What is more, we want to highlight the changes proposed by the JURI Committee’s Opinion  to the scope of Annex III to the AI Act. The JURI Committee suggests shifting certain AI systems the intended use of which concerns assessment and decision-making regarding access to education, employment, public assistance benefit and services, creditworthiness and credit scoring, assessment of individuals and predicting practices by law enforcement public authorities, assessment by competent authorities in the migration domain from being high-risk AI systems to prohibited artificial intelligence practices. The rationale behind this amendment given by the JURI Committee is that AI systems being based on the historical data will only “exasperate existing inequalities”, which is already proven by some deployed systems, and inexplicability of AI systems’ decision-making poses unacceptable greater risk on human fundamental rights . Notably, in its Amendment 549 the JURI Committee expressly states that decisions regarding work-related contractual relationships severely affect the lives of individuals and shall be made exclusively by human beings . Considering that Article 22 of the GDPR has already prohibited such automated decisions, presumably, the JURI Committee’s intention is to address derogations from Article 22 of the GDPR in terms of AI systems application and to ensure that such decisions in no case will be made by AI systems. It is worth noting that the proposed areas for determination of high-risk AI systems were left unchanged. 
Based on the above suggestions of the European Parliament to the European Commission and their justifications, we may assume that at the moment even among the institutions of the European Union there is no consensus on the level of risk posed by AI Systems intended to use for evaluation, assessment and decision-making in such critical domains as education, employment, creditworthiness, public assistance services and benefits and law enforcement; and in the opinion of the JURI Committee, the new regulation and additional safeguards provided by the AI Act will not adequately protect individuals vis-à-vis threats brought by AI Systems intended to use for the said purposes in these critical domains. We might assume that the rationale thereof is that assessment, evaluation, and decision-making, even safeguarded by human intervention by Article 22 of the GDPR, should not be delegated to AI systems due to such systems replicate existing biases and discrimination presented in historical data and as a consequence of their limited explainability which is not acceptable when affecting fundamental human rights. In our opinion, it may mean that the lawmakers do not believe that human intervenors may actually overtake decision-making when relevant AI systems are employed as required for compliance with Article 22 of the GDPR. 
It is interesting that in addition to already foreseen possibility to amend high-risk AI systems list by way of delegated acts, the JURI Committee proposed the same instrument to amend the scope of additionally prohibited artificial intelligence practices . As delegated acts provide for more efficient and faster change of the legislation to keep pace with technological advancement, there is a potential to see further how certain purposes and domains will be migrating between banned and high-risk AI systems lists or will be added or, in turn, excluded from those lists, showing how the underlying “trust” in those systems will change in eyes of the European legislators.
Ultimately, we want to stress out once again that the areas where certain AI systems may be used were saved by the JURI Committee’s Opinion  and specific purposes were preserved or added. For instance, the JURI Committee introduces AI systems to be used for task allocation in work related contractual relationships, to access insurance premiums and claims, for individual risk assessment in context of access to private and public services, including determining the amounts of insurance premiums, to access medical treatments, or in context of payment and debt collection as high-risk AI systems . AI systems intended to be used for crime analytics regarding natural persons by law enforcement authorities and for verification of the authenticity of travel documents and supporting documentation of natural persons by competent public authorities stay in Annex III to the AI Act. 
Considering the above and the overall European approach to new technologies, including artificial intelligence, brining potential harm to privacy and data protection among others fundamental human rights, one may notice certain overlapping between Article 22 of the GDPR and AI regulation proposed by the European Commission which presumably, from our point of view, also means recognition of insufficiency of the current regulation in automated decision-making domain. Further, we will explore what requirements the regulators introduced for the purpose of managing of the high risk in terms of the AI Act while building trust in artificial intelligence application and how they intersect with the European data protection regulation.    
 
2. Trust in AI systems: transparency and accountability

As we already described earlier in this work, the phenomena of algorithmic aversion, based on distrust thereto, may affect employment and use of ADMS, including AI systems, by human operators. Considering the high level of risk posed by AI systems and fundamental human rights at stake, the legislators created a set of requirements and obligations applicable to AI systems which performance presents a serious threat to individuals’ rights, including data protection and privacy, and their providers. It may be evidenced that those requirements were dictated by the necessity to align development, deployment, and use of AI systems with the principles established to ensure trustworthiness of artificial intelligence in terms of AI HLEG Guidelines  which potentially may migrate to the AI Act . 
The JURI Committee in its Opinion states that although being discussed frequently between the EU Institutions the notion of trustworthiness has not been legally reflected yet . It is worth noting that researchers also put their attention to the existing gap between various industry documents on ethics and practical implementation of the principles contained therein . We deem necessary to add here that the GDPR already established certain principles for data processing also applicable to AI systems when automated decision-making is concerned: lawfulness, fairness and transparency, purpose limitation, data minimisation, accuracy, storage limitation, integrity and confidentiality (security), and accountability . Further we will explore how these principles intersects and are built-in to certain legal requirements of the AI Act in attempt to build trustworthy AI and create ecosystem of trust for its development, deployment, and use.
Although some authors claim that “technology itself [including artificial intelligence] cannot be trusted or considered trustworthy” , and it would be more appropriate to refer to reliable AI instead of trustworthy AI” , we will keep using the terminology accepted in the industry and almost brought to the level of European legislation. As stated by the JURI Committee’s Opinion, AI systems and human behind them shall be capable to demonstrate that they are worthy of trust . Arguably, trustworthiness may be tight to transparency  which indeed constitutes an important principle of data processing under the GDPR and results in corresponding data subjects’ rights and data controllers’ obligations regarding obtaining and provision of information, as discussed in the previous Chapter. There is a viewpoint that trust is built on transparency, responsibility, understanding, stewardship, and truth . It is our opinion that these criterium perfectly fall into the scope of data protection principles where the principle of lawfulness, fairness and transparency covers transparency, understanding, and truth; the principle of accountability includes responsibility; and stewardship, i.e., a good custodian of data, falls under the accuracy and security principles of the GDPR. 
These principles also apply and construct a solid base for the AI governance. Following the approach of trust in multi-agent system  and the JURI Committee’s opinion on building trust to both AI systems and involved human agents (either developers, deployers or users), we are going to explore technology governance and non-technology governance  in artificial intelligence domain in terms of the AI Act. This section will cover algorithm-targeted regulation which we examine from the transparency and accountability perspectives to analyse what legal requirements are proposed in order to ensure trustworthiness of AI systems per se, mainly from the human-in-the-loop perspective. In the next section, we will explore what legislative provisions are provided for building trust in human agents (i.e., non-technology governance). 
2.1.	Transparency
As we discussed in the previous Chapter, to enable human agent to meaningfully monitor, control, intervene and override operations and outputs of automated decision-making systems, including AI systems, it is of utmost importance that such involved human agents have access to adequate volume and types of information on automated decision-making process. Compared to the GDPR, The AI Act provides for a different approach towards explainability of AI systems. If the GDPR foresees the requirement of meaningful human intervention in ADMS - and it may be withdrawn from the regulation and the EDPB guidelines that human decision-makers shall receive information about and understand corresponding automated decision-making processes (user-enabling transparency); and the data subject right to information, i.e. to receive at least information about the logic involved, the significance and the envisaged consequences of automated decisions  (data-subject-enabling transparency), the AI Act provides for transparency and information provision towards users of AI systems (user-enabling transparency). It means that in case of automated decision-making under Article 22 of the GDPR a concerned individual will not directly benefit from the transparency obligations under the AI Act as it covers only provider-user relations. This notwithstanding, a user of AI system, who may act as a data controller under the GDPR, may receive more extensive information on ADMS, its functioning and the output, under the framework of the AI Act. Therefore, Article 14 of the AI Act would complement the relevant provisions of the GDPR and enable data controllers to comply with their obligation to provide meaningful information to data subjects, as the AI Act envisages a broader scope of transparency and explainability instruments. Accordingly, data subjects may benefit from the AI Act mandated enhanced transparency requirements as the last link in the information supply chain. Although, such information to be received by data subjects will pass through the data controller “gate” and may be somehow twisted by the data controller lenses and its interpretative capacity and limitations in understanding. We see a two-folded character of such a pass-through. On the one hand data subjects, as non-experts, may enjoy understandable explanations given by data controllers (assumably, experts). On the other hand, there will be a room for the information to be unintentionally corrupted or misinterpreted by data controllers. Furthermore, data controllers’ expertise may also be questioned.   
Here we want to deviate from the legislative requirements to address some theoretical findings regarding the notion of explainability in conjunction with transparency. Although the issue of explainability of AI is being broadly discussed by computer scientists, legal professionals and experts in philosophy, psychology, and linguistics , there is no unified approach thereto . It is deemed that there is a tension between usage of such terms as interpretability, explanation and causality . Some authors suggest that algorithm interpretability and algorithm explainability may be used synonymously, however, provide for a slight difference: the former deals with “cause and effect within a system” and the latter explain internal mechanics of algorithm in human language . There is also a viewpoint that explainability and interpretability together constitute the explicability of a system . Mi-Young Kim and Shahin Atakishiyev et al. suggested that interpretation refers to a model (system), and explanation is somewhat provided by a system regarding its output to explainee . 
Even though there is no unified terminology regarding the “explanation” of AI systems or their outputs, unarguably, AI systems shall have a quality of explainability to a certain extent. While the lawmakers elaborate on the scope of transparency principles and information to be provided, some researchers also intend to explore whether quantitative characteristics to the quality of explanations could apply . By introducing the AI Act the legislators envisage a list of information to be provided to users of high-risk AI systems to gain a certain level of explainability and understanding. Apart from that, the legislators name two purposes  of providing of this information: “to enable users to interpret the system’s output and use it appropriately” (transparency) and “to achiev[e] compliance with the relevant obligations of the user and of the provider”  (accountability). 
With regard to information provision, Article 13 (2) of the AI Act states:
“High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to users.”
This information may include inter alia the characteristics, capabilities and limitations of performance of the high-risk AI system; notably, its performance as regards the persons or groups of persons on which system is intended to be used; the human oversight measures, including the technical measures put in place to facilitate the interpretation of the outputs of AI systems by users, and necessary maintenance and care measures to ensure its proper functioning . It may be concluded from Article 13 of the AI Act that the possibility of provision of the required information shall be ensured by the provider of an AI system, first, by way of design and development of the AI system aiming to facilitate the appropriate degree of transparency and, second, by accompanying the system with instructions for users. 
Article 13 of the AI Act is complemented with the record-keeping (logging) requirements: logging capabilities shall serve the purpose of traceability of compliance of the AI system functioning with its intended purpose during its whole lifecycle , and shall enable monitoring of its operation with regard to potential risks, including to data protection and privacy . Another legal provision which may contribute to building the transparency of AI systems is the requirement to draw up technical documentation for the purpose of demonstrating the compliance with the regulation by providers . Detailed requirements to the technical documentation under Article 11 are placed in Annex IV to the AI Act which overlaps to the certain extent with the list of information to be provided to users of AI systems under Article 13 (3) and includes among others the assessment of the human oversight measures . Thus, providers are required to fulfil the corresponding obligations to draw up technical documentation  and to keep automatically generated logs  and ensure overall compliance of AI systems with the regulation . It is our opinion that these articles together frame “the two-fold goal of Article 13 (1)”  of the AI Act. 
It may be evidenced that the legislators are aiming to provide for both ex ante and ex post explanation of AI systems. Thus, Article 11 of the AI Act foresees drawing-up of the technical documentation of a high-risk AI system before that system is placed on the market or put into the service and updating of the technical documentation, as appropriate. As elaborated in Annex IV to the AI Act, the information contained in the technical documentation shall include among other “the design specifications of the system, namely the general logic of the AI system and of the algorithms; the key design choices including the rationale and assumptions made, also with regard to persons or groups of persons on which the system is intended to be used; the main classification choices; what the system is designed to optimise for and the relevance of different parameters…” , “where relevant, the data requirements in terms of datasheets describing the training methodologies and techniques and the training data sets used, including information about the provenance of those data sets, their scope and main characteristics; how data was obtained and selected, labelling procedures, data cleaning methodologies” , “the validation and testing procedures used…metrics used to measure accuracy, robustness, cybersecurity…as well as potentially discriminatory impacts…”  and “[d]etailed information about the monitoring, functioning and control of the AI system, in particular with regard to: its capabilities and limitations in performance, including the degrees of accuracy for specific persons or group of persons on which the system is intended to be used and the overall expected level of accuracy in relation to its intended purpose; the foreseeable unintended outcomes and sources of risks to…fundamental rights and discrimination in view of intended purpose of AI system…specifications on input data, as appropriate” . Notably, the list of information to be included in technical documentation may be amended by way of delegated act to be aligned with technical progress  - another example of the design of a flexible legal framework compatible with evolving technology development which may be used to enhance the scope of human oversight further, as a need may be.
The above information may contribute to prospective transparency , i.e., upfront provision of information about methods, designs, data, logic involved and even possible outcomes. Thus, users of AI systems may understand how the system may make decisions in general and they can do so prior to using those systems. 
Furthermore, Article 12 of the AI Act provides for record-keeping with regard to high-risk AI systems. Record-keeping is an automatic recording of events (logs) that shall be enabled while the high-risk AI system is operating . These “logging capabilities shall ensure a level of traceability of the AI system’s functioning throughout its lifecycle that is appropriate to the intended purpose of the system”  and “[i]n particular,…shall enable the monitoring of the operation of the high-risk AI system with respect to the occurrence of situations that may result in the AI system presenting a risk…” , where a risk includes a risk to fundamental rights of persons . Although the term “appropriate” may be vague from a practical viewpoint, it is clear that the legislators intend to introduce a post hoc traceability of the AI systems in place to control whether they present a risk to privacy and data protection being human fundamental rights, and which, in our opinion, constitutes retrospective transparency . 
Although, as mentioned above, the AI Act refers to the user-enabling and not to the data-subject-enabling transparency, we are of the opinion that the provided legal framework regarding information obligations when using AI systems perfectly complements the GDPR approach to the provision of information to data subjects. Thus, prospective transparency may contribute to a more complex provision of meaningful information about the logic involved in automated decision-making as well as significance and envisaged consequences thereof by enabling users of high-risk AI systems to obtain a broader scope of information from AI systems providers. Whereas, the traceability of AI system functioning may provide a better context for the data subjects enjoying their rights to express their point of view and to contest the decision due to better understanding of the steps and considerations made in the decision-making process.
Interestingly, the AI Act envisages the provision of information not only about AI systems themselves, but also about data sets used during training of the systems as well as training methodologies and techniques, and labelling procedures. Moreover, the new regulation addresses data and data governance by establishing certain data quality criteria to be met . For instance, Article 10 (2) foresees the requirements for AI systems providers to apply data governance and management practices which shall concern, inter alia, a prior assessment of availability, quantity and suitability of the data sets that are needed and their examination in view of possible biases . Furthermore, Article 10 (3) states that “data sets shall be relevant, representative, free of errors and complete” and “shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons on which the high-risk AI system is intended to be used”, and Article 10 (4) augments that “data sets shall take into account…the characteristics or elements that are particular to the specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used”. 
Comparing to the GDPR principles of accuracy, which provides for accurate and updated data to be processed, and of data minimisation, which requires to process only adequate, relevant and limited to what is necessary, the AI Act stipulates the requirements to quality of data used in training, validation and testing of high-risk AI systems to ensure high level of the data sets to be used for those purposes and to provide for diversity depending on geographical, behavioural or functional setting acknowledging their intended domains of usage. Additionally, the AI Act imposes on developers of high-risk AI systems the obligation to review data sets for possible bias which means that ex ante control in terms of possible discrimination and under- or misrepresentation of certain groups shall be fulfilled. Furthermore, as we mentioned earlier in this Chapter, developers of high-risk AI systems shall include this information on training, validation and testing data sets and procedures as well as potentially discriminatory impacts in technical documentation  to be provided to users which may execute then certain data processing activities through high-risk AI systems in regard to data subjects. Thus, data controllers, being users of high-risk AI systems, may consider the information about quality of data sets, including their completeness, relevance and potential risks furnished by providers to demonstrate data controllers’ compliance with the GDPR. It is our opinion that such additional requirements to data quality are dictated by nature of the AI Act which addresses specific technology and reflects risk-based approach to ensure that where a high risk is in place, technology shall be designed, developed and used in compliance with a tighter regulation specifically targeting that high risk, whereas the GDPR was enacted as a technology neutral regulation . Such a differentiation may also evidence the legislators’ position that the GDPR may be not adequate enough to protect individuals vis-à-vis advancing technologies used including for automated decision-making. 
Notably, pure existence of information or explanation, nonetheless, may not be sufficient to ensure better protection of individuals as such . As discussed earlier, it is deemed that black-boxed ADMS may bring threats to fundamental rights, including human dignity, and, in turn, explainability, being a prerequisite of transparency, may enable humans to secure their personal dignity per se (“personhood argument”) . For instance, Colaner suggests that there are three justifications advocating for the personhood argument: personal dignity demands, first, a possibility of meaningful participation in decision-making regarding a person or his or her community , second, understanding the reason of what has happened to him or her , and finally, a possibility to govern their lives and general institutions , which would be practically unfeasible if automated decision-making is unexplainable.
Although we share this point of view, we would like to elaborate on its relevance in regard to the AI Act and the GDPR. As we described above, the AI Act stipulates the provision of a broader scope of information relating to high-risk AI systems, nonetheless such information is to be available for users of those systems and not to individuals who might be affected by their outcomes. Even though data subjects may consequentially benefit from such extended information available to data controllers - the users of high-risk AI systems - it is not necessarily so. First, we want to highlight that the flow of information and explanation in case of high-risk AI systems will contain an intermediary (data controller, or user of AI system) on its way from developers to data subject, and considering the fact that the scope of data subjects rights under the GDPR is not concerned by the AI Act, data controllers will not be legally required to improve neither procedural nor material aspect of their procedures for information provision to data subjects whose rights and protection will still be limited to those granted by the GDPR. 
Further, it is worth noting that once relevant information or explanation is received by users, it may be conditioned or interpreted by those users themselves. Thus, consideration must be given to skills and literacy of users and their ability to correctly interpret algorithmic outcomes . Consequently, explanation per se, even being intrinsically valuable , quality-wise may not be useful due to personal and professional characteristics of the recipient of the explanation , a data controller’s representative, and may not improve data subjects setting without additional legal requirements towards such a representative in order to ensure that individuals affected by use of high-risk AI systems for the purpose of decision-making could benefit from the enhanced transparency provisions of the AI Act. From this perspective, nevertheless, the AI Act goes beyond the meaningfulness requirements enshrined by the GDPR and the EDPB guidelines by providing certain requirements to ensure appropriate human oversight  which we will address further in this Chapter . 
Notwithstanding the above, the requirements of the AI towards transparency of algorithms based on the explainability and traceability of high-risk AI systems are deemed crucial to proportionally diminish the risks to fundamental rights, including data protection, brought by the employment of those systems  and, consequentially, shall contribute to building overall trustworthiness of AI systems. Even so, some authors  argue that trust is not build only on transparency and a consideration must be given to accountability as a primary measure of trustworthiness. Accountability may be also linked to or enhanced with transparency , however being transparent does not necessarily mean being accountable  as accountability goes beyond transparency and depends on the context and a certain demand to hold to account . 
2.2.	Accountability.
As mentioned above, another broadly discussed notion constituting one more pilar of trustworthiness is accountability. For instance, accountability is named as one of the general principles for design, development, and implementation of AI systems by the IEEE Global Initiative . In terms of the GDPR, accountability is named as one of the principles relating to data processing  holding data controllers responsible for what they do with personal data and how they comply with other GDPR principles. The AI Act intends to create a framework for accountability with reference to AI systems and in particular to those of high-risk. However, the context of these “accountabilities” differs and we will discuss below what may be expected from high-risk AI systems in terms of accountability, how this intersects with the GDPR and whether data subject concerned by Article 22 of the GDPR may benefit from the new regulation. 
In literature accountability is normally defined as answerability to somebody else  or as an obligation of an actor to explain and justify his or her conduct to a forum, where the forum may contest the conduct and the actor may bear consequences . As Bovens further elaborates , accountability foresees that (i) the actor is obliged to provide information about his or her conduct and (ii) the forum should have a possibility to pose questions and (iii) may then pass judgement thereof and (iv) consequentially, the actor may face certain consequences either informal or formal which may take form of sanctions. 
Based on these components of the notion of accountability we would like to address possible interactions and associated accountability therein as foreseen in the AI Act. The Act provides for different sets of obligations for several stakeholders in relation to high-risk AI systems: providers , importers , distributors  and users , of which the major burden of requirements in relation to high-risk AI systems compliance with the regulation rests upon the shoulders of providers and a limited list of obligations is provided for users of those systems. The AI Act gives the definition of providers and users as follows:
“‘provider’ means a natural or legal person, public authority, agency or other body that develops an AI system or that has an AI system developed with a view to placing it on the market or putting it into service under its own name or trademark, whether for payment or free of charge” , and
“‘user’ means any natural or legal person, public authority, agency or other body using an AI system under its authority, except where the AI system is used in the course of a personal non-professional activity” .
It may be argued that the personhoods of a provider and a user may be merged where the roles are fulfilled by the same entity. Nonetheless, we would like to highlight that in case of automated decision-making in terms of Article 22 of the GDPR where high-risk AI systems are employed users will act as data controllers (or processors), therefore will be obliged to comply with their respective obligations under the GDPR together with those imposed by the AI Act. Providers, in turn, may be not concerned by Article 22 of the GDPR if they merely develop high-risk AI systems and do not act as users. It is noteworthy that the AI Act foresees that the role of users may be switched to that of provider under certain circumstances , basically, when users make changes to the architecture or the purpose of the system, i.e., modify the underlying technological basis of automated decision-making.
As we mentioned above, different roles trigger a distinct scope of applicable requirements. Thus, Article 16 envisages the following obligations for providers of high-risk AI systems:
•	to ensure that their high-risk AI systems are compliant with the requirements for high-risk AI systems prescribed by the AI Act;
•	to put a quality management system in place;
•	to draw-up technical documentation;
•	when under their control, to keep automatically generated logs;
•	to ensure conformity assessment performance;
•	to comply with obligation of registration of high-risk AI systems; 
•	to take corrective actions;
•	to inform competent authorities and notified bodies, as applicable, of non-compliance and of any corrective actions;
•	to affix CE marking; and
•	to demonstrate compliance with the requirements for high-risk AI systems.
Meanwhile users’ obligations are mostly limited to proper use (in accordance with what was mandated by providers) of high-risk AI systems and monitoring of correctness of systems operations and notify providers in case of incidents and malfunctions . 
It is argued by some authors  that such an allocation of obligations in the AI Act constitutes a framework distinct from that proposed by the European Commission in its White Paper on Artificial Intelligence . However, our opinion is that the legislators might acknowledge lack of expertise and skills of users compared to those of providers and accordingly reallocated the burden of accountability in relation to high-risk AI systems to the latter yet providing a mechanism for handover of accounts to users by virtue of Article 29 of the AI Act to address those risk arisen at and associated with the stage of deployment, use and refinement . Thus, it may be said that the European Commission hold providers (and users when considered as providers) to account for their creations as users might not have adequate competence or resources to efficiently control the technology. 
Here, we want to address the respondeat superior doctrine which foresees that the superior shall bear liability for actions of his or her subordinates while performing their duties . The question should be asked who or what acts as subordinate and as a superior and vis-à-vis whom. Further, Bovens raises, among others, two additional matters we would like to highlight for the purpose of this discussion. His typology of accountability underlines “the problem of many eyes”, which refers to a possibility of existence of several fora, and “the problem of many hands”, which in turn refers to multiple actors . These two issues may be raised also in the realm of data protection relating to use of high-risk AI systems for the purpose of decision-making as the GDPR together with the AI Act envisage multiple stakeholders such as data subject and data controller, user and provider, where users are likely to act as data controllers in case of automated decision-making and their role may change from an actor in regard to data subjects to a forum in relation to providers of high-risk AI systems. Thus, from data subject’s point of view, it may appear that both or either of data controller or AI systems provider may share accounts. In turn, providers being accountable for design, development and deployment of AI systems may be responsible towards both or either of users (assumably data controllers) or data subjects, where the latter are concerned by algorithmic outcome. 
It may be seen as unarguable that providers, being in charge of design and development of their systems are vicariously liable for high-risk AI systems per se  and where applicable under Article 29 of the AI Act users may be held to accounts. However, this might be ambiguous from the viewpoint of a data subject concerned by an automated decision under Article 22 of the GDPR as capability of such a data subject to identify and separately assess performance of ADMS and human decision-maker is practically impossible (as we discussed earlier), and, ultimately, data subjects will only be entitled to question data controllers in this regard. It is our opinion that in this case it might be more appropriate to consider human actors as the superiors of inferior ADMS, although, under the framework of the GDPR. Hence, high-risk AI systems may act as “subordinates” (although without own legal personhood) of both providers and users in terms of accountability. 
Further, consideration must be given to the term “accountability” in relation to AI systems per se and to humans who interact (design, develop, deploy or use) those systems. Although algorithmic accountability is a broadly discussed notion , it seems undisputable that only persons, either natural or legal, may be held to accounts . Thus, it is our opinion that AI systems may only be requested to be quasi-accountable to those human agents involved in the overall lifecycle of automated decision-making. From this point of view, unarguably transparency of AI systems including their auditability, responsiveness, and reporting  contributes to quasi-accountability of algorithms towards involved humans be they providers or users of those systems. Nonetheless, additional instruments shall be given to both to ensure accountability of the high-risk AI systems, and the AI Act provides for such mechanisms by virtue of the provisions described below.
As mentioned above, Article 16 of the AI Act foresees that providers shall ensure that their high-risk AI systems are compliant with the applicable requirements. For this purpose, providers shall put in place a proportionate quality management system which shall include general strategy for regulatory compliance , certain techniques, procedures and systematic actions relating to design and development stages , risk management system , certain measures and procedures applicable to use phase  and accountability framework establishing management and personnel responsibilities regarding the components of such a quality management system . 
Although we share the opinion that as such accountability requirements may have positive effect on ex ante decisions  about how (not) to act and in relation to ADMS, including high-risk AI systems, on design, development, and deployment choices, it is noteworthy that accountability is deemed to be retrospective characteristic , as accounts may be given only after occurrence of certain action or inaction. Therefore, we are going to address below those measures and procedures envisaged by the legislators to ensure possibility of holding high-risk AI systems to account when such systems are already deployed and are in their use phase.
First, and foremost, obligation of providers in regard to accountability is that of post-market monitoring . Article 61(1) of the AI Act states that “[p]roviders shall establish and document a post-market monitoring system in a manner that is proportionate to the nature of the artificial intelligence technologies and the risks of the high-risk AI system”. Interestingly, the legislators make a reference to different nature and level of risk of high-risk AI systems as a criterion for setting up proportionate post-market monitoring systems. It is yet unclear what could be deemed as an appropriate factor for justification of distinct scopes thereof and further authoritative guidance will be needed as by virtue of Annex III to the AI Act all listed therein AI systems are categorized as high-risk. However, we could assume that a possible criterion may be a scale at which such AI systems are employed and may result, for instance, in variation of frequency of monitoring and reporting.
The purpose of post-market monitoring system is collection, documentation, and analysis of data on the performance of AI systems during their lifetime, provided by users or otherwise, and ensuring of compliance with the applicable requirements of AI systems by providers . Notably, this passive sourcing of information of (non)performance of AI systems is one more tool for providers to receive users’ feedback on their operations in addition to the obligation of users to actively report incidents and malfunctioning thereof . Considering, first, the fact that the legislators did not foresee any timeline for users to report non- or improper performance and that the practical requirements to post-market monitoring are yet vague and, second, a probable lack of expertise, skills or knowledge of users to detect AI systems’ dysfunction, such ambiguous provisions with regard to post-market monitoring may result in significant gaps between incidents and corrective measures thereof and may adversely affect fundamental rights of individuals concerned by such incidents or malfunctioning. This may also have negative implications on data controllers’ obligations under the GDPR which we will address further.
Moreover, the AI Act obliges providers to report any serious incident or any malfunctioning of high-risk AI systems which constitutes a breach of obligations under the European Union law intended to protect fundamental rights to the market surveillance authorities . Such notification shall be given upon establishment of a causal link between the AI system and occurred incident or malfunctioning immediately but, in any event, not later than within fifteen days . Undoubtedly, those incidents or malfunctioning that comprise a breach in data protection domain fall into the scope of this regulatory provision. However, the legislators did not foresee providers’ obligation of reporting these breaches to users, which may be obliged, in their role of data controllers, to report such breaches without undue delay and, where feasible, not later than 72 hours after having become aware of it , neither providers may be requested to give such a notification under the GDPR unless they are joint data controllers together with or data processors on behalf of users. Although, most probably, in such cases data controllers will be exempt of liability under the GDPR due to their unawareness of the breach, it is worth considering that notification of a personal data breach serves as a transparency tool as well as aims at better protection of data subjects. Thus, data subjects might not be (timely) aware of an occurred incident that, potentially, might impact them.
Another accountability tool prescribed by the AI Act is conformity assessment . Article 19 envisages that providers shall ensure that their high-risk AI systems undergo conformity assessment procedure and, in case of conformity, draw up an EU declaration of conformity and affix CE marking. There are two conformity assessment procedures under the AI Act: first one is based on internal control and the second foresees involvement of a notified body . Yet an external control of a notified body will only be present in case harmonized standards  published in the Official Journal of European Union are not applied or applied only partially, or in case such harmonized standards does not exist and common specifications  are not available . Although harmonized standards and common specifications are mandated by the European Union, their application and conformity assessment stay in the locus of providers of high-risk AI systems. Consequently, certain high-risk AI systems may fall out of the scope of external auditing, and conformity assessment may be carried out in an inappropriate manner for the benefit of providers pursuing their own interests.
Besides, the legislators obliged providers, where high-risk AI systems are not in conformity with the AI Act, to take immediate corrective actions to remedy such non-compliance, or withdraw or recall such systems . Such actions shall be taken if providers consider or have reason to consider that the systems are non-compliant. Knowledge about such a risk may be based on providers own monitoring of their systems or on the information sourced from users. Although yet again this remedy is left at mercy of providers’ assessment, assumably as they are in better position to control the risk of non-compliance, we believe that this obligation may have more positive outcome as providers are obliged to report non-compliance and any corrective actions to the national competent authorities and the notified bodies, where applicable .  
It is our opinion that as the most reliable accountability tool under the AI Act may serve automatically generated logs. Such logs shall be kept by both providers  and users  to the extent they are under their respective control and may be accessible to the national competent authority for evaluation of compliance of high-risk AI systems with the AI Act . Logging capabilities may contribute to traceability of the systems per se, but, moreover, to backtracking of events and incidents and their connection with human actors’ behaviour be it preceding or subsequent (non)actions thereof. Thus, automatically generated logs may provide a possibility, to the certain extent, to evaluate human actors’ response and to better locate the cause of overall outcomes of systems operations. Nonetheless, one may argue that logging capabilities are not exempt from risk of manipulation, cybersecurity threats, errors, and bugs due to their nature. We will not address this matter as technical settings of robustness, accuracy, and security of technologies, including their logging capabilities, are not the subject of this work. 
Finally, we want to mention the requirement of “high-risk AI systems to be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons”  which is aimed to prevent or minimise “the risks to health, safety or fundamental rights that may emerge” in relation to (mis)use of high-risk AI systems . Human oversight is deemed to ensure respect of fundamental rights, including data protection,  by mitigating errors and bias that AI systems may bring. This is the only instrument under the AI Act which is provided exclusively to users and reflects accountability of high-risk AI systems thereto (and not to providers). Thus, this regulatory provision may intersect with, and potentially amplify, human intervenor status (respective rights and obligations) under Article 22 of the GDPR. We will explore the scope of human oversight in terms of the AI Act deeper further. 
It may be evidenced that the regulators intended to provide an enhanced framework for AI systems (quasi-)accountability to their providers and users by introducing technology-specific requirements to improve transparency and accountability thereof. However, such a quasi-accountability may only be useful if the critical forum (providers and data controllers) exists and is capable to knowledgeably operate algorithms . Thus, it is worth exploring whether the AI Act introduce or amplify requirements to human operators which have control over development or use of AI systems. As for automated decisions, we consider overall automated decision-making process, including where high-risk AI systems are employed, which by default shall include human input , as a socio-technical process. Consequently, human agents involved in this process shall be deemed as actors in terms of accountability and human-algorithm interaction  towards the forum of data subjects and a final combined decision shall be transparent and accountable in their eyes. Hence, we strongly believe that human agents, as a part of decision-making cycle, shall be required to meet certain criteria to enhance trust of individuals concerned by the automated decisions in terms of Article 22 of the GDPR, and we will explore further how the AI Act contributes to building trustworthiness in this regard. It is our opinion that the trustworthiness which the legislators are trying to achieve through implementation of the AI Act goes beyond the GDPR’s requirement of meaningful human intervention in automated decision-making, and confirms that the GDPR framework is not a sufficient protection of data subjects, and that human component of overall decision-making should comply with certain requirements so that the final decisional outcome received by a data subject be lawful, fair and trustworthy.
As a final note, we would like to refer to Olsen’s argument regarding accountability which originates from level of settlement of polities. He indicates that settled polities tend to have more clear understanding and agreement about allocation of responsibilities and criteria for their occurrence and the unsettled by contrast are prone to ambiguity thereof . Although Olsen’s realm of analysing accountability is public administration, we are of the opinion that this distinction may be equally applied to any phenomena. Thus, high-risk AI systems per se will be deemed as unsettled in Olsen’s terminology and accountability thereof is likely to be controversial. Therefore, it might be helpful to put them into ecosystem where already-known social agreements, as a basis of expectations in regards accountability, exists, for instance, where interaction between a vulnerable human and a human expert arises. If we assume that this is the case, it would be reasonable to imagine that humans concerned by the decisional outcome will trustfully rely on the expertise of human operators of ADMS, including high-risk AI systems, and will expect their responsibility for a final decision without regard to level of accountability in human-machine interaction. Thus, as the legislators intend to build trustworthiness of AI systems by introducing enhanced regulation thereof, it may be deemed appropriate that requirements to human operators as a part of socio-technical phenomenon of automated decision-making systems shall be amplified as well. 
 
3. Trust in humans: accountability and competence

As we stated above, automated decision-making is a complex socio-technical process which involves human actors almost in any cases . Human-centric approach is a recurrent topic in the European policies and laws relating to ADM . The context of such a position of the European legislators is protection of human dignity vis-à-vis machines and ensuring human control over automated decision-making tools . Human oversight of ADMS is deemed to cure algorithmic bias and discrimination safeguarding individuals’ fundamental rights . However, as we discussed in Chapter II of this work, there are two types of human presence in case of automated decision-making in terms of Article 22 of the GDPR: human intervenors on the side of data controllers and humans in their capacity of data subjects. Unarguably, their positions towards ADMS cannot be equal, neither from scope of rights and/or obligations nor from possible impacts points of view. Furthermore, previously we addressed the problem of accountability arisen in the domain of high-risk AI systems application which relates to “many hands” and “many eyes”. We concluded that data subjects cannot directly benefit from the newly introduced AI Act as its provisions concern providers and users of those systems, where users most likely act as data controllers. Thus, the AI Act may only contribute consequentially to a better protection of data subjects by envisaging additional obligations with respect to high-risk AI systems development, deployment, and use. 
We discussed how the AI Act may improve data subjects’ right to information and enhance transparency and accountability of systems involved in the decision-making per se by providing additional instruments to users (data controllers) and imposing certain obligations on providers making them accountable for their creation. This notwithstanding, as our opinion is based on inclusion of human actors on the side of data controllers into overall decision-making process due to the data subjects facing decisional outcomes aggregating (meaningful) human and machine inputs, we deem vital that such human actors, apart from being enabled to perform their obligations under the GDPR properly, are required to meet certain requirements to assure that data subjects are not subjected to unfair or discriminatory decisions. We assume that by addressing their goal as building an ecosystem of trust around use of AI systems, the legislators imply enhanced regulation not only of AI systems per se, but of additional elements constituting such an ecosystem existing in their development and use phases, including human actors . We will discuss further what their regulatory position on the notion of human involvement consists of and whether such a position strengthen data subjects protection in comparison with the GDPR.
3.1. Users accountability
Although the AI Act stipulates that providers of high-risk AI systems shall ensure that those systems meet legislative requirements and are compliant with law, their users (data controllers) are not released from their obligation to comply with the data protection regulation while processing personal data where AI systems are employed (in particular, for the purpose of automated decision-making). Thus, it may be evidenced that for the data-subjects forum the “actor”, that may be held to accounts, will be the data controller. 
Being users in terms of the AI Act, data controllers are provided with certain tools regarding their usage of high-risk AI systems. Thus, Article 14 states as follows (bold added):
“High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which the AI system is in use”, 
and
“Human oversight shall be ensured through either one or all of the following types of measures:
(a) measures identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the market or put into service;
(b) measures identified by the provider before placing the high-risk AI system on the market or putting it into service and that are appropriate to be implemented by the user.” 
These provisions clearly make us understand that human oversight shall be attained by mechanisms built-in at the design and development stage by providers or by those measures indicated by providers and performed by users in the use phase. This may prove that providers of high-risk AI systems are in a better position to identify tools for ensuring a possibility of human oversight by users by making their design choices. However, human oversight nor its quality are not guaranteed, at least, unless such systems are designed to contain certain “constraints that cannot be overridden by the system itself” . If this is the case, it is yet unclear what the human oversight would consist of and what it the appropriateness  criteria for placing of such measures. Another question to ask is whether users will be able to implement those measures with their skills and qualifications even when granted with such an opportunity by providers. 
The purpose of the oversight is prevention or minimization of the risks to health, safety and fundamental rights that may arisen during systems functioning in accordance with their intended purpose or under conditions of reasonably foreseeable misuse . Interestingly, if the data protection regulation (through authoritative guidelines of the EDPB) ties human intervention to the criteria of meaningfulness, the AI Act introduces criteria of risk in relation to variation of type and degree of the human oversight . Thus, Article 14 articulates appropriateness and proportionality to the circumstances regarding human oversight capabilities . Further, it envisages that AI systems shall be provided in such a way that human operator are enabled:
“(a) to understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation;
(b) to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (‘automation bias);
(c) to correctly interpret the high-risk AI system’s output, taking into account for example the interpretation tools and methods available;
(d) to decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system;
(e) to intervene on the operation of the high-risk AI system or interrupt the system through a “stop” button or a similar procedure.” 
Notably, all the listed actions may be performed only by human agents while the systems shall enable them to do so, that is to say, there should be technical capabilities and sufficient information to ensure that human operators are able to act accordingly. 
Subject to appropriateness and proportionality criteria, providers should define measures, either pre-implemented during the development or to-be-implemented by users during the use, that allow users to fulfill the above actions and provide information about those measures in the form of instructions . In turn, users shall use high-risk AI systems in accordance with such instructions  and shall implement human oversight and monitor the operation of the systems on the basis of those instructions . It may be assumed that human oversight may consist of those actions foreseen by Article 14(4) of the AI Act which high-risk AI system shall enable, however, there is no express obligation to do so nor any other requirements to the scope of such oversight, and further regulatory guidance is necessary to address this gap. Otherwise, providing for these tools to ensure algorithmic accountability these provisions may prove useless or, at least, with low practical sense resting disabled on the side of users. 
Notwithstanding the above, there are several appealing provisions relating to accountability of users, or their designated human operators, introduced by the AI Act with respect to automated decision-making. First, Article 14(5) provides for two controls where decision is made based on identification produced by remote identification systems . Such enhanced verification of algorithmic input is evidenced by significance of consequences for individuals in case of a system’s errors . At the same time, we assume that another reason for a stricter regulation hereof might be dictated by the nature of data processed being personal data of special categories under the GDPR. Nonetheless, the AI Act ruled out this requirement for the purpose of law enforcement, migration, border control or asylum, and as me lifted by Union or national laws due to disproportionality . Notably, the legislators have not envisaged collegial nature for other decisions made on a basis of algorithmic output, apparently, considering that single human operator suffices to ensure legitimacy and fairness of the final decision and not to impose unreasonable burden on users.
Further, Article 29 foresees certain obligations of users of high-risk AI systems. Thus, its paragraph 3 states that users shall use input data relevant for the intended purpose of the system . This obligation overlaps with the principle of data minimization provided by Article 5(1)(c) of the GDPR, and, in our opinion, does not amplify data subjects’ protection. As a part of their obligations, users shall keep automatically generated logs, where they are under users’ control, and the sectoral laws shall apply to such logs, if users are financial institutions . Considering the technical nature of measures to be implemented to ensure human oversight, we are of the opinion that such logs may prove traceability not only of high-risk AI systems operations, but of the actions taken by human operators, when requested to do so. We suggest this could be a powerful tool to allow audit and inspectability of human-machine interactions and assess what human input was at the time. Although it would be dependent on the design choices regarding the systems, but the conjunction of measures to guarantee human oversight and keeping automatically generated logs may contribute to transparency and improve decision-making process also where triage is concerned (we address the problem of identifying of the moment when decision, in terms of Article 22 of the GDPR is made in previous chapter). Thus, providers may foresee certain constrains to the system in such a way that it stops its operations unless (meaningful) human input is introduced, then the system continues its functioning ; this procedure may be repeated until the very end of a decisional process. Logs, in turn, will keep information about the moment and the outcome of ADMS for that stage and input of a human operator before resumption of operations. If such logs are properly kept and accessible, they can contribute not only to accountability of high-risk AI systems, but only to validate whether the human input was appropriately introduced. This kind of validations may be included in the scope of organizational measures implemented by data controllers under the GDPR  and may prove useful for the purposes of data protection impact assessment  as well. 
Another interesting novella of the AI Act is obligation of users that are public authorities, except for law enforcement, border control, immigration or asylum authorities, or entities acting on their behalf to register themselves as operators of high-risk AI systems in the EU database with indication of systems they opted to use . Arguably, this obligation may improve transparency for data subjects by allowing them to be aware of what systems may affect them and is aligned with the requirement of the GDPR to inform data subjects about existence of automated decision-making . Nonetheless, such a registration per se would not constitute provision of information compliant with the GDPR. Notably, the AI Act does not oblige private entities to proceed with the registration under Article 51(2), assumably, as it would be unreasonable intervention to their business. 
Considering the above, we reckon that the AI Act provides for enhanced tools regarding accountability of the users, or human agents appointed by them, while operating high-risk AI systems. Fulfilment of their obligations under the AI Act may also help to demonstrate users (data controllers) compliance with the GDPR, including by implementing human oversight measures and the traceability of human input. This notwithstanding these technical safeguards providing certain capabilities for human operators to intervene do not necessarily guarantee that the designated human will be able or willing to provide for meaningful input, neither this “meaningfulness” assure quality of the human intervention in relation to data subject affected by high-risk AI systems . Additional requirements of the AI Act towards human intervenors may, however, help improving this scenario.
3.2. CompetenceEthics
As we discussed previously, algorithmic accountability may only prove useful if the forum (users in this case) is capable to understand and assess their operations critically . Consideration must be given to human-algorithm interaction  and to the fact that human operators may act out of under- or over-reliance on algorithmic output  or introduce their own bias  to final decisional outcome, therefore, human intervention (or oversight) per se does not guarantee better decisions . Owing to high stakes at risk and considering socio-technical nature of algorithms, it would be unreasonably to left human-element of decision-making without being asked with certain requirements since it is placed as a gatekeeper for combatting with algorithmic threats. 
First, the EDPB introduced the meaningfulness qualifier to avoid rubber-stamping of automated output . Nonetheless, there is not enough practice nor discussion to contemplate its realistic implications . The European Commission also recognized that human oversight is not adequately addressed in the legislation . An attempt to deal with this gap is made by the AI Act . Thus, it is proposed for the first time at the legislative level from an overseer to comply with certain qualification. Article 29(1a) stipulates as follows:
“Users shall assign human oversight to natural persons who have the necessary competence, training and authority.”
It is our opinion, that “competence” is a key word in this requirement with a view to ensuring trustworthiness of engagement of high-risk AI systems and decision-making based on those systems. Unfortunately, the AI Act does not elaborate on what shall be included in the scope of “the necessary competence”. Unarguably, human operators shall possess adequate technical knowledge, experience, and education . These requirements are set, however, to address lack of technical skills and knowledge shortage  and do not seek to ensure added value of human input by demanding humans to demonstrate profound level of ethicality or morality neither their ability to make fair and non-discriminatory decisions nor adequate (proportionate) response in human-algorithm interaction. The law also states that measures implemented to facilitate human oversight shall enable the human operator to remain aware of automatically relying or over-relying on algorithmic outcome . However, it is not guaranteed that such awareness will be present in the human overseer neither under-reliance on algorithmic output is concerned. It might be more appropriate to establish a stronger framework of competence for human operators not only towards algorithms, but also to include those criteria necessary to improve decision-making facing “end-users” (data subjects).
However, the AI Act envisages two additional provisions through which such a framework of competence may be settled yet left to the discretion of users (data controllers) of high-risk AI systems. Thus, for the purpose of ensuring human oversight measures implementation, users may employ their own resources and activities and shall comply with other legal obligations , including those imposed by data protection legislation. Another interplay between the AI Act and the GDPR concerns data protection impact assessment (DPIA). Hence, Article 29(6) of the AI Act obliges users to utilize information in relation to high-risk AI systems obtained under Article 13 of the AI Act to comply with their obligation of DPIA . 
Although the AI Act does not include data subjects as a direct beneficiary of the new legislation focusing on providers and users, the provisions mentioned above, most likely, will find their place as a contributor to accountability of data controllers  and, consequently, will add value to ensuring better data subjects protection under the GDPR. For instance, Article 29(2) provision for usage of other resources and activities to ensure human oversight measures implementation is aligned, in our opinion, with data controllers’ obligation to implement technical and organizational measure to demonstrate compliance with the data protection regulation. Therefore, data controllers shall include those oversight measures in their technical and organizational measures implemented in course of accountability demonstration and, moreover, shall dedicate appropriate  resources and activities to implement those measures. Thus, data subjects may benefit from instructions given by providers (that assumably have better knowledge and control over AI systems developed by them) to users instead of relying only on data controllers’ internal understanding of employed algorithms (which indeed may be limited). 
Further, the obligation of users to utilize information provided by providers in their DPIA activities will most likely concern position of data subjects. Although DPIA obligation sits the locus of data controllers’ evaluation of risk to the rights and freedoms of natural persons, in case of automated-decision-making DPIA is likely to be inevitable . Consequently, data controllers will be obliged to consider information about high-risk AI systems operations, including human oversight measures and among those measures facilitating interpretation of algorithmic outputs . Human intervention in terms of the GDPR will be also assessed by data controllers in course of the DPIA . Arguably, this interplay between the regulations may provide enhanced framework of accountability of data controllers (and their designated human intervenors in ADM)  by assessing of information and measures regarding human-element of automated decision-making obtained from different sources. 
We share the opinion that such assessment may identify and define whether the scope of human participation (intervention) complies with the pre-requisite of meaningfulness, and therefore is compliant with the GDPR, and may prove beneficent due to the broader possibilities of data controllers to employ additional relevant instruments under the AI Act . Thus, the AI Act may amplify room for human oversight and complement the GDPR’s requirement of human intervention in terms of Article 22. However, we think that providing instruments for “human governance”  only affects mechanics available to data controllers for the purpose of their accountability, i.e., compliance with the data protection regulation, but does not improve quality of the foreseen human intervention.  Focusing on human governance as on “how” human intervention is introduced, should this be its place (human-in-the-loop, human-out-of-the-loop, human-on-the-loop, human-back-in-control)  or its applicability to individual or collective decisions  addresses mostly its technical side of applicability, but not its quality. Even in the scenario of complementing the GDPR by additional measures for ensuring of human oversight under provided by the AI Act, which indeed may enhance meaningfulness of human intervention, one should ask whether meaningfulness is enough, whether better understanding of explanations received by human actors or amplified human-algorithm interaction will improve actions of those who stand over automated decision-making systems  and whether data subjects will enjoy more qualitative decision-making.
Considering the claimed goal of the AI Act being creating an ecosystem of excellence and trust  and socio-technical nature of algorithms, we believe that human element of that ecosystem shall not be overlooked. From this perspective, we think the AI Act makes one step further by imposing requirements of necessary competence, training and authority being present in natural persons to whom human oversight is assigned. Assessment of such competence and skills may contribute to combating humans under- or over-reliance on employed technologies and address their own bias and accountability. This assessment may also be performed within the DPIA procedures or other forms of risk assessment as suggested in literature  although not yet received their reflection on a legislative level. Hence, we are of the opinion that although the AI Act enhances accountability instruments for data controllers (users) varying on a risk-allocation basis, which may help improving meaningfulness requirement compliance, the question of trustworthiness, that assumably goes beyond meaningfulness, of combined human-algorithmic decision-making employed by data controllers is not yet well-addressed by the regulation and needs further discussion. 
 
4. Ecosystem of trust. Is it real?

The AI Act is built on the Guidelines for Trustworthy Artificial Intelligence developed by the High-level Expert Group on Artificial Intelligence  which set up ethical principles and requirements for Trustworthy AI . The principle of respect of human autonomy says, among other things, that “the allocation of functions between humans and AI systems should follow human-centric design principles and leave meaningful opportunity for human choice” which means “securing human oversight over work processes in AI systems” . The requirement of human agency and oversight stands for AI systems being enabler and supporter of human autonomy and decision-making by allowing humans to make better, more informed choices in accordance with their goals . 
There is a recognition that AI systems bring “substantial benefits to individuals and society”, but at the same time “pose certain risks and may have negative impact” which shall be mitigated appropriately and proportionately . Thus, the AI Act builds a pyramid of AI systems based on the level of risk they may bring and provides for the requirements in relation to development, deployment, and use of those systems, mainly by addressing high-risk AI systems and prohibiting certain practices. It may be argued, nonetheless, that it rests unclear what are criteria for certain systems to be allocated to one or another group as there is only a typology of systems classified based on a level of risk, they may bring . Although the AI Act retains the possibility to further amend the list of high-risk AI systems, which are subject to major regulation under the Act, it may prove ambiguous in practice how this list will be altered with time . This notwithstanding, we are of the opinion that the listed domains, the use of AI systems in which makes such systems high-risk, stem from data protection regulation, in particular from guidelines on automated individual decision-making and profiling . 
Considering the above, it could be implied that the AI Act may enhance protection of data subjects by providing stricter rules regarding AI systems, being automated means for decision-making, due to their sophisticated nature and potential to affect society at large. We reckon the idea of trustworthiness, proliferated with regard to AI systems, goes beyond trust to decision-making wherein ADMS are involved which, assumably, shall be gained by meaningful human intervention prescribed under data protection regulation. Following this direction, we may notice that the AI Act enshrines amplified requirements to quality, safety and (quasi-)accountability of AI systems by imposing heightened transparency and explainability standard  and rules on data governance  as well as more room for human oversight . However, evidentially, these provisions apply to relations between providers and users (data controllers) of AI systems and, consequently, may only complement better performance of data controllers’ obligations towards data subjects  and improve data protection impact assessment  while depriving data subjects or their representatives from being a part of the assessment of AI systems or, at least express their point of view . 
Further, this enhanced regulation applies only to high-risk AI systems which means that in case if other ADMS are employed data controllers will stay under the burden of compliance with the GDPR and authoritative guidance in relation thereto. From the perspective of human intervention, this signifies the requirement of such intervention to be meaningful which practicality is broadly questioned . Equally, where high-risk AI systems are in place for the purpose of automated decision-making, better protection may be granted due to the absence of certain limitations thar are present in the GDPR. For instance, automated decision-making based on high-risk AI systems shall not be based on personal data only, therefore requirements to used datasets as discussed above will apply to any type of data. Further, the AI Act does not make any distinction between individual or collective decision-making. Hence, it may positively improve transparency and better control of impact produced on society as a whole or groups of individuals rather than on a single individual.
With regard to human agent role, it is noteworthy that while the GDPR requires human meaningful input once automated decision is made while the AI Act provides for human oversight which on the one hand does not necessarily means that the final decision shall be evaluated by human agent (unless it does not fall under the scope of Article 22(1) of the GDPR), but on the other hand enhance human agent participation in different stages of overall process. Assumably, this shall boost human participation and contribute to better understanding and control over algorithmic outcome of a human in charge, thus, ensuring data controllers’ accountability for compliance with the GDPR . Nonetheless, we wonder whether such accountability is enough, whether the AI Act by complementing the GDPR realistically provides data subjects with better control over their data and with amplified protection vis-à-vis automated decision-making and whether ADMS are capable to be trustworthy in the given legal framework.
The GDPR holds data controller accountable for compliance with the regulation . We share the opinion that humans shall be able to control, understand and intervene in the automated decision-making process and, where the critical areas of life are concerned, human shall be always present in ADM and, where necessary, be capable to intervene and interrupt such process. As we discussed above Article 22(1) and related authoritative guidance proved practically insufficient to bring to life human governance of algorithmic decisions. Certain constraints were evidenced in regards information rights of data subjects when automated decision-making is in place, and capabilities of humans, being designated personnel of data controllers for the purpose of human intervention, to perform their obligations, due to such reasons as algorithmic under- or over-reliance, lack of knowledge, skills, and competence. The AI Act intends to address this gap not only by introducing enhanced transparency requirements benefiting data protection impact assessment obligation under the GDPR , but also by imposing certain requirements to human overseers . It may be argued that the newly introduced regulation may revitalize and help enforcing provisions of data protection regulation related to human intervention in ADM . We share the opinion that in practice human intervention and its meaningfulness may be evaluated within DPIA  which shall be performed during the whole lifecycle of AMDS. 
Considering the AI Act amplifies informational basis for DPIA, more aspects of ADM impact may be addressed through the assessment including quality and relevance of data other than personal used for the purpose of ADM and societal impact together with quality of individual decisions . A separate mechanism envisaged by the AI Act to mitigate possible risks is risk management system  that constitute a continuous iterative process performed during the lifecycle of high-risk AI systems for the purpose of identification, analysis, and evaluation of known and foreseeable risks to fundamental rights and adoption of measures to mitigate those risks. Notably, the provision addresses considering of technical knowledge, experience, education, and training to be expected by the user in relation to high-risk AI systems . However, Article 9 of the AI Act considers only those risks which may be addressed through the development and design stages, i.e., risk management system shall be developed by providers of AI systems, and it is not clear whether users (data controllers) are required to run any specific risk management under the AI Act even though they will be obliged, most likely, to perform data protection impact assessment under the GDPR taking into consideration information obtained through the additional instruments granted by the AI Act. Therefore, our opinion is that although the AI Act intends to tackle the problem of lack of knowledge, skills, and competence on the side of human element of the automated decision-making, in practice these obligations may prove silent or, at least, unverifiable.
As we earlier stated in this work, data protection impact assessment may be a powerful tool to evaluate meaningfulness of human intervention in ADM, and we agree that amplified provisions on human oversight may improve the outcome of such assessment . However, taking into consideration the socio-technical nature of automated decision-making, i.e., combining of automated (algorithmic) and human elements in overall decisional outcome, human decision-maker shall be required to meet certain criteria of competence and quality of their input. Bearing in mind the learning capabilities of ADMS, consideration shall be given to the probability of further usage of wrongful or unfair final decision, containing human approval, in training and operations of ADMS which may result in reenforcing of discriminatory and unfair outcomes at large scale. Therefore, we strongly believe that natural persons assigned with human oversight, including, intervention shall be assessed, and not only scope of their (meaningful) intervention, just as the algorithmic aspect of decision-making to ensure trustworthiness of socio-technical notion of automated decision-making in eyes of those individual who are affected by automated decisions. 
From the above perspective, we reckon that further discussion and regulation are needed to include mandatory human governance evaluation in data protection impact assessment or other impact assessment tools, as the case may be, where human governance, contrasted to the meaning proposed by Lazcoz and de Hert , consists not only of ways in which human intervention is presented and its meaningfulness is achieved, but also of requirements to human agents’ competence, knowledge and moral character . 
